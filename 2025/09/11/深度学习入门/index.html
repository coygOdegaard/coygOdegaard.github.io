<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>深度学习入门 | coygOdegaard</title>

  <!-- keywords -->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="神经网络激活函数在神经网络发展的历史上， sigmoid函数很早就开始被使用了，而最近则主要使用ReLU（Rectified Linear Unit）函数。 ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0。 ReLU函数可以表示为下面的式。   def relu(x): 	return np.maximum(0, x)   输出层的激活函数用σ()表示，不同于隐藏层的激活函数">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门">
<meta property="og:url" content="http://example.com/2025/09/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="coygOdegaard">
<meta property="og:description" content="神经网络激活函数在神经网络发展的历史上， sigmoid函数很早就开始被使用了，而最近则主要使用ReLU（Rectified Linear Unit）函数。 ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0。 ReLU函数可以表示为下面的式。   def relu(x): 	return np.maximum(0, x)   输出层的激活函数用σ()表示，不同于隐藏层的激活函数">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250911110202414.png">
<meta property="og:image" content="http://example.com/source/imgs/$%7Bfiilname%7D/image-20250911110300246.png">
<meta property="og:image" content="http://example.com/source/imgs/$%7Bfiilname%7D/image-20250911154019871.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250911154756373.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250911155201797.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250911195950571.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250911200008808.png">
<meta property="og:image" content="http://example.com/source/imgs/$%7Bfiilname%7D/image-20250911202019486.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913095941450.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913100104125.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913100143104.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913101100478.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913101122323.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913101457738.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913101724780.png">
<meta property="og:image" content="http://example.com/source/imgs/$%7Bfiilname%7D/image-20250913102830344.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913103053341.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915200803430.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915194354567.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915194431174.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915194538985.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915194751261.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915194807290.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915194942860.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915203234346.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915201507943.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915202216158.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915202243610.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915202334230.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915203148184.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915203909822.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915203954895.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915213751053.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915213919994.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915214127994.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915214229318.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915214401550.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915214753398.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250915214852076.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916094735875.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916095036960.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916095355658.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916095541658.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916100018954.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916100528410.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913105112819.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913105141247.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913110528152.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913111620630.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913111730093.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913111919701.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913112226658.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913220538308.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913220620533.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913220655204.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913220857845.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913220924250.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913221152636.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913221249515.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913221526687.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913221932677.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913222046256.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250913222423357.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914191528335.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914191948788.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914192306122.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914202637781.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914205228236.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914205402764.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914210146223.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914210654169.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914210719918.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914210954006.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914211135593.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250914211227133.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916111157835.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916152546197.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916153339950.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916153356093.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916155041776.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916160121430.png">
<meta property="og:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250916160828227.png">
<meta property="article:published_time" content="2025-09-11T02:01:58.000Z">
<meta property="article:modified_time" content="2025-09-16T09:18:46.833Z">
<meta property="article:author" content="Odegaard">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/09/11/source/imgs/$%7Bfiilname%7D/image-20250911110202414.png">
  
    <link rel="alternative" href="/atom.xml" title="coygOdegaard" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  

  
<script src="//cdn.bootcss.com/require.js/2.3.2/require.min.js"></script>

  
<script src="//cdn.bootcss.com/jquery/3.1.1/jquery.min.js"></script>


  
<meta name="generator" content="Hexo 7.3.0"></head>
<body>
  <div id="container">
    <div id="particles-js"></div>
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
				<img lazy-src="" class="js-avatar">
			
		</a>

		<hgroup>
			<h1 class="header-author"><a href="/">Odegaard</a></h1>
		</hgroup>

		

		<div class="switch-area">
			<div class="switch-wrap">
				<section class="switch-part switch-part1">
					<nav class="header-menu">
						<ul>
						
							<li><a href="/">Home</a></li>
				        
							<li><a href="/archives">Archives</a></li>
				        
						</ul>
					</nav>
					<nav class="half-header-menu">
						<a class="hide">Home</a>
						<a>Tags</a>
						<a>Links</a>
						<a>About</a>
					</nav>
					<nav class="header-nav">
						<div class="social">
							
						</div>
						<!-- music -->
						
					</nav>
				</section>
				
				
				<section class="switch-part switch-part2">
					<div class="widget tagcloud" id="js-tagcloud">
						<a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 10px;">大数据</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 20px;">算法</a> <a href="/tags/%E8%AF%AD%E8%A8%80/" style="font-size: 15px;">语言</a>
					</div>
				</section>
				
				
				
				<section class="switch-part switch-part3">
					<div id="js-friends">
					
			          <a target="_blank" class="main-nav-link switch-friends-link" href="https://github.com/">github</a>
			        
			        </div>
				</section>
				

				
				
				<section class="switch-part switch-part4">
				
					<div id="js-aboutme">I&#39;m a developer.</div>
				</section>
				
			</div>
		</div>
	</header>				
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"></div>
  		<h1 class="header-author js-mobile-header hide"></h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img lazy-src="" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author"></h1>
			</hgroup>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">Home</a></li>
		        
					<li><a href="/archives">Archives</a></li>
		        
		        <div class="clearfix"></div>
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
				</div>
			</nav>
		</header>				
	</div>
</nav>
      <div class="body-wrap"><article id="post-深度学习入门" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a href="/2025/09/11/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" class="article-date">
  	<time datetime="2025-09-11T02:01:58.000Z" itemprop="datePublished">2025-09-11</time>
</a>
    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      深度学习入门
      
    </h1>
  

      </header>
      
      <div class="article-info article-info-post">
        
        

        
        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>在神经网络发展的历史上， sigmoid函数很早就开始被使用了，而最近则主要使用ReLU（Rectified Linear Unit）函数。</p>
<p>ReLU函数在输入大于0时，直接输出该值；在输入小于等于0时，输出0。</p>
<p>ReLU函数可以表示为下面的式。</p>
<img src="../source/imgs/${fiilname}/image-20250911110202414.png" alt="image-20250911110202414" style="zoom:50%;" />

<pre><code class="hljs plaintext">def relu(x):
	return np.maximum(0, x)</code></pre>

<p><img src="/../source/imgs/$%7Bfiilname%7D/image-20250911110300246.png" alt="image-20250911110300246"></p>
<p>输出层的激活函数用σ()表示，不同于隐藏层的激活函数h()（σ读作sigma）。</p>
<p><img src="/../source/imgs/$%7Bfiilname%7D/image-20250911154019871.png" alt="image-20250911154019871"></p>
<p>输出层所用的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid函数，多元分类问题可以使用 softmax函数。</p>
<p><strong>恒等函数</strong>会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。因此，在输出层使用恒等函数时，输入信号会原封不动地被输出。</p>
<p>分类问题中使用的<strong>softmax函数</strong>可以用下面的式表示。<img src="../source/imgs/${fiilname}/image-20250911154756373.png" alt="image-20250911154756373" style="zoom:50%;" /></p>
<pre><code class="hljs plaintext">def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y</code></pre>

<p>上面的softmax函数的实现虽然正确描述了式，但在计算机的运算上有一定的缺陷。这个缺陷就是溢出问题。 softmax函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。</p>
<p>softmax函数的实现可以像式这样进行改进。</p>
<img src="../source/imgs/${fiilname}/image-20250911155201797.png" alt="image-20250911155201797" style="zoom:50%;" />

<p>首先，在分子和分母上都乘上C这个任意的常数（因为同时对分母和分子乘以相同的常数，所以计算结果不变）。然后，把这个C移动到指数函数（exp）中，记为log C。最后，把log C替换为另一个符号<img src="../source/imgs/${fiilname}/image-20250911195950571.png" alt="image-20250911195950571" style="zoom:50%;" />。说明，在进行softmax的指数函数的运算时，加上（或者减去)某个常数并不会改变运算的结果。这里的<img src="../source/imgs/${fiilname}/image-20250911200008808.png" alt="image-20250911200008808" style="zoom:50%;" />可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值。</p>
<pre><code class="hljs plaintext">def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c) # 溢出对策
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y</code></pre>

<p>softmax函数的<strong>输出是0.0到1.0之间的实数</strong>。并且， <strong>softmax函数的输出值的总和是1</strong>。输出总和为1是softmax函数的一个重要性质。正因为有了这个性质，我们才可以把softmax函数的输出解释为“概率”。</p>
<p>比如，上面的例子可以解释成 y[0]的概率是0.018（1.8 %）， y[1]的概率是0.245（24.5 %）， y[2]的概率是0.737（73.7 %）。</p>
<p>即便使用了softmax函数，<strong>各个元素之间的大小关系也不会改变</strong>。这是因为指数函数（y &#x3D; exp(x)）是单调递增函数。实际上，上例中a的各元素的大小关系和y的各元素的大小关系并没有改变。比如， a的最大值是第2个元素， y的最大值也仍是第2个元素。</p>
<p>一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用softmax函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，<strong>输出层的softmax函数可以省略</strong>。</p>
<h2 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h2><p>这里使用的数据集是MNIST手写数字图像集。 MNIST是机器学习领域最有名的数据集之一，被应用于从简单的实验到发表的论文研究等各种场合。实际上，在阅读图像识别或机器学习的论文时， MNIST数据集经常作为实验用的数据出现。MNIST数据集是由0到9的数字图像构成的.</p>
<p><img src="/../source/imgs/$%7Bfiilname%7D/image-20250911202019486.png" alt="image-20250911202019486"></p>
<p>MNIST的图像数据是28像素 × 28像素的灰度图像（1通道），各个像素的取值在0到255之间。每个图像数据都相应地标有“7”“2”“1”等标签。</p>
<h1 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。</p>
<p>可以用作损失函数的函数有很多，其中最有名的是<strong>均方误差</strong><img src="../source/imgs/${fiilname}/image-20250913095941450.png" alt="image-20250913095941450" style="zoom:50%;" /></p>
<p>这里， yk是表示神经网络的输出， tk表示监督数据， k表示数据的维数。</p>
<p>在3.6节手写数字识别的例子中， yk、 tk是由如下10个元素构成的数据。</p>
<img src="../source/imgs/${fiilname}/image-20250913100104125.png" alt="image-20250913100104125" style="zoom:70%;" />

<p>t是监督数据，将正确解标签设为1，其他均设为0。这里，标签“2”为1，表示正确解是“2”。将正确解标签表示为1，其他标签表示为0的表示方法称为<strong>one-hot</strong>表示。</p>
<p>除了均方误差之外， <strong>交叉熵误差</strong>（cross entropy error）也经常被用作损失函数<img src="../source/imgs/${fiilname}/image-20250913100143104.png" alt="image-20250913100143104" style="zoom:50%;" /></p>
<p>这里， log表示以e为底数的自然对数（log e）。 yk是神经网络的输出， tk是正确解标签。并且， tk中只有正确解标签的索引为1，其他均为0（one-hot表示）。因此，式（4.2）实际上只计算<strong>对应正确解标签的输出的自然对数</strong>。比如，假设正确解标签的索引是“2”，与之对应的神经网络的输出是0.6，则交叉熵误差是-log 0.6 &#x3D; 0.51；若“2”对应的输出是0.1，则交叉熵误差为-log 0.1 &#x3D; 2.30。也就是说，交叉熵误差的值是由正确解标签所对应的输出结果决定的。</p>
<pre><code class="hljs plaintext">def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y + delta))</code></pre>

<p>这里，参数y和t是NumPy数组。函数内部在计算np.log时，<strong>加上了一个微小值delta</strong>。这是因为，当出现np.log(0)时， np.log(0)会变为负无限大的-inf，这样一来就会导致后续计算无法进行。作为保护性对策，<strong>添加一个微小值可以防止负无限大的发生</strong>。</p>
<p>此外，当监督数据是标签形式（非one-hot表示，而是像“2”“7”这样的标签）时，交叉熵误差可通过如下代码实现。</p>
<pre><code class="hljs plaintext">def cross_entropy_error(y, t):
    if y.ndim == 1:
    t = t.reshape(1, t.size)
    y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size</code></pre>

<p>需要求batch_size是因为此时的y不是一个数据。</p>
<p>作为参考，简单介绍一下np.log( y[np.arange(batch_size), t] )。 np.arange (batch_size)会生成一个从0到 batch_size-1的数组。比如当 batch_size为5时， np.arange(batch_size)会生成一个NumPy 数组 [0, 1, 2, 3, 4]。因为t中标签是以 [2, 7, 0, 9, 4]的形式存储的，所以 y[np.arange(batch_size), t]能抽出各个数据的正确解标签对应的神经网络的输出（在这个例子中， y[np.arange(batch_size), t] 会 生 成 NumPy 数 组 [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]）。</p>
<p><strong>epoch</strong>是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时的更新次数。比如，对于 10000笔训练数据，用大小为 100笔数据的mini-batch进行学习时，重复随机梯度下降法100次，所有的训练数据就都被“看过”了 A。此时，100次就是一个epoch。</p>
<p>实际上，一般做法是事先将所有训练数据随机打乱，然后按指定的批次大小，按序生成mini-batch。这样每个mini-batch均有一个索引号，比如此例可以是0, 1, 2, . . . , 99，然后用索引号可以遍历所有的mini-batch。遍历一次所有数据，就称为一个epoch。</p>
<h1 id="误差反向传播法"><a href="#误差反向传播法" class="headerlink" title="误差反向传播法"></a>误差反向传播法</h1><h2 id="激活函数层的实现"><a href="#激活函数层的实现" class="headerlink" title="激活函数层的实现"></a>激活函数层的实现</h2><p><strong>ReLU层</strong></p>
<p>激活函数ReLU（Rectified Linear Unit）<img src="../source/imgs/${fiilname}/image-20250913101100478.png" alt="image-20250913101100478" style="zoom:50%;" />，它的导数是<img src="../source/imgs/${fiilname}/image-20250913101122323.png" alt="image-20250913101122323" style="zoom:50%;" /></p>
<pre><code class="hljs plaintext">class Relu:
    def __init__(self):
        self.mask = None
        def forward(self, x):
        self.mask = (x &lt;= 0)
        out = x.copy()
        out[self.mask] = 0
        return out
    def backward(self, dout):
        dout[self.mask] = 0
        dx = dout
        return dx</code></pre>

<p><strong>Sigmoid层</strong></p>
<p>sigmoid函数<img src="../source/imgs/${fiilname}/image-20250913101457738.png" alt="image-20250913101457738" style="zoom:50%;" />，它的导数是<img src="../source/imgs/${fiilname}/image-20250913101724780.png" alt="image-20250913101724780" style="zoom:50%;" /></p>
<pre><code class="hljs plaintext">class Sigmoid:
    def __init__(self):
        self.out = None
        def forward(self, x):
        out = 1 / (1 + np.exp(-x))
        self.out = out
        return out
    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx</code></pre>

<p>这个实现中，正向传播时将输出保存在了实例变量 out中。然后，反向传播时，使用该变量out进行计算。</p>
<h2 id="Affine-Softmax层的实现"><a href="#Affine-Softmax层的实现" class="headerlink" title="Affine&#x2F;Softmax层的实现"></a>Affine&#x2F;Softmax层的实现</h2><p><strong>Affine层</strong></p>
<p>神经网络的<strong>正向传播</strong>中进行的<strong>矩阵的乘积运算</strong>在几何学领域被称为“仿射变换” 。因此，这里将进行仿射变换的处理实现为“Affine层”。</p>
<p><strong>Softmax-with-Loss 层</strong></p>
<p>最后介绍一下输出层的softmax函数。前面我们提到过， softmax函数会将输入值正规化之后再输出。比如手写数字识别时， Softmax层的输出如：</p>
<p><img src="/../source/imgs/$%7Bfiilname%7D/image-20250913102830344.png" alt="image-20250913102830344"></p>
<p>因为手写数字识别要进行10类分类，所以向Softmax层的输入也有10个。</p>
<p>神经网络中进行的处理有<strong>推理</strong>（inference）和<strong>学习</strong>两个阶段。神经网络的<strong>推理通常不使用Softmax层</strong>。比如，用图5-28的网络进行推理时，会将最后一个 Affine层的输出作为识别结果。神经网络中未被正规化的输出结果（图 5-28中 Softmax层前面的 Affine层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要Softmax层。不过，神经网络的<strong>学习阶段则需要Softmax层</strong>。</p>
<p>softmax函数记为Softmax层，交叉熵误差记为Cross Entropy Error层。这里假设要进行3类分类，从前面的层接收3个输入（得分）。如图5-30所示， Softmax层将输入（a1, a2, a3）正规化，输出（y1, y2, y3）。 Cross Entropy Error层接收Softmax的输出（y1, y2, y3）和教师标签（t1, t2, t3），从这些数据中输出损失L。</p>
<img src="../source/imgs/${fiilname}/image-20250913103053341.png" alt="image-20250913103053341" style="zoom:50%;" />

<p>Softmax层的反向传播得到了（y1 - t1, y2 - t2, y3 - t3）这样“漂亮”的结果。由于（y1, y2, y3）是Softmax层的输出，（t1, t2, t3）是监督数据，所以（y1 - t1, y2 - t2, y3 - t3）是Softmax层的输出和教师标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层。</p>
<h1 id="与学习相关的技巧"><a href="#与学习相关的技巧" class="headerlink" title="与学习相关的技巧"></a>与学习相关的技巧</h1><h2 id="参数的更新"><a href="#参数的更新" class="headerlink" title="参数的更新"></a>参数的更新</h2><h3 id="随机梯度下降法"><a href="#随机梯度下降法" class="headerlink" title="随机梯度下降法"></a>随机梯度下降法</h3><p><strong>随机梯度下降法</strong>（SGD），在解决某些问题的时候没有效率，因为梯度的方向没有指向最小值。<img src="../source/imgs/${fiilname}/image-20250915200803430.png" alt="image-20250915200803430" style="zoom:40%;" /></p>
<p><img src="../source/imgs/${fiilname}/image-20250915194354567.png" alt="image-20250915194354567" style="zoom:40%;" />，表示的函数是向x轴方向延伸的“碗”状函数。</p>
<img src="../source/imgs/${fiilname}/image-20250915194431174.png" alt="image-20250915194431174" style="zoom:50%;" />

<p>应用SGD，SGD呈“之”字形移动。这是一个相当低效的路径。也就是说， SGD的缺点是，如果函数的形状非均向，比如呈延伸状，搜索的路径就会非常低效。</p>
<img src="../source/imgs/${fiilname}/image-20250915194538985.png" alt="image-20250915194538985" style="zoom:40%;" />

<h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum是“动量”的意思，和物理有关。用数学式表示Momentum方法，<img src="../source/imgs/${fiilname}/image-20250915194751261.png" alt="image-20250915194751261" style="zoom:40%;" />，<img src="../source/imgs/${fiilname}/image-20250915194807290.png" alt="image-20250915194807290" style="zoom:40%;" /></p>
<p>和前面的SGD一样， W表示要更新的权重参数，<img src="../source/imgs/${fiilname}/image-20250915194942860.png" alt="image-20250915194942860" style="zoom:50%;" />表示损失函数关于W的梯度， η表示学习率。这里新出现了一个变量v，对应物理上的速度。第一个式子表示了物体在梯度方向上受力，在这个力的作用下，物体的速度增加这一物理法则。</p>
<pre><code class="hljs plaintext">class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None
    def update(self, params, grads):
        if self.v is None:
        self.v = &#123;&#125;
        for key, val in params.items():
        self.v[key] = np.zeros_like(val)
        for key in params.keys():
        self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]
        params[key] += self.v[key]</code></pre>

<p><code>numpy.zeros_like()</code> ，用于创建一个与给定数组<strong>形状和数据类型相同</strong>的新数组，但所有元素都被初始化为 <strong>0</strong>。</p>
<img src="../source/imgs/${fiilname}/image-20250915203234346.png" alt="image-20250915203234346" style="zoom:50%;" />

<img src="../source/imgs/${fiilname}/image-20250915201507943.png" alt="image-20250915201507943" style="zoom:50%;" />

<p><strong>与SGD的对比及其优势</strong></p>
<ul>
<li><strong>普通SGD（随机梯度下降）</strong>：<br><code>W ← W - η(∂L/∂W)</code><br>在梯度为零的区域（<code>∂L/∂W = 0</code>），更新量直接为零，优化过程会<strong>立刻停止</strong>。这很容易陷入一些平坦的局部最优点或鞍点（这些点的梯度也是零，但不是最优解）。</li>
<li><strong>Momentum方法</strong>：<br>即使在梯度为零的点，由于 <code>αv</code> 项的存在，​<strong>速度不会立刻为零</strong>。之前积累的动量会推动参数 <code>W</code> 继续向前移动一段距离。这带来了两大好处：<ol>
<li><strong>有助于逃离局部最优点和鞍点</strong>：如果小球有足够的动量，它就可以“滚过”一个平坦的局部极小点或鞍点，而不是陷在里面。</li>
<li><strong>抑制震荡，加速收敛</strong>：在沟壑（类似峡谷地形）中，梯度方向在沟壁间剧烈摇摆。普通的SGD会剧烈震荡，收敛缓慢。而Momentum方法中的 <code>αv</code> 项相当于一个迟滞系统，会将速度平均化，使更新方向更加一致地沿着沟壑的中心线（下降的主方向），从而<strong>大大减少震荡并加快收敛速度</strong></li>
</ol>
</li>
</ul>
<p>在优化过程中，“速度” <code>v</code> 是一个矢量，它同时记录了<strong>方向</strong>和<strong>大小</strong>。<code>αv</code> 项代表的是“历史移动趋势”。</p>
<ul>
<li><strong>当上一时刻的速度 <code>v</code> 为负值，且当前梯度 <code>-η(∂L/∂W)</code> 也是负值时</strong>：<ul>
<li>两者方向相同，<code>αv</code> 项（负值）与梯度力（负值）叠加，确实起到了<strong>加速</strong>的作用。这会使得本次更新沿着负方向走得更远、更快。</li>
</ul>
</li>
<li><strong>当上一时刻的速度 <code>v</code> 为负值，但当前梯度 <code>-η(∂L/∂W)</code> 为正值时</strong>：<ul>
<li>这通常发生在参数即将越过最低点（谷底）时。此时，梯度方向改变了（从向右下变为向左下），但历史动量（<code>αv</code>）仍然试图将它往原来的方向（右下）推。</li>
<li>这时，<code>αv</code>（负值）和梯度力（正值）方向相反，会<strong>相互抵消一部分</strong>。这起到了“刹车”或缓冲的作用，防止参数更新在最低点附近发生过于剧烈的震荡和摇摆。</li>
</ul>
</li>
</ul>
<p><strong>结论：</strong> <code>αv</code> 项并不总是起到“减速”作用。它的角色是<strong>保持历史运动趋势</strong>。这个趋势既可能加速（当与梯度同向时），也可能减速（当与梯度反向时）。这种机制使得优化过程在正确的方向上更快，同时能平滑掉一些错误方向的震荡。</p>
<p>但是，如果梯度为0的地方就是最低点，<strong>相比于没有动量的SGD（梯度为0就立刻停止），Momentum方法可能需要更多的迭代步骤来让这种振荡停止下来，最终精确收敛到最低点。</strong></p>
<h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>在关于学习率的有效技巧中，有一种被称为<strong>学习率衰减</strong>的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法，在神经网络的学习中经常被使用。</p>
<p>逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低。而AdaGrad进一步发展了这个想法，针对“<strong>一个一个</strong>”的参数，赋予其“定制”的值。AdaGrad会为参数的<strong>每个元素</strong>适当地调整学习率，与此同时进行学习。</p>
<img src="../source/imgs/${fiilname}/image-20250915202216158.png" alt="image-20250915202216158" style="zoom:50%;" />

<p>和前面的SGD一样， W表示要更新的权重参数， 表示损失函数关于W的梯度， η表示学习率。这里新出现了变量h，如式(6.5)所示，它保存了以前的所有梯度值的<strong>平方和</strong>（式（6.5）中的<img src="../source/imgs/${fiilname}/image-20250915202243610.png" alt="image-20250915202243610" style="zoom:50%;" />表示矩阵中对应元素的乘法）。然后，在更新参数时，通过乘以<img src="../source/imgs/${fiilname}/image-20250915202334230.png" alt="image-20250915202334230" style="zoom:50%;" />，就可以调整学习的尺度。这意味着，参数的<strong>元素中变动较大</strong>（被大幅更新)的元素的<strong>学习率将变小</strong>。也就是说，可以按参数的元素进行学习率衰减，使变动大的参数的学习率逐渐减小。</p>
<p>AdaGrad会记录<strong>过去所有梯度</strong>的平方和。因此，学习<strong>越深入</strong>，更新的<strong>幅度就越小</strong>。实际上，如果无止境地学习，更新量就会变为 0，完全不再更新。为了改善这个问题，可以使用 <strong>RMSProp</strong>方法。RMSProp方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。</p>
<pre><code class="hljs plaintext">class AdaGrad:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None
        
    def update(self, params, grads):
        if self.h is None:
            self.h = &#123;&#125;
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)            
        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)</code></pre>

<img src="../source/imgs/${fiilname}/image-20250915203148184.png" alt="image-20250915203148184" style="zoom:50%;" />

<p>由于y轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此， y轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。</p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Momentum参照小球在碗中滚动的物理规则进行移动， AdaGrad为参数的每个元素适当地调整更新步伐。Adam就是融合了这两个方法。具体的想看去看论文。</p>
<img src="../source/imgs/${fiilname}/image-20250915203909822.png" alt="image-20250915203909822" style="zoom:50%;" />

<p>虽然Momentun也有类似的移动，但是相比之下， Adam的小球左右摇晃的程度有所减轻。这得益于学习的更新程度被适当地调整了。</p>
<img src="../source/imgs/${fiilname}/image-20250915203954895.png" alt="image-20250915203954895" style="zoom:50%;" />

<pre><code class="hljs plaintext">class Adam:

    &quot;&quot;&quot;Adam (http://arxiv.org/abs/1412.6980v8)&quot;&quot;&quot;

    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
        
    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = &#123;&#125;, &#123;&#125;
            for key, val in params.items():
                self.m[key] = np.zeros_like(val)
                self.v[key] = np.zeros_like(val)
        
        self.iter += 1
        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        
        for key in params.keys():
            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            
            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
            
            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias
            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias
            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)</code></pre>

<h2 id="权重的初始值"><a href="#权重的初始值" class="headerlink" title="权重的初始值"></a>权重的初始值</h2><h3 id="隐藏层的激活值分布"><a href="#隐藏层的激活值分布" class="headerlink" title="隐藏层的激活值分布"></a>隐藏层的激活值分布</h3><p>做一个简单的实验，观察权重初始值是如何影响隐藏层的激活值的分布的。这里要做的实验是，向一个5层神经网络（激活函数使用sigmoid函数）传入随机生成的输入数据，用直方图绘制各层激活值的数据分布。</p>
<pre><code class="hljs plaintext">import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))
    
x = np.random.randn(1000, 100) # 1000个数据
node_num = 100 # 各隐藏层的节点（神经元）数
hidden_layer_size = 5 # 隐藏层有5层
activations = &#123;&#125; # 激活值的结果保存在这里
for i in range(hidden_layer_size):
    if i != 0:
    	x = activations[i-1]
    w = np.random.randn(node_num, node_num) * 1
    z = np.dot(x, w)
    a = sigmoid(z) # sigmoid函数
    activations[i] = a
for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + &quot;-layer&quot;)
    plt.hist(a.flatten(), 30, range=(0,1))
plt.show()</code></pre>

<img src="../source/imgs/${fiilname}/image-20250915213751053.png" alt="image-20250915213751053" style="zoom:50%;" />

<p>各层的激活值呈偏向0和1的分布。这里使用的sigmoid函数是S型函数，随着输出不断地靠近0（或者靠近1），它的导数的值逐渐接近0。因此，偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失。这个问题称为<strong>梯度消失</strong>。层次加深的深度学习中，梯度消失的问题可能会更加严重。</p>
<p>下面，将权重的标准差设为0.01，进行相同的实验。<code>w = np.random.randn(node_num, node_num) * 0.01</code></p>
<img src="../source/imgs/${fiilname}/image-20250915213919994.png" alt="image-20250915213919994" style="zoom:50%;" />

<p>这次呈集中在0.5附近的分布。因为不像刚才的例子那样偏向0和1，所以不会发生梯度消失的问题。但是，激活值的分布有所<strong>偏向</strong>，说明在表现力上会有很大问题。为什么这么说呢？因为如果有多个神经元都输出几乎相同的值，那它们就没有存在的意义了。比如，如果100个神经元都输出几乎相同的值，那么也可以由1个神经元来表达基本相同的事情。因此，激活值在分布上有所偏向会出现“<strong>表现力受限</strong>”的问题。</p>
<p><strong>各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。</strong></p>
<p>接着，我们尝试使用Xavier Glorot等人的论文中推荐的权重初始值（俗称“Xavier初始值”）。现在，在一般的深度学习框架中， Xavier初始值已被作为标准使用。</p>
<p>如果前一层的节点数为n，则初始值使用标准差为<img src="../source/imgs/${fiilname}/image-20250915214127994.png" alt="image-20250915214127994" style="zoom:50%;" />的分布。</p>
<p><strong>Xavier初始值：与前一层有n个节点连接时，初始值使用标准差为<img src="../source/imgs/${fiilname}/image-20250915214229318.png" alt="image-20250915214229318" style="zoom:50%;" />的分布。</strong></p>
<p>使用Xavier初始值后，前一层的节点数<strong>越多</strong>，要设定为目标节点的初始值的权重尺度就<strong>越小</strong>。现在，我们使用Xavier初始值进行实验。</p>
<pre><code class="hljs plaintext">node_num = 100 # 前一层的节点数
w = np.random.randn(node_num, node_num) / np.sqrt(node_num)</code></pre>

<img src="../source/imgs/${fiilname}/image-20250915214401550.png" alt="image-20250915214401550" style="zoom:50%;" />

<p>后面的层的分布呈稍微歪斜的形状。如果用<strong>tanh函数</strong>（双曲线函数）代替 sigmoid函数，这个稍微歪斜的问题就能得到改善。实际上，使用 tanh函数后，会呈漂亮的吊钟型分布。 tanh函数和sigmoid函数同是S型曲线函数，但tanh函数是<strong>关于原点(0, 0)对称</strong>的S型曲线，而 sigmoid函数是关于(x, y)&#x3D;(0, 0.5)对称的S型曲线。众所周知，<strong>用作激活函数的函数最好具有关于原点对称的性质</strong>。</p>
<h3 id="ReLU的权重初始值"><a href="#ReLU的权重初始值" class="headerlink" title="ReLU的权重初始值"></a>ReLU的权重初始值</h3><p>Xavier初始值是以激活函数是<strong>线性函数</strong>为前提而推导出来的。因为sigmoid函数和 tanh函数左右对称，且中央附近可以视作线性函数，所以适合使用Xavier初始值。但当激活函数<strong>使用ReLU</strong>时，一般推荐使用ReLU专用的初始值，也就是Kaiming He等人推荐的初始值，也称为“<strong>He初始值</strong>” 。当前一层的节点数为n时， He初始值使用标准差为<img src="../source/imgs/${fiilname}/image-20250915214753398.png" alt="image-20250915214753398" style="zoom:50%;" />的高斯分布。</p>
<img src="../source/imgs/${fiilname}/image-20250915214852076.png" alt="image-20250915214852076" style="zoom:50%;" />

<p>当“std &#x3D; 0.01”时，各层的激活值非常小 。神经网络上传递的是非常小的值，说明逆向传播时权重的梯度也同样很小。这是很严重的问题，实际上学习基本上没有进展。</p>
<p>接下来是初始值为<strong>Xavier初始值</strong>时的结果。在这种情况下，随着层的<strong>加深</strong>，<strong>偏向</strong>一点点<strong>变大</strong>。实际上，层加深后，激活值的偏向变大，学习时会出现<strong>梯度消失</strong>的问题。而当初始值为<strong>He初始值</strong>时，各层中<strong>分布的广度相同</strong>。由于即便层加深，数据的广度也能保持不变，因此逆向传播时，也会传递合适的值。</p>
<p><strong>当激活函数使用ReLU时，权重初始值使用He初始值，当激活函数为 sigmoid或 tanh等S型曲线函数时，初始值使用Xavier初始值。</strong></p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。那么，为了使各层拥有适当的广度，<strong>“强制性”地调整激活值的分布</strong>会怎样呢？实际上， Batch Normalization方法就是基于这个想法而产生的。</p>
<h3 id="Batch-Normalization-的算法"><a href="#Batch-Normalization-的算法" class="headerlink" title="Batch Normalization 的算法"></a>Batch Normalization 的算法</h3><p>优点：</p>
<p>• 可以使学习快速进行（可以增大学习率）。</p>
<p>• 不那么依赖初始值（对于初始值不用那么神经质）。</p>
<p>• 抑制过拟合（降低Dropout等的必要性）。</p>
<p>Batch Norm的思路是<strong>调整各层的激活值分布</strong>使其拥有适当的广度。为此，要向神经网络中插入对数据分布进行<strong>正规化</strong>的层，即Batch Normalization层（下文简称Batch Norm层），如图6-16所示。</p>
<img src="../source/imgs/${fiilname}/image-20250916094735875.png" alt="image-20250916094735875" style="zoom:50%;" />

<p>Batch Norm，顾名思义，以进行学习时的mini-batch为单位，按minibatch进行正规化。具体而言，就是进行使数据分布的均值为0、方差为1的<strong>正规化</strong>。</p>
<img src="../source/imgs/${fiilname}/image-20250916095036960.png" alt="image-20250916095036960" style="zoom:50%;" />

<p>式（6.7）中的ε是一个微小值（比如， 10e-7等），它是为了防止出现除以0的情况。</p>
<p>通过将这个处理插入到激活函数的前面（或者后面） ，可以减小数据分布的偏向。</p>
<p>接着， Batch Norm层会对正规化后的数据进行缩放和平移的变换，<img src="../source/imgs/${fiilname}/image-20250916095355658.png" alt="image-20250916095355658" style="zoom:50%;" />，γ和β是参数。一开始γ &#x3D; 1， β &#x3D; 0，然后再通过学习调整到合适的值。</p>
<p>Batch Norm的反向传播在Frederik Kratzert 的博客“Understanding the backward pass through Batch Normalization Layer”里有详细说明。</p>
<img src="../source/imgs/${fiilname}/image-20250916095541658.png" alt="image-20250916095541658" style="zoom:50%;" />

<p>几乎所有的情况下都是使用Batch Norm时学习进行得更快。同时也可以发现，实际上，在不使用Batch Norm的情况下，如果不赋予一个尺度好的初始值，学习将完全无法进行。</p>
<h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="权值衰减"><a href="#权值衰减" class="headerlink" title="权值衰减"></a>权值衰减</h3><p>该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。</p>
<p>对于所有权重，权值衰减方法都会为损失函数加上<img src="../source/imgs/${fiilname}/image-20250916100018954.png" alt="image-20250916100018954" style="zoom:50%;" />，因此，在求权重梯度的计算中，要为之前的误差反向传播法的结果加上正则化项的导数λW。</p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>如果网络的模型变得很复杂，只用权值衰减就难以应对了。在这种情况下，我们经常会使用Dropout方法。</p>
<p>Dropout是一种在学习的过程中<strong>随机删除神经元</strong>的方法。训练时，随机选出隐藏层的神经元，然后将其删除。<strong>被删除的神经元不再进行信号的传递</strong>，如图6-22所示。训练时，<strong>每传递一次</strong>数据，就会随机选择要删除的神经元。然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。</p>
<img src="../source/imgs/${fiilname}/image-20250916100528410.png" alt="image-20250916100528410" style="zoom:50%;" />

<p>下面的实现重视易理解性。不过，因为训练时如果进行恰当的计算的话，正向传播时单纯地传递数据就可以了（不用乘以删除比例），所以深度学习的框架中进行了这样的实现。  </p>
<pre><code class="hljs plaintext">class Dropout:
    def __init__(self, dropout_ratio=0.5):
        self.dropout_ratio = dropout_ratio
        self.mask = None
    def forward(self, x, train_flg=True):
        if train_flg:
            self.mask = np.random.rand(*x.shape) &gt; self.dropout_ratio
            return x * self.mask
        else:
        	return x * (1.0 - self.dropout_ratio)
    def backward(self, dout):
    	return dout * self.mask</code></pre>

<p>机器学习中经常使用<strong>集成学习</strong>。所谓集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值。用神经网络的语境来说，比如，准备5个结构相同（或者类似）的网络，分别进行学习，测试时，以这5个网络的输出的平均值作为答案。实验告诉我们，通过进行集成学习，神经网络的识别精度可以提高好几个百分点。这个集成学习与 Dropout有密切的关系。这是因为可以将 Dropout理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例（比如，0.5等），可以取得模型的平均值。也就是说，可以理解成， <strong>Dropout将集成学习的效果（模拟地）通过一个网络实现了</strong>。</p>
<h2 id="超参数的验证"><a href="#超参数的验证" class="headerlink" title="超参数的验证"></a>超参数的验证</h2><p>超参数是指，比如各层的神经元数量、 batch大小、参数更新时的学习率或权值衰减等。如果这些超参数没有设置合适的值，模型的性能就会很差。虽然超参数的取值非常重要，但是在决定超参数的过程中一般会伴随很多的试错。</p>
<h3 id="验证数据"><a href="#验证数据" class="headerlink" title="验证数据"></a>验证数据</h3><p><strong>不能使用测试数据评估超参数的性能。</strong></p>
<p>为什么不能用测试数据评估超参数的性能呢？这是因为如果使用测试数据调整超参数，超参数的值会对测试数据发生过拟合。换句话说，用测试数据确认超参数的值的“好坏”，就会导致超参数的值被调整为只拟合测试数据。这样的话，可能就会得到不能拟合其他数据、泛化能力低的模型。</p>
<p>因此，调整超参数时，必须使用超参数专用的确认数据。用于调整超参数的数据，一般称为验证数据。我们使用这个验证数据来评估超参数的好坏。</p>
<p>训练数据用于参数（权重和偏置）的学习，验证数据用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）测试数据。</p>
<h3 id="超参数的最优化"><a href="#超参数的最优化" class="headerlink" title="超参数的最优化"></a>超参数的最优化</h3><p>进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。</p>
<p>在进行神经网络的超参数的最优化时，与网格搜索等有规律的搜索相比，<strong>随机采样</strong>的搜索方式效果更好。</p>
<p>超参数的范围只要“大致地指定”就可以了。所谓“大致地指定”，是指像0.001（10^-3）到1000（10^3）这样，以“<strong>10的阶乘</strong>”的尺度指定范围（也表述为“用对数尺度（log scale）指定”）。这在Python中可以写成 <code>10 ** np.random. uniform(-3, 3)</code>。</p>
<p>在超参数的最优化中，要注意的是深度学习需要很长时间（比如，几天或几周）。因此，在超参数的搜索中，需要尽早放弃那些不符合逻辑的超参数。于是，在超参数的最优化中，<strong>减少学习的epoch</strong>，缩短一次评估所需的时间是一个不错的办法。  </p>
<p><strong>步骤0</strong></p>
<p>设定超参数的范围。</p>
<p><strong>步骤1</strong></p>
<p>从设定的超参数范围中随机采样。</p>
<p><strong>步骤2</strong></p>
<p>使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将epoch设置得很小）。</p>
<p><strong>步骤3</strong></p>
<p>重复步骤1和步骤2（100次等），根据它们的识别精度的结果，缩小超参数的范围。</p>
<p>反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。</p>
<p>在超参数的最优化中，如果需要更精炼的方法，可以使用<strong>贝叶斯最优化</strong>。贝叶斯最优化运用以贝叶斯定理为中心的数学理论，能够更加严密、高效地进行最优化。</p>
<h1 id="卷积神经网络-CNN"><a href="#卷积神经网络-CNN" class="headerlink" title="卷积神经网络 CNN"></a>卷积神经网络 CNN</h1><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p>CNN中新出现了卷积层（Convolution层）和池化层（Pooling层）。</p>
<p>之前介绍的神经网络中，相邻层的所有神经元之间都有连接，这称为<strong>全连接</strong>（fully-connected）。另外，我们用Affine层实现了全连接层。如果使用这个Affine层，一个5层的全连接的神经网络就可以通过图7-1所示的网络结构来实现。</p>
<p>如图7-1所示，全连接的神经网络中， Affine层后面跟着激活函数ReLU层（或者Sigmoid层）。这里堆叠了4层“Affine-ReLU”组合，然后第5层是Affine层，最后由Softmax层输出最终结果（概率）。</p>
<img src="../source/imgs/${fiilname}/image-20250913105112819.png" alt="image-20250913105112819" style="zoom:50%;" />

<p>CNN的一个例子：<img src="../source/imgs/${fiilname}/image-20250913105141247.png" alt="image-20250913105141247" style="zoom:50%;" /></p>
<p>CNN 中 新 增 了 Convolution 层 和 Pooling 层。 CNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling层有时会被省略）。这可以理解为之前的“Affi ne - ReLU”连接被替换成了“Convolution - ReLU -（Pooling）”连接。</p>
<p>还需要注意的是，在图7-2的CNN中，靠近输出的层中使用了之前的“Affine - ReLU”组合。此外，最后的输出层中使用了之前的“Affine - Softmax”组合。这些都是一般的CNN中比较常见的结构。</p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><h3 id="全连接层的问题"><a href="#全连接层的问题" class="headerlink" title="全连接层的问题"></a>全连接层的问题</h3><p>之前介绍的全连接的神经网络中使用了全连接层（Affine层）。在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意决定。</p>
<p>全连接层存在什么问题呢？那就是<strong>数据的形状被“忽视”</strong>了。比如，输入数据是图像时，图像通常是高、长、通道方向上的3维形状。但是，向全连接层输入时，需要将3维数据拉平为1维数据。实际上，前面提到的使用了MNIST数据集的例子中，输入图像就是1通道、高28像素、长28像素的（1, 28, 28）形状，但却被排成1列，以784个数据的形式输入到最开始的Affine层。</p>
<p>图像是3维形状，这个形状中应该含有重要的空间信息。比如，空间上邻近的像素为相似的值、 RBG的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等， 3维形状中可能隐藏有值得提取的本质模式。但是，因为<strong>全连接层</strong>会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以<strong>无法利用与形状相关的信息</strong>。</p>
<p>而<strong>卷积层可以保持形状不变</strong>。当输入数据是图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式输出至下一层。因此，在CNN中，<strong>可以（有可能）正确理解图像等具有形状的数据</strong>。</p>
<p>另外， CNN 中，有时将卷积层的输入输出数据称为<strong>特征图</strong>（feature map）。其中，卷积层的输入数据称为<strong>输入特征图</strong>（input feature map），输出数据称为<strong>输出特征图</strong>（output feature map）。</p>
<h3 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h3><p>卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。</p>
<p>滤波器运算，可以把它理解为给图像“戴上一副特殊的眼镜”或“使用一个修图工具”，来达到某种特定的效果。</p>
<p>在图像处理中，<strong>滤波器（Filter）</strong>，有时也称为<strong>内核（Kernel）</strong>或<strong>掩模（Mask）</strong>，是一个小的数字矩阵。滤波器运算就是<strong>将这个小的矩阵（滤波器）在大的数字矩阵（原始图像）上滑动，并在每个位置进行一系列数学计算，从而生成一幅新图像的过程</strong>。</p>
<p>这个过程在数学上称为<strong>卷积（Convolution）</strong>，因此也常被称为<strong>卷积运算</strong>。</p>
<img src="../source/imgs/${fiilname}/image-20250913110528152.png" alt="image-20250913110528152" style="zoom:50%;" />

<p>有的文献中也会用“核”这个词来表示这里所说的“滤波器”。</p>
<p>如何计算：</p>
<p>对于输入数据，卷积运算以一定间隔滑动滤波器的窗口并应用。这里所说的窗口是指图7-4中灰色的3 × 3的部分。如图7-4所示，将各个位置上滤波器的元素和输入的对应元素相乘，然后再求和（有时将这个计算称为<strong>乘积累加运算</strong>）。然后，将这个结果保存到输出的对应位置。将这个过程在所有位置都进行一遍，就可以得到卷积运算的输出。</p>
<img src="../source/imgs/${fiilname}/image-20250913111620630.png" alt="image-20250913111620630" style="zoom:50%;" />

<p>在全连接的神经网络中，除了权重参数，还存在偏置。 CNN中，滤波器的参数就对应之前的权重。并且， CNN中也存在偏置。图7-3的卷积运算的例子一直展示到了应用滤波器的阶段。包含偏置的卷积运算的处理流如图7-5所示。</p>
<p>如图7-5所示，向应用了滤波器的数据加上了<strong>偏置</strong>。偏置通常只有1个（1 × 1）（本例中，相对于应用了滤波器的4个数据，偏置只有1个），这个值会被<strong>加到应用了滤波器的所有元素上</strong>。</p>
<img src="../source/imgs/${fiilname}/image-20250913111730093.png" alt="image-20250913111730093" style="zoom:67%;" />

<h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>在进行卷积层的处理之前，有时要<strong>向输入数据的周围填入固定的数据</strong>（比如0等），这称为<strong>填充</strong>（padding），是卷积运算中经常会用到的处理。比如，在图7-6的例子中，对大小为(4, 4)的输入数据应用了幅度为1的填充。“<strong>幅度为1的填充</strong>”是指用幅度为1像素的0填充周围。</p>
<img src="../source/imgs/${fiilname}/image-20250913111919701.png" alt="image-20250913111919701" style="zoom:50%;" />

<p>使用填充主要是为了<strong>调整输出的大小</strong>。比如，对大小为(4, 4)的输入数据应用(3, 3)的滤波器时，输出大小变为(2, 2)，相当于输出大小比输入大小缩小了2个元素。这在反复进行多次卷积运算的深度网络中会成为问题。为什么呢？因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例子中，将填充的幅度设为1，那么相对于输入大小(4, 4)，输出大小也保持为原来的(4, 4)。因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。</p>
<h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>应用滤波器的位置间隔称为<strong>步幅</strong>（stride）。之前的例子中步幅都是1，如果将步幅设为2，则如图7-7所示，应用滤波器的窗口的间隔变为2个元素。</p>
<img src="../source/imgs/${fiilname}/image-20250913112226658.png" alt="image-20250913112226658" style="zoom:50%;" />

<p>步幅可以指定应用滤波器的间隔。</p>
<p>综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。如果将这样的关系写成算式，会如何呢？接下来，我们看一下对于填充和步幅，如何计算输出大小。</p>
<p>这里，假设输入大小为(H, W)，滤波器大小为(FH, FW)，输出大小为(OH, OW)，填充为P，步幅为S。</p>
<img src="../source/imgs/${fiilname}/image-20250913220538308.png" alt="image-20250913220538308" style="zoom:50%;" />

<img src="../source/imgs/${fiilname}/image-20250913220620533.png" alt="image-20250913220620533" style="zoom:50%;" />

<p>这里需要注意的是，虽然只要代入值就可以计算输出大小，但是所设定的值必须使<img src="../source/imgs/${fiilname}/image-20250913220655204.png" alt="image-20250913220655204" style="zoom:50%;" />分别可以除尽。当输出大小无法除尽时（结果是小数时），需要采取报错等对策。顺便说一下，根据<strong>深度学习的框架的不同</strong>，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。</p>
<h3 id="3维数据的卷积运算"><a href="#3维数据的卷积运算" class="headerlink" title="3维数据的卷积运算"></a>3维数据的卷积运算</h3><p>图像是3维数据，除了高、长方向之外，还需要处理通道方向。这里，我们按照与之前相同的顺序，看一下对加上了通道方向的3维数据进行卷积运算的例子。</p>
<p>图7-8是卷积运算的例子，图7-9是计算顺序。这里以3通道的数据为例，展示了卷积运算的结果。和2维数据时（图7-3的例子）相比，可以发现纵深方向（通道方向）上特征图增加了。通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并<strong>将结果相加</strong>，从而得到输出。</p>
<img src="../source/imgs/${fiilname}/image-20250913220857845.png" alt="image-20250913220857845" style="zoom:50%;" />

<img src="../source/imgs/${fiilname}/image-20250913220924250.png" alt="image-20250913220924250" style="zoom:60%;" />

<p>在3维数据的卷积运算中，<strong>输入数据和滤波器的通道数要设为相同的值</strong>。在这个例子中，输入数据和滤波器的通道数一致，均为3。滤波器大小可以设定为任意值（不过，<strong>每个通道的滤波器大小要全部相同</strong>）。这个例子中滤波器大小为(3, 3)，但也可以设定为(2, 2)、 (1, 1)、 (5, 5)等任意值。再强调一下，通道数只能设定为和输入数据的通道数相同的值（本例中为3）。</p>
<p>通道数为 C、高度为H、长度为W的数据的形状可以写成（C, H, W）。滤波器也一样，要按（channel, height, width）的顺序书写。比如，通道数为C、滤波器高度为FH（Filter Height）、长度为FW（Filter Width）时，可以写成（C, FH, FW）。</p>
<img src="../source/imgs/${fiilname}/image-20250913221152636.png" alt="image-20250913221152636" style="zoom:67%;" />

<p>如果要在通道方向上也拥有多个卷积运算的输出，该怎么做呢？为此，就需要用到<strong>多个滤波器</strong>（权重）。用图表示的话，如图7-11所示。</p>
<img src="../source/imgs/${fiilname}/image-20250913221249515.png" alt="image-20250913221249515" style="zoom:50%;" />

<p>通过应用FN个滤波器，输出特征图也生成了FN个。如果将这FN个特征图汇集在一起，就得到了形状为(FN, OH, OW)的方块。将这个方块传给下一层，就是CNN的处理流。</p>
<p>关于卷积运算的滤波器，也必须考虑滤波器的数量。因此，作为4维数据，滤波器的权重数据要按(output_channel, input_ channel , height, width)的顺序书写。比如，通道数为3、大小为5 × 5的滤波器有20个时，可以写成(20, 3, 5, 5)。</p>
<p>卷积运算中（和全连接层一样）<strong>存在偏置</strong>。在图7-11的例子中，如果进一步追加偏置的加法运算处理，则结果如下面的图7-12所示。</p>
<p>图7-12中，每个通道只有一个偏置。这里，偏置的形状是(FN, 1, 1)，滤波器的输出结果的形状是(FN, OH, OW)。这两个方块相加时，要对滤波器的输出结果(FN, OH, OW)<strong>按通道加上相同的偏置值</strong>。另外，不同形状的方块相加时，可以基于NumPy的广播功能轻松实现（1.5.5节）。</p>
<img src="../source/imgs/${fiilname}/image-20250913221526687.png" alt="image-20250913221526687" style="zoom:50%;" />

<h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>需要将在各层间传递的数据保存为4维数据。具体地讲，就是按(batch_num, channel, height, width)的顺序保存数据。比如，将图7-12中的处理改成对N个数据进行批处理时，数据的形状如图7-13所示。</p>
<p>图7-13的批处理版的数据流中，在各个数据的开头添加了批用的维度。像这样，数据作为4维的形状在各层间传递。这里需要注意的是，网络间传递的是4维数据，对这N个数据进行了卷积运算。也就是说，批处理将N次的处理汇总成了1次进行。</p>
<img src="../source/imgs/${fiilname}/image-20250913221932677.png" alt="image-20250913221932677" style="zoom:50%;" />

<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化是<strong>缩小高、长方向上的空间</strong>的运算。比如，如图7-14所示，进行将2 × 2的区域集约成1个元素的处理，缩小空间大小。</p>
<img src="../source/imgs/${fiilname}/image-20250913222046256.png" alt="image-20250913222046256" style="zoom:50%;" />

<p>图7-14的例子是按步幅2进行2 × 2的Max池化时的处理顺序。“Max池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。一般来说，<strong>池化的窗口大小会和步幅设定成相同的值</strong>。比如， 3 × 3的窗口的步幅会设为3， 4 × 4的窗口的步幅会设为4等。</p>
<p>除了Max池化之外，还有Average池化等。相对于Max池化是从目标区域中取出最大值，<strong>Average池化</strong>则是计算目标区域的平均值。在图像识别领域，主要使用Max池化。因此，本书中说到“池化层”时，指的是Max池化。</p>
<p>池化层有以下<strong>特征</strong>：</p>
<p><strong>没有要学习的参数</strong></p>
<p>池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。</p>
<p><strong>通道数不发生变化</strong></p>
<p>经过池化运算，输入数据和输出数据的通道数不会发生变化。如图7-15所示，计算是按通道独立进行的。</p>
<p><strong>对微小的位置变化具有鲁棒性（健壮）</strong></p>
<p>输入数据发生微小偏差时，池化仍会返回相同的结果。因此，池化对输入数据的微小偏差具有鲁棒性。比如， 3 × 3的池化的情况下，如图7-16所示，池化会吸收输入数据的偏差（根据数据的不同，结果有可能不一致）。</p>
<img src="../source/imgs/${fiilname}/image-20250913222423357.png" alt="image-20250913222423357" style="zoom:50%;" />

<h2 id="卷积层和池化层的实现"><a href="#卷积层和池化层的实现" class="headerlink" title="卷积层和池化层的实现"></a>卷积层和池化层的实现</h2><h3 id="4维数组"><a href="#4维数组" class="headerlink" title="4维数组"></a>4维数组</h3><p>CNN中各层间传递的数据是4维数据。所谓4维数据，比如数据的形状是(10, 1, 28, 28)，则它对应10个高为28、长为28、通道为1的数据。用Python来实现的话，如下所示。</p>
<pre><code class="hljs plaintext">&gt;&gt;&gt; x = np.random.rand(10, 1, 28, 28) # 随机生成数据
&gt;&gt;&gt; x.shape
(10, 1, 28, 28)</code></pre>

<p>如果要访问第1个数据，只要写 x[0]就可以了。<code>&gt;&gt;&gt; x[0].shape # (1, 28, 28)</code>  </p>
<p>如果要访问第1个数据的第1个通道的空间数据：<code>&gt;&gt;&gt; x[0, 0] # 或者x[0][0]</code>  </p>
<h3 id="基于-im2col的展开"><a href="#基于-im2col的展开" class="headerlink" title="基于 im2col的展开"></a>基于 im2col的展开</h3><p>如果老老实实地实现卷积运算，估计要重复好几层的 for语句。这样的实现有点麻烦，而且， NumPy中存在使用for语句后处理变慢的缺点（NumPy中，访问元素时最好不要用 for语句）。这里，我们不使用 for语句，而是使用im2col这个便利的函数进行简单的实现。</p>
<p>im2col是一个函数，将输入数据展开以适合滤波器（权重）。如图7-17所示，对3维的输入数据<strong>应用im2col</strong>后，<strong>数据转换为2维矩阵</strong>（正确地讲，是把包含批数量的4维数据转换成了2维数据）。</p>
<img src="../source/imgs/${fiilname}/image-20250914191528335.png" alt="image-20250914191528335" style="zoom:50%;" />

<p>im2col会把输入数据展开以适合滤波器（权重）。具体地说，如图7-18所示，对于输入数据，<strong>将应用滤波器的区域（3维方块）横向展开为1列</strong>。 im2col会在所有应用滤波器的地方进行这个展开处理。</p>
<img src="../source/imgs/${fiilname}/image-20250914191948788.png" alt="image-20250914191948788" style="zoom:50%;" />

<p>在图7-18中，为了便于观察，将步幅设置得很大，以使滤波器的应用区域不重叠。而在实际的卷积运算中，滤波器的应用区域几乎都是重叠的。在滤波器的应用区域重叠的情况下，使用im2col展开后，展开后的元素个数会多于原方块的元素个数。因此，使用im2col的实现存在比普通的实现<strong>消耗更多内存</strong>的缺点。但是，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处。</p>
<p>使用 im2col展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为1列，并计算2个矩阵的乘积即可（参照图7-19）。这和全连接层的Affi ne层进行的处理基本相同。</p>
<p>如图7-19所示，基于 im2col方式的<strong>输出结果是2维矩阵</strong>。因为CNN中数据会保存为4维数组，所以要将2维输出数据<strong>转换</strong>为合适的形状。以上就是卷积层的实现流程。</p>
<img src="../source/imgs/${fiilname}/image-20250914192306122.png" alt="image-20250914192306122" style="zoom:50%;" />

<h3 id="卷积层的实现"><a href="#卷积层的实现" class="headerlink" title="卷积层的实现"></a>卷积层的实现</h3><pre><code class="hljs plaintext">def im2col(input_data, filter_h, filter_w, stride=1, pad=0):
    &quot;&quot;&quot;

    Parameters
    ----------
    input_data : 由(数据量, 通道, 高, 长)的4维数组构成的输入数据
    filter_h : 滤波器的高
    filter_w : 滤波器的长
    stride : 步幅
    pad : 填充

    Returns
    -------
    col : 2维数组
    &quot;&quot;&quot;
    N, C, H, W = input_data.shape
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1

    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], &#x27;constant&#x27;)
    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))

    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]

    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)
    return col</code></pre>

<p>重塑为2D矩阵：</p>
<ul>
<li>行数：<code>N * out_h * out_w</code>（所有输出位置）</li>
<li>列数：<code>C * filter_h * filter_w</code>（每个位置的滤波器窗口）</li>
</ul>
<p>现在使用im2col来实现卷积层。这里我们将卷积层实现为名为Convolution的类。</p>
<pre><code class="hljs plaintext">class Convolution:
    def __init__(self, W, b, stride=1, pad=0):
        self.W = W
        self.b = b
        self.stride = stride
        self.pad = pad
    def forward(self, x):
        FN, C, FH, FW = self.W.shape
        N, C, H, W = x.shape
        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)
        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)
        col = im2col(x, FH, FW, self.stride, self.pad)
        col_W = self.W.reshape(FN, -1).T # 滤波器的展开
        out = np.dot(col, col_W) + self.b
        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)
        return out</code></pre>

<p>卷积层的初始化方法将滤波器（权重）、偏置、步幅、填充作为参数接收。滤波器是 (FN, C, FH, FW)的 4 维形状。另外， FN、 C、 FH、 FW分别是 Filter Number（滤波器数量）、 Channel、 Filter Height、 Filter Width的缩写。</p>
<p>这里通过 reshape(FN,-1)将参数指定为 -1，这是reshape的一个便利的功能。通过在 reshape时指定为 -1， reshape函数会自动计算 -1维度上的元素个数，以使多维数组的元素个数前后一致。比如， (10, 3, 5, 5)形状的数组的元素个数共有750个，指定 reshape(10,-1)后，就会转换成(10, 75)形状的数组。forward的实现中，最后会将输出大小转换为合适的形状。转换时使用了NumPy的transpose函数。 transpose会更改多维数组的轴的顺序。</p>
<img src="../source/imgs/${fiilname}/image-20250914202637781.png" alt="image-20250914202637781" style="zoom:50%;" />

<p>以上就是卷积层的 forward处理的实现。通过使用 im2col进行展开，基本上可以像实现全连接层的Affine层一样来实现。接下来是卷积层的反向传播的实现，因为和Affine层的实现有很多共通的地方，所以就不再介绍了。但有一点需要注意，在<strong>进行卷积层的反向传播时，必须进行im2col的逆处理</strong>。</p>
<p>“必须进行 im2col 的逆处理” 指的是将梯度信息从展开的矩阵形式转换回原始图像格式的关键步骤。</p>
<p><strong>反向传播过程：</strong></p>
<ol>
<li>计算输出梯度（损失函数对输出的导数）</li>
<li><strong>关键步骤：将梯度转换回 im2col 格式</strong></li>
<li>计算滤波器梯度：<code>滤波器梯度 = im2col(input)^T × 输出梯度</code></li>
<li><strong>关键步骤：计算输入梯度（需要 im2col 的逆操作）</strong></li>
</ol>
<ul>
<li>前向传播通过 <code>im2col</code> 改变了数据表示形式</li>
<li>反向传播必须沿相同路径反向传播梯度</li>
<li>需要将梯度从矩阵形式映射回原始图像格式</li>
</ul>
<pre><code class="hljs plaintext">def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):
    &quot;&quot;&quot;

    Parameters
    ----------
    col :
    input_shape : 输入数据的形状（例：(10, 1, 28, 28)）
    filter_h :
    filter_w
    stride
    pad

    Returns
    -------

    &quot;&quot;&quot;
    N, C, H, W = input_shape
    out_h = (H + 2*pad - filter_h)//stride + 1
    out_w = (W + 2*pad - filter_w)//stride + 1
    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)

    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))
    for y in range(filter_h):
        y_max = y + stride*out_h
        for x in range(filter_w):
            x_max = x + stride*out_w
            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]

    return img[:, :, pad:H + pad, pad:W + pad]</code></pre>

<h3 id="池化层的实现"><a href="#池化层的实现" class="headerlink" title="池化层的实现"></a>池化层的实现</h3><p>池化层的实现和卷积层相同，都使用im2col展开输入数据。不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同。具体地讲，如图7-21所示，<strong>池化的应用区域按通道单独展开</strong>。</p>
<img src="../source/imgs/${fiilname}/image-20250914205228236.png" alt="image-20250914205228236" style="zoom:50%;" />

<p>像这样展开之后，只需对展开的矩阵求各行的最大值，并转换为合适的形状即可（图7-22）。</p>
<img src="../source/imgs/${fiilname}/image-20250914205402764.png" alt="image-20250914205402764" style="zoom:50%;" />

<p>Python的实现示例：</p>
<pre><code class="hljs plaintext">class Pooling:
    def __init__(self, pool_h, pool_w, stride=1, pad=0):
        self.pool_h = pool_h
        self.pool_w = pool_w
        self.stride = stride
        self.pad = pad
    def forward(self, x):
        N, C, H, W = x.shape
        out_h = int(1 + (H - self.pool_h) / self.stride)
        out_w = int(1 + (W - self.pool_w) / self.stride)
        # 展开(1)
        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)
        col = col.reshape(-1, self.pool_h*self.pool_w)
        # 最大值(2)
        out = np.max(col, axis=1)
        # 转换(3)
        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)
        return out</code></pre>

<p>池化层的实现按下面3个阶段进行：</p>
<p>1.展开输入数据。2.求各行的最大值。3.转换为合适的输出大小。</p>
<h2 id="CNN的实现"><a href="#CNN的实现" class="headerlink" title="CNN的实现"></a>CNN的实现</h2><p>要实现如图7-23所示的CNN。</p>
<img src="../source/imgs/${fiilname}/image-20250914210146223.png" alt="image-20250914210146223" style="zoom:50%;" />

<p>自己去看</p>
<h2 id="CNN的可视化"><a href="#CNN的可视化" class="headerlink" title="CNN的可视化"></a>CNN的可视化</h2><h3 id="第1层权重的可视化"><a href="#第1层权重的可视化" class="headerlink" title="第1层权重的可视化"></a>第1层权重的可视化</h3><p>第1层的卷积层的权重的形状是(30, 1, 5, 5)，即30个大小为5 × 5、通道为1的滤波器。滤波器大小是5 × 5、通道数是1，意味着滤波器可以可视化为1通道的灰度图像。现在，我们将卷积层（第1层）的滤波器显示为图像。这里，我们来比较一下学习前和学习后的权重，结果如图7-24所示。</p>
<p>图7-24中，学习前的滤波器是随机进行初始化的，所以在黑白的浓淡上没有规律可循，但学习后的滤波器变成了有规律的图像。我们发现，通过学习，滤波器被更新成了有规律的滤波器，比如从白到黑渐变的滤波器、含有块状区域（称为blob）的滤波器等。</p>
<img src="../source/imgs/${fiilname}/image-20250914210654169.png" alt="image-20250914210654169" style="zoom:50%;" />

<p>右边的有规律的滤波器在“观察”什么，答案就是它在观察边缘（颜色变化的分界线）和斑块（局部的块状区域）等。比如，左半部分为白色、右半部分为黑色的滤波器的情况下，如图7-25所示，会对垂直方向上的边缘有响应。</p>
<img src="../source/imgs/${fiilname}/image-20250914210719918.png" alt="image-20250914210719918" style="zoom:50%;" />

<p>图7-25中显示了选择两个学习完的滤波器对输入图像进行卷积处理时的结果。我们发现“滤波器1”对垂直方向上的边缘有响应，“滤波器2</p>
<p>”对水平方向上的边缘有响应。</p>
<p>由此可知，卷积层的滤波器会提取边缘或斑块等原始信息。而刚才实现的CNN会将这些原始信息传递给后面的层。</p>
<h3 id="基于分层结构的信息提取"><a href="#基于分层结构的信息提取" class="headerlink" title="基于分层结构的信息提取"></a>基于分层结构的信息提取</h3><p>第1层的卷积层中提取了边缘或斑块等“低级”信息，那么在堆叠了多层的CNN中，各层中又会提取什么样的信息呢？</p>
<p>图7-26中展示了进行一般物体识别（车或狗等）的8层CNN。这个网络结构的名称是下一节要介绍的AlexNet。 AlexNet网络结构堆叠了多层卷积层和池化层，最后经过全连接层输出结果。图7-26的方块表示的是中间数据，对于这些中间数据，会连续应用卷积运算。</p>
<img src="../source/imgs/${fiilname}/image-20250914210954006.png" alt="image-20250914210954006" style="zoom:50%;" />

<p>如图7-26所示，如果堆叠了多层卷积层，则随着<strong>层次加深</strong>，<strong>提取的信息也愈加复杂、抽象</strong>，这是深度学习中很有意思的一个地方。最开始的层对简单的边缘有响应，接下来的层对纹理有响应，再后面的层对更加复杂的物体部件有响应。也就是说，随着层次加深，神经元从简单的形状向“高级”信息变化。换句话说，就像我们理解东西的“含义”一样，响应的对象在逐渐变化。</p>
<h2 id="具有代表性的CNN"><a href="#具有代表性的CNN" class="headerlink" title="具有代表性的CNN"></a>具有代表性的CNN</h2><p><strong>LeNet</strong></p>
<p>LeNet在1998年被提出，是进行手写数字识别的网络。如图7-27所示，它有连续的卷积层和池化层（正确地讲，是只“抽选元素”的子采样层），最后经全连接层输出结果。</p>
<img src="../source/imgs/${fiilname}/image-20250914211135593.png" alt="image-20250914211135593" style="zoom:50%;" />

<p>和“现在的CNN”相比， LeNet有几个不同点。第一个不同点在于激活函数。 LeNet中使用sigmoid函数，而现在的CNN中主要使用ReLU函数。此外，原始的LeNet中使用子采样（subsampling）缩小中间数据的大小，而现在的CNN中Max池化是主流。</p>
<p><strong>AlexNet</strong></p>
<img src="../source/imgs/${fiilname}/image-20250914211227133.png" alt="image-20250914211227133" style="zoom:50%;" />

<p>AlexNet叠有多个卷积层和池化层，最后经由全连接层输出结果。虽然结构上AlexNet和LeNet没有大的不同，但有以下几点差异。</p>
<p>• 激活函数使用ReLU。</p>
<p>• 使用进行局部正规化的LRN（Local Response Normalization）层。</p>
<p>• 使用Dropout。</p>
<h1 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h1><p>深度学习是加深了层的深度神经网络。基于之前介绍的网络，只需通过叠加层，就可以创建深度网络。</p>
<h2 id="加深网络"><a href="#加深网络" class="headerlink" title="加深网络"></a>加深网络</h2><p>创建一个如图8-1所示的网络结构的CNN（一个比之前的网络都深的网络）。这里使用的卷积层全都是3 × 3的小型滤波器，特点是随着层的加深，通道数变大（卷积层的通道数从前面的层开始按顺序以16、 16、 32、 32、 64、 64的方式增加）。此外，如图8-1所示，插入了池化层，以逐渐减小中间数据的空间大小；并且，后面的全连接层中使用了Dropout层。</p>
<img src="../source/imgs/${fiilname}/image-20250916111157835.png" alt="image-20250916111157835" style="zoom:50%;" />

<p>这个网络使用He初始值作为权重的初始值，使用Adam</p>
<p>更新权重参数。把上述内容总结起来，这个网络有如下特点。</p>
<p>• 基于3× 3的小型滤波器的卷积层。</p>
<p>• 激活函数是ReLU。</p>
<p>• 全连接层的后面使用Dropout层。</p>
<p>• 基于Adam的最优化。</p>
<p>• 使用He初始值作为权重初始值。</p>
<p>对于手写数字识别这样一个比较简单的任务，没有必要将网络的表现力提高到那么高的程度。因此，可以说加深层的好处并不大。而之后要介绍的大规模的一般物体识别的情况，因为问题复杂，所以加深层对提高识别精度大有裨益。</p>
<p>集成学习、学习率衰减、 Data Augmentation（数据扩充）等都有助于提高识别精度。尤其是Data Augmentation，虽然方法很简单，但在提高识别精度上效果显著。</p>
<p>Data Augmentation基于算法“人为地”扩充输入图像（训练图像）。具体地说，如图8-4所示，对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化，增加图像的数量。这在数据集的图像数量有限时尤其有效。</p>
<img src="../source/imgs/${fiilname}/image-20250916152546197.png" alt="image-20250916152546197" style="zoom:50%;" />

<p>除了如图8-4所示的变形之外， Data Augmentation还可以通过其他各种方法扩充图像，比如裁剪图像的“crop处理”、将图像左右翻转的“flip处理”（flip处理只在不需要考虑图像对称性的情况下有效。）等。对于一般的图像，施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。</p>
<p><strong>加深层的好处</strong>：</p>
<p><strong>可以减少网络的参数数量</strong>。说得详细一点，就是与没有加深层的网络相比，加深了层的网络可以用更少的参数达到同等水平（或者更强）的表现力。这一点结合卷积运算中的滤波器大小来思考就好理解了。比如，图8-5展示了由5 × 5的滤波器构成的卷积层。</p>
<img src="../source/imgs/${fiilname}/image-20250916153339950.png" alt="image-20250916153339950" style="zoom:50%;" />

<img src="../source/imgs/${fiilname}/image-20250916153356093.png" alt="image-20250916153356093" style="zoom:50%;" />

<p>一次5 × 5的卷积运算的区域可以由两次3 × 3的卷积运算抵充。并且，相对于前者的参数数量25（5 × 5），后者一共是18（2 × 3 × 3），通过叠加卷积层，参数数量减少了。而且，这个参数数量之差会随着层的加深而变大。比如，重复三次3 × 3的卷积运算时，参数的数量总共是27。而为了用一次卷积运算“观察”与之相同的区域，需要一个7 × 7的滤波器，此时的参数数量是49。</p>
<p><strong>叠加小型滤波器</strong>来加深网络的好处是<strong>可以减少参数的数量，扩大感受野</strong>（receptive field，给神经元施加变化的某个局部空间区域）。并且，通过叠加层，将 <strong>ReLU等</strong>激活函数夹在卷积层的中间，进一步提高了网络的表现力。这是因为向网络添加了基于激活函数的“非线性”表现力，通过<strong>非线性函数</strong>的叠加，可以表现更加复杂的东西。  </p>
<p>加深层的另一个好处就是<strong>使学习更加高效</strong>。与没有加深层的网络相比，通过加深层，可以减少学习数据，从而高效地进行学习。为了直观地理解这一点，CNN的卷积层会分层次地提取信息。具体地说，在前面的卷积层中，神经元会对边缘等简单的形状有响应，随着层的加深，开始对纹理、物体部件等更加复杂的东西有响应。</p>
<p>我们先牢记这个网络的分层结构，然后考虑一下“狗”的识别问题。要用浅层网络解决这个问题的话，卷积层需要一下子理解很多“狗”的特征。“狗”有各种各样的种类，根据拍摄环境的不同，外观变化也很大。因此，要理解“狗”的特征，需要大量富有差异性的学习数据，而这会导致学习需要花费很多时间。</p>
<p>不过，通过加深网络，就可以分层次地分解需要学习的问题。因此，各层需要学习的问题就变成了更简单的问题。比如，最开始的层只要专注于学习边缘就好，这样一来，只需用较少的学习数据就可以高效地进行学习。这是为什么呢？因为和印有“狗”的照片相比，包含边缘的图像数量众多，并且边缘的模式比“狗”的模式结构更简单。</p>
<p>通过加深层，<strong>可以分层次地传递信息</strong>。比如，因为提取了边缘的层的下一层能够使用边缘的信息，所以应该能够高效地学习更加高级的模式。也就是说，通过加深层，可以<strong>将各层要学习的问题分解成容易解决的简单问题</strong>，从而可以进行高效的学习。</p>
<h2 id="深度学习的高速化"><a href="#深度学习的高速化" class="headerlink" title="深度学习的高速化"></a>深度学习的高速化</h2><p>深度学习中什么样的处理比较耗时。图8-14中以AlexNet的 forward处理为对象，用饼图展示了各层所耗费的时间。</p>
<p>从图中可知， AlexNex中，<strong>大多数时间都被耗费在卷积层上</strong>。实际上，卷积层的处理时间加起来占GPU整体的95%，占CPU整体的89%！因此，如何高速、高效地进行卷积层中的运算是深度学习的一大课题。虽然图8-14是推理时的结果，不过学习时也一样，卷积层中会耗费大量时间。</p>
<img src="../source/imgs/${fiilname}/image-20250916155041776.png" alt="image-20250916155041776" style="zoom:50%;" />

<p>卷积层中进行的运算可以追溯至乘积累加运算。因此，深度学习的高速化的主要课题就变成了如何高速、高效地进行大量的乘积累加运算。</p>
<p>GPU计算，是指基于GPU进行通用的数值计算的操作。</p>
<p>深度学习中需要进行大量的<strong>乘积累加运算</strong>（或者<strong>大型矩阵的乘积运算</strong>）。这种大量的<strong>并行运算</strong>正是GPU所擅长的（反过来说， CPU比较擅长连续的、复杂的计算）。因此，与使用单个CPU相比，使用GPU进行深度学习的运算可以达到惊人的高速化。下面我们就来看一下基于GPU可以实现多大程度的高速化。图8-15是基于CPU和GPU进行AlexNet的学习时分别所需的时间。</p>
<p>从图中可知，使用CPU要花40天以上的时间，而使用GPU则可以将时间缩短至6天。此外，还可以看出，通过<strong>使用cuDNN</strong>这个最优化的库，可以进一步实现高速化。</p>
<img src="../source/imgs/${fiilname}/image-20250916160121430.png" alt="image-20250916160121430" style="zoom:50%;" />

<p>大多数深度学习框架只受益于NVIDIA的GPU。这是因为深度学习的框架中使用了NVIDIA提供的CUDA这个面向GPU计算的综合开发环境。</p>
<p>通过 im2col可以将卷积层进行的运算转换为大型矩阵的乘积。相比按小规模的单位进行计算，GPU更擅长计算大规模的汇总好的数据。也就是说，通过基于 im2col以大型矩阵的乘积的方式汇总计算，更容易发挥出GPU的能力。</p>
<p>为了进一步提高深度学习所需的计算的速度，可以考虑在<strong>多个GPU或者多台机器</strong>上进行分布式计算。现在的深度学习框架中，出现了好几个支持多GPU或者多机器的分布式学习的框架。其中， Google的TensorFlow、微软的CNTK（Computational Network Toolki）在开发过程中高度重视分布式学习。以大型数据中心的低延迟· 高吞吐网络作为支撑，基于这些框架的分布式学习呈现出惊人的效果。</p>
<p>基于分布式学习，可以达到何种程度的高速化呢？图8-16中显示了基于TensorFlow的分布式学习的效果。</p>
<img src="../source/imgs/${fiilname}/image-20250916160828227.png" alt="image-20250916160828227" style="zoom:50%;" />

<p>如图8-16所示，随着GPU个数的增加，学习速度也在提高。实际上，与使用1个GPU时相比，使用100个GPU（设置在多台机器上，共100个）似乎可以实现56倍的高速化。</p>
<p>关于分布式学习，“如何进行分布式计算”是一个非常难的课题。它包含了机器间的通信、数据的同步等多个无法轻易解决的问题。可以将这些难题都交给TensorFlow等优秀的框架。</p>
<p>在深度学习的高速化中，除了计算量之外，内存容量、总线带宽等也有可能成为瓶颈。关于内存容量，需要考虑将大量的权重参数或中间数据放在内存中。关于总线带宽，当流经GPU（或者CPU）总线的数据超过某个限制时，就会成为瓶颈。考虑到这些情况，我们希望尽可能<strong>减少流经网络的数据的位数</strong>。</p>
<p>计算机中为了表示实数，主要使用64位或者32位的浮点数。通过使用较多的位来表示数字，虽然数值计算时的误差造成的影响变小了，但计算的处理成本、内存使用量却相应地增加了，还给总线带宽带来了负荷。</p>
<p>关于数值精度（用几位数据表示数值），我们已经知道<strong>深度学习并不那么需要数值精度的位数</strong>。这是神经网络的一个重要性质。这个性质是基于神经网络的健壮性而产生的。这里所说的健壮性是指，比如，即便输入图像附有一些小的噪声，输出结果也仍然保持不变。可以认为，正是因为有了这个健壮性，流经网络的数据即便有所“劣化”，对输出结果的影响也较小。</p>
<p>计算机中表示小数时，有32位的单精度浮点数和64位的双精度浮点数等格式。根据以往的实验结果，在深度学习中，即便是<strong>16位的半精度浮点数</strong>（half float），也可以顺利地进行学习。</p>
<p>以往的深度学习的实现中并没有注意数值的精度，不过Python中一般使用64位的浮点数。 NumPy中提供了16位的半精度浮点数类型（不过，只有16位类型的存储，运算本身不用16位进行），即便使用NumPy的半精度浮点数，识别精度也不会下降。</p>

      
    </div>
    
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2025/07/08/python%E6%9A%91%E5%81%87/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">python暑假</div>
      <strong class="article-nav-caption">&gt;</strong>
    </a>
  
</nav>

  
</article>






</div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
      <div class="footer-left">
        &copy; 2025 Odegaard
      </div>
        <div class="footer-right">
          <a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/preccrep/hexo-theme-jelly" target="_blank">Jelly</a>
        </div>
    </div>
  </div>
</footer>
    </div>
    
  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">



<script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: false,
		animate: true,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false
	}
</script>

<script src="/js/main.js"></script>




  </div>
</body>
</html>