<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>机器学习 | coygOdegaard</title><meta name="author" content="Odegaard"><meta name="copyright" content="Odegaard"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="机器学习"><meta name="application-name" content="机器学习"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="机器学习"><meta property="og:url" content="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html"><meta property="og:site_name" content="coygOdegaard"><meta property="og:description" content="引言机器学习是什么 目前存在几种不同类型的学习算法。主要的两种类型被我们称之为监督学习和无监督学习。监督学习这个想法是指，我们将教计算机如何去完成任务，而在无监督学习中，我们打算让它自己进行学习。 监督学习监督学习指的就是我们给学习算法一个数据集，这个数据集由“正确答案”组成。在房价的例子中，我们给"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="Odegaard"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="引言机器学习是什么 目前存在几种不同类型的学习算法。主要的两种类型被我们称之为监督学习和无监督学习。监督学习这个想法是指，我们将教计算机如何去完成任务，而在无监督学习中，我们打算让它自己进行学习。 监督学习监督学习指的就是我们给学习算法一个数据集，这个数据集由“正确答案”组成。在房价的例子中，我们给"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Odegaard","link":"链接: ","source":"来源: coygOdegaard","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'coygOdegaard',
  title: '机器学习',
  postAI: '',
  pageFillDescription: '引言, 机器学习是什么, 监督学习, 无监督学习, 单变量线性回归, 模型表示, 代价函数, 代价函数的直观理解, 梯度下降, 梯度下降的直观理解, 梯度下降的线性回归, 多变量线性回归, 多维特征, 多变量梯度下降, 梯度下降法实践 1-特征缩放, 梯度下降法实践 2-学习率, 特征和多项式回归, 正规方程, 逻辑回归, 分类问题, 假说表示, 判定边界, 代价函数, 高级优化, 寻找决策边界, 构造多项式特征, 多类别分类：一对多, 正则化, 过拟合的问题, 代价函数, 正则化线性回归, 正则化的逻辑回归模型, 神经网络：表述, 非线性假设, 模型表示, 特征和直观理解, 多类分类, 神经网络的学习, 代价函数, 反向传播算法, 梯度检验, 随机初始化, 综合, 应用机器学习的建议, 决定下一步做什么, 评估一个假设, 模型选择和交叉验证集, 诊断偏差和方差, 正则化和偏差x2F方差, 学习曲线, 决定下一步做什么, 机器学习系统的设计, 误差分析, 类偏斜的误差度量, 查准率和查全率之间的权衡, 支持向量机, 优化目标, 大边界的直观理解, 大边界分类背后的数学（ 选修）, 核函数引言机器学习是什么目前存在几种不同类型的学习算法主要的两种类型被我们称之为监督学习和无监督学习监督学习这个想法是指我们将教计算机如何去完成任务而在无监督学习中我们打算让它自己进行学习监督学习监督学习指的就是我们给学习算法一个数据集这个数据集由正确答案组成在房价的例子中我们给了一系列房子的数据我们给定数据集中每个样本的正确价格即它们实际的售价然后运用学习算法算出更多的正确答案回归问题试着推测出一个连续值的结果下面的房子例子就是回归问题要推测的结果就是房子的价格回归这个词的意思是我们在试着推测出这一系列连续值属性分类问题分类指的是我们试着推测出离散的输出值或良性或恶性感觉就是判断给出的数据属于哪一类在这个例子中只有一个特征就是肿瘤的尺寸在其它一些机器学习问题中可能会遇到不止一种特征举个例子我们不仅知道肿瘤的尺寸还知道对应患者的年龄在其他机器学习问题中通常有更多的特征比如肿块密度肿瘤细胞尺寸的一致性和形状的一致性等等还有一些其他的特征之后会讲一个算法叫支持向量机里面有一个巧妙的数学技巧能让计算机处理无限多个特征回归问题和分类问题都属于监督学习其基本思想是数据集中的每个样本都有相应的正确答案再根据这些样本作出预测就像房子和肿瘤的例子中做的那样回归问题即通过回归来推出一个连续的输出分类问题其目标是推出一组离散的结果无监督学习不同于监督学习的数据的样子即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签所以我们已知数据集却不知如何处理也未告知每个数据点是什么任何信息都不知道只知道是一个数据集这个图是上面肿瘤的例子代表良性代表恶性在监督学习中有这种标志说明是什么情况但在无监督学习中没有标志只是数据针对数据集无监督学习能判断出数据有两个不同的聚集簇这是一个那是另一个二者不同是的无监督学习算法可能会把这些数据分成两个不同的簇这个就叫做聚类算法无监督学习就是我们没法提前告知算法一些信息就是这里是有一堆数据我不知道数据里面有什么我不知道谁是什么类型我甚至不知道人们有哪些不同的类型这些类型又是什么但你能自动地找到数据中的结构吗就是说你要自动地聚类那些个体到各个类我没法提前知道哪些是哪些因为我们没有给算法正确答案来回应数据集中的数据所以这就是无监督学习上面的都是聚类的例子聚类只是无监督学习的一种接下来介绍的鸡尾酒宴问题属于无监督学习中的盲源分离问题可能在一个这样的鸡尾酒宴中的两个人他俩同时都在说话假设现在是在个有些小的鸡尾酒宴中我们放两个麦克风在房间中因为这些麦克风在两个地方离说话人的距离不同每个麦克风记录下不同的声音虽然是同样的两个说话人听起来像是两份录音被叠加到一起或是被归结到一起产生了我们现在的这些录音另外这个算法还会区分出两个音频资源这两个可以合成或合并成之前的录音实际上鸡尾酒算法的第一个输出结果是第二个输出是这样第一个输出代表分离出的第一个声源第二个输出代表分离出的第二个声源这里的数字序列可能是对分离后信号的简化表示实际应用中输出是时间序列信号如音频波形每个数字可能代表某个时间点的信号强度或特征无需去深度思考单变量线性回归模型表示监督学习的第一个例子预测住房价格的我们要使用一个数据集数据集包含俄勒冈州波特兰市的住房价格在这里我要根据不同房屋尺寸所售出的价格画出我的数据集比方说如果你朋友的房子是平方尺大小你要告诉他们这房子能卖多少钱它被称作监督学习是因为对于每个数据来说我们给出了正确的答案即告诉我们根据我们的数据来说房子实际的价格是多少而且更具体来说这是一个回归问题回归一词指的是我们根据之前的数据预测出一个准确的输出值对于这个例子就是价格在监督学习中我们有一个数据集这个数据集被称训练集在整个课程中用小写的来表示训练样本的数目假如上面房子的回归问题的训练集如下表所示将训练集喂给我们的学习算法进而学习得到一个假设然后将我们要预测的房屋的尺寸作为输入变量输入给预测出该房屋的交易价格作为输出变量输出为结果表示的是一个函数由学习算法根据训练集输出输入是房屋尺寸大小输出的是房子价格的一种可能表达方式为因为只含有一个特征输入变量因此这样的问题叫作单变量线性回归问题代价函数有一个像这样的训练集代表了训练样本的数量比如而我们的假设函数也就是用来进行预测的函数是这样的线性函数形式接下来为我们的模型选择合适的参数和在房价问题这个例子中便是直线的斜率和在轴上的截距我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度模型所预测的值与训练集中实际值之间的差距下图中蓝线所指就是建模误差目标便是选择出可以使得建模误差的平方和能够最小的模型参数即使得代价函数最小绘制一个等高线图三个坐标分别为和和可以看出在三维空间中存在一个使得最小的点代价函数也被称作平方误差函数有时也被称为平方误差代价函数代价函数是解决回归问题最常用的手段代价函数的直观理解代价函数是用来干嘛的我们为什么要用它为了便于理解使代表的是训练集中的数据的参数是的参数是上图可以看出当时代价函数接下来时当时等于时对于每个的值都对应着一个假设函数的值或者一条直线并且根据每个不同的我们都可以得到一个不同的的值梯度下降梯度下降是一个用来求函数最小值的算法我们将使用梯度下降算法来求出代价函数的最小值梯度下降背后的思想是开始时我们随机选择一个参数的组合计算代价函数然后我们寻找下一个能让代价函数值下降最多的参数组合我们持续这么做直到到到一个局部最小值因为我们并没有尝试完所有的参数组合所以不能确定我们得到的局部最小值是否便是全局最小值选择不同的初始参数组合可能会找到不同的局部最小值这个算法时怎么工作的可以这样想想象一下你正站立在山的一点上在梯度下降算法中我们要做的就是旋转度看看我们的周围哪个方向可以最快下山来到山坡上我们站在山坡上的一点你看一下周围你会发现最佳的下山方向你再看看周围然后再一次想想我应该从什么方向下山然后你按照自己的判断又迈出一步重复上面的步骤从这个新的点你环顾四周并决定从什么方向将会最快下山然后又迈进了一小步并依此类推直到你接近局部最低点的位置批量梯度下降算法的公式为上面那行英语的意思是反复用这个公式直到收敛其中是学习率它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大在批量梯度下降中我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数符号的意思是赋值这是一个赋值运算符单独的代表的是比较运算符梯度下降中我们要同时更新和当和时会产生更新所以你将更新和记住要同时更新不能先更新一个再更新另一个先更新其中一个的话会导致接下来算出的微分项的值出现变换因为其中一个值变了梯度下降的直观理解梯度下降算法描述对赋值使得按梯度下降最快方向进行一直迭代下去最终得到局部最小值其中是学习率它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大求导的目的基本上可以说取这个红点的切线现在这条线有一个正斜率也就是说它有正导数因此我得到的新的更新后等于减去一个正数乘以如果太小或太大会出现什么情况如果太小了即我的学习速率太小结果就是只能这样像小宝宝一样一点点地挪动去努力接近最低点这样就需要很多步才能到达最低点所以如果太小的话可能会很慢因为它会一点点挪动它会需要很多步才能到达全局最低点如果太大那么梯度下降法可能会越过最低点甚至可能无法收敛下一次迭代又移动了一大步越过一次又越过一次一次次越过最低点直到你发现实际上离最低点越来越远所以如果太大它会导致无法收敛甚至发散假设将初始化在局部最低点因为它已经在一个局部的最优处或局部最低点结果是局部最优点的导数将等于零使得不再改变因此如果你的参数已经处于局部最低点那么梯度下降法更新其实什么都没做它不会改变参数的值这也解释了为什么即使学习速率保持不变时梯度下降也可以收敛到局部最低点在梯度下降法中当我们接近局部最低点时梯度下降法会自动采取更小的幅度这是因为当我们接近局部最低点时很显然在局部最低时导数等于零所以当我们接近局部最低时导数值会自动变得越来越小所以梯度下降将自动采取较小的幅度这就是梯度下降的做法所以实际上没有必要再另外减小可以用梯度下降算法来最小化任何代价函数不只是线性回归中的代价函数梯度下降的线性回归用梯度下降算法并将其应用于具体的拟合直线的线性回归算法里先计算微分项所以算法会被改写为不断重复直到收敛记住和要同时更新使用梯度下降算法是因为它更容易到达局部最小值而根据初始化的不同会得到不同的局部最优解但是事实证明用于线性回归的代价函数总是一个弓形样子的函数叫作凸函数这种函数没有局部最优解只有一个全局最优解一般来说初始化参数的时候都设为刚刚使用的算法有时也称为批量梯度下降批量梯度下降指的是在梯度下降的每一步中我们都用到了所有的训练样本在梯度下降中在计算微分求导项时我们需要进行求和运算所以在每一个单独的梯度下降中我们最终都要计算这样一个东西这个项需要对所有个训练样本求和多变量线性回归多维特征对房价模型增加更多的特征例如房间数楼层等构成一个含有多个变量的模型模型中的特征为这上面的公式是的原因是上面的是一列形状是里面的数据的形状是所以里面的公式是具体的情况要具体分析记住基本公式参数乘以变量多变量梯度下降在多变量线性回归中的代价函数这个代价函数是所有建模误差的平方和即其中多变量线性回归的批量梯度下降算法为即求导得跟前面单变量的公式没有什么大变化就是求导后需要计算的变多了计算代价函数的代码如下梯度下降法实践特征缩放在我们面对多维特征问题的时候我们要保证这些特征都具有相近的尺度这将帮助梯度下降算法更快地收敛以房价问题为例假设我们使用两个特征房屋的尺寸和房间的数量尺寸的值为平方英尺而房间数量的值则是以两个参数分别为横纵坐标绘制代价函数的等高线图在这里先忽略能看出图像会显得很扁梯度下降算法需要非常多次的迭代才能收敛解决的方法是尝试将所有特征的尺度都尽量缩放到到之间这也是在做特征缩放时的通常目的但其实并不严格要求必须是到在这些附近都可以重点是将范围靠近到所以如果有一个特征也就是变量的范围是到的话得对其进行扩展一般在到到都是可以的除了将特征除以它的最大值外还可以进行一种叫作均值归一化的工作包括将原来的变量值减去平均值除以最大值最小值一般用这个就足够了其中是平均值是标准差梯度下降法实践学习率如何确定梯度下降算法在正常工作画图表梯度下降算法的每次迭代受到学习率的影响如果学习率过小则达到收敛所需的迭代次数会非常高如果学习率过大每次迭代可能不会减小代价函数可能会越过局部最小值导致无法收敛通常可以考虑尝试些学习率特征和多项式回归多项式回归可以使用线性回归的方式来拟合非常复杂的函数或者是非线性函数以预测房价模型为例在线性回归模型中你可以选择提供的特征作为特征也可以选择自己创建一个新的特征哎下面的例子中题目给了临街宽度和纵深两个特征我们也可以自己创建一个特征面积这样子可以简化线性回归模型得到一个更好的模型线性回归并不适用于所有数据有时我们需要曲线来适应我们的数据二次方或者三次方模型另外我们可以令从而将模型转化为线性回归模型由于次方的存在导致参数范围被扩大了很多所以在运行梯度下降算法前必须进行特征缩放除了上面给出的这一种还有一种是开平方通过不同的参数形式最后的曲线也会有所不同正规方程到目前为止我们都在使用梯度下降算法但是对于某些线性回归问题正规方程方法是更好的解决方案如运用正规方程方法求解参数对于那些不可逆的矩阵通常是因为特征之间不独立如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征也有可能是特征数量大于训练集的数量正规方程方法是不能用的只要特征变量的数目并不大标准方程是一个很好的计算参数的替代方法具体地说只要特征变量数量小于一万通常使用标准方程法而不使用梯度下降法等价于注意这里返回的不是一个数而是一个元组逻辑回归分类问题在分类问题中要预测的变量是离散的值在分类问题中我们尝试预测的是结果是否属于某一个类例如正确或错误从二元的分类问题开始我们将因变量可能属于的两个类分别称为负向类和正向类则因变量或其中表示负向类表示正向类逻辑回归算法的性质是它的输出值永远在到之间逻辑回归算法是一个分类算法适用于取离散的值的情况下假说表示为什么线性回归算法不适用于分类问题根据线性回归模型我们只能预测连续的值然而对于分类问题例子是肿瘤分类我们需要输出或我们可以预测当时预测当时预测没有极端数据出现的时候使用线性回归算法看着也可以但一旦极端数据出现整体的判断标准就会被破坏有极端数据出现所以线性回归模型并不适用于分类问题逻辑回归模型的假设是其中代表特征向量代表逻辑函数是一个常用的逻辑函数形函数公式为这个就是线性回归模型的结果这里的参数向量是行列的的数据是一列一列的所以逻辑回归模型是对线性回归模型的值进行处理的意思是对于给定的输入变量根据选择的参数计算输出变量的可能性如果对于给定的通过已经确定的参数计算得出则表示有的几率为正向类相应地为负向类的几率为逻辑回归的本质逻辑回归是一种线性分类模型它通过一个线性方程例如将输入特征如测试和测试的结果映射到一个概率值通过函数决策边界即区分接受抛弃的阈值是线性的比如一条直线在二维特征空间中判定边界根据逻辑回归模型的这个图我们知道当时时时又即时预测时预测对于上面那个模型我们可以很明显地看出是一条直线将预测结果分成两部分又两个例子可以看出我们要根据分界线的形状来判断我们应该使用的分界线函数是什么代价函数对于线性回归模型定义的代价函数是所有模型误差的平方和要是将逻辑回归模型的函数代入到这个代价函数中得到的代价函数将是一个非凸函数这意味着代价函数会有许多局部最小值这将影响梯度下降算法寻找全局最小值定义代价函数为的取值范围在这样构建的函数的特点是当实际的且也为时误差为当但不为时误差随着变小而变大当实际的且也为时代价为当但不为时误差随着的变大而变大将函数进行简化就是用一个表达式表达出来如下代入代价函数为函数在上面定义了梯度下降算法的公式和前面的一样记住同时更新所有参数通过观察梯度下降算法的式子可以发现这个式子和之前线性回归的梯度下降算法的式子是一样的但的式子是不同的特征缩放的技巧也适用于逻辑回归高级优化共轭梯度法变尺度法和限制变尺度法就是一些更高级的优化算法它们需要有一种方法来计算以及需要一种方法计算导数项然后使用比梯度下降更复杂的算法来最小化代价函数这三种算法的具体细节可以不用取探究因为过于复杂使用这其中任何一个算法通常不需要手动选择学习率所以对于这些算法的一种思路是给出计算导数项和代价函数的方法你可以认为算法有一个智能的内部循环而且事实上他们确实有一个智能的内部循环称为线性搜索算法它可以自动尝试不同的学习速率并自动选择一个好的学习速率因此它甚至可以为每次迭代选择不同的学习速率最好不要使用这些算法除非你是数值计算方面的专家如何使用这些算法这些算法适合在很大的机器学习问题中使用在中利用的是中的函数这是一个使用截断牛顿法寻找局部最小值的优化函数特别适用于有界约束的优化问题寻找决策边界所以中的寻找决策边界会除以第三个参数值构造多项式特征由上图可知其中没有线性决策界限来良好的分开两类数据原数据有两个特征和可以明显看出就这两个特征无法较好拟合这些数据所以要构造从原始特征的多项式中得到的特征即通过数学变换将原始特征和扩展为一组新特征这些新特征是原始特征的高阶多项式组合例如平方平方立方等然后在这些新特征上训练逻辑回归模型为什么能解决非线性问题尽管逻辑回归本身是线性的但通过添加非线性特征如平方项或交互项模型在扩展后的高维特征空间中学习到的决策边界仍然是线性的但这个边界在原始特征空间中会呈现为曲线椭圆或其他非线性形状这相当于给模型添加了非线性能力而不改变其核心算法首先要选择阶数阶数决定了多项式的复杂性从二阶开始通常足够处理大多数非线性问题然后根据模型性能调整平方项捕捉单个测试的非线性效应交互项捕捉两个测试的联合效应由于选择了高阶数的模型为了避免过拟合通常还要进行正则化操作多类别分类一对多如何使用逻辑回归来解决多类别分类问题之前的二元分类问题的图和现在的多类分类问题的图用种不同的符号来代表个类别问题就是给出个类型的数据集如何得到一个学习算法来进行分类呢面对二元分类问题可以使用逻辑回归也可以将数据集一分为二为正类和负类而一对多的分类思想我们可以将其用在多类分类问题上这个方法也被称为一对余方法现在我们有一个训练集好比上图表示的有个类别我们用三角形表示方框表示叉叉表示使用一个训练集将三元分类问题转化为三个二元分类问题先从用三角形代表的类别开始实际上我们可以创建一个新的伪训练集类型和类型定为负类类型设定为正类我们创建一个新的训练集如下图所示的那样我们要拟合出一个合适的分类器这里的三角形是正样本而圆形代表负样本可以这样想设置三角形的值为圆形的值为下面可以训练一个标准的逻辑回归分类器这样就得到一个边界选择出哪一个分类器是可信度最高效果最好的那么就可认为得到一个正确的分类无论值是多少我们都有最高的概率值我们预测就是那个值最后我们得到一系列的模型简记为其中正则化过拟合的问题就是过于强调拟合原始数据第一个模型是一个线性模型欠拟合不能很好地适应训练集第三个模型是一个四次方的模型过于强调拟合原始数据而丢失了算法的本质预测新数据可以看出若给出一个新的值使之预测它将表现的很差是过拟合虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好分类问题中也存在这样的问题以多项式理解的次数越高拟合的越好但相应的预测的能力就可能变差如何处理过拟合问题丢弃一些不能帮助我们正确预测的特征可以是手工选择保留哪些特征或者使用一些模型选择的算法来帮忙例如正则化保留所有的特征但是减少参数的大小代价函数在上面过拟合的回归问题中有以下模型正是高次项导致了过拟合的产生所以如果能让这些高次项的系数接近于的话那就能很好的拟合了所以我们要做的就是在一定程度上减小这些参数的值这就是正则化的基本方法我们决定要减少和的大小我们要做的便是修改代价函数在其中和设置一点惩罚这样做的话在尝试最小化代价时也会将这个惩罚纳入考虑中并最终导致选择较小一些的和惩罚就是在代价函数中使和的占比变高使得在最小化代价函数时也会更多地考虑这两个参数修改后的代价函数假如有非常多的特征我们并不知道其中哪些特征要惩罚那么就对所有的特征进行惩罚并且让代价函数最优化的软件来选择这些惩罚的程度这样的结果是得到了一个较为简单的能防止过拟合问题的假设其中又称为正则化参数根据惯例我们不对进行惩罚经过正则化处理的模型与原模型的可能对比如下图所示如果选择的正则化参数过大则会把所有的参数都最小化了导致模型变成也就是上图中红色直线所示的情况造成欠拟合正则化线性回归正则化线性回归的代价函数是由于没有进行正则化所以梯度下降算法将会分成两种情况对第二个式子进行调整可以得到可以看出正则化线性回归的梯度下降算法的变化在于每次都在原有算法更新规则的基础上令值减少了一个额外的值利用正规方程来求解正则化线性回归模型图中的矩阵尺寸为因为不算还有个特征正则化的逻辑回归模型这是正则化的逻辑回归的代价函数给代价函数增加一个正则化的表达式得到代价函数这边的函数是函数不参与其中的任何一个正则化接下来的课程中我们将学习一个非常强大的非线性分类器无论是线性回归问题还是逻辑回归问题都可以构造多项式来解决你将逐渐发现还有更强大的非线性分类器可以用来解决多项式回归问题神经网络表述非线性假设无论是线性回归还是逻辑回归都有这样一个缺点即当特征太多时计算的负荷会非常大使用非线性的多项式项能够帮助我们建立更好的分类模型但相应的我们要计算的特征数会大大增多普通的逻辑回归模型不能有效地处理这么多的特征这时候就需要神经网络模型表示神经网络模型建立在很多神经元之上每一个神经元又是一个个学习模型这些神经元也叫激活单元采纳一些特征作为输出并且根据本身的模型提供一个输出以逻辑回归模型作为学习模型的神经元示例解读上面的黄圈看作是神经元左边的蓝圈和黄圈的连线看作是输入树突黄圈右边的线看作是输出轴突通过树突传递一些信息然后神经元做一些计算然后通过轴突输出计算结果这个图表表示的是对的计算而是函数是输入结点额外的结点被称为偏置单位因为总是等于可画可不画根据具体情况来在神经网络中参数又可被称为权重上面的一个小黄圈代表一个单一的神经元而神经网络是不同的神经元组合在一起的集合第一层叫作输入层在这一层输入特征项最后一层第层叫作输出层因为这一层的神经元输出假设的最终计算结果中间的一层称作隐藏层神经网络中可以有不止一个隐藏层非输出层和输入层的都叫做隐藏层在隐藏层出现的蓝色圈被称作偏置单位它的值永远是下面的图有个输入单元和个隐藏单元每一个都是由上一层所有的和每一个所对应的参数决定的这样从左到右的算法称为前向传播算法不会等于因为这边要转置应该是因为是一行一行地输入数据的要注意偏置单位的添加如果我们暂时只看第二层和第三层的话可以发现其实神经网络就像是只不过我们把中的输入向量变成了中间层的即特征项是作为输入的函数来学习的所以在神经网络中它没有使用输入特征来训练逻辑回归而是自己根据来训练逻辑回归所以如果在中选择了不同的参数那就可以学习到比较复杂的特征就可以得到一个更好的假设比使用原始输入时得到的假设更好构造多项式特征这里的特征也是通过学习模型得出来的神经网络中神经元相互连接的方式称为神经网络的架构特征和直观理解从本质上讲神经网络能够通过学习得出其自身的一系列特征在预设的网络架构和激活函数框架下通过训练数据动态调整权重参数使用梯度下降等优化算法调整使模型逼近目标函数在普通的逻辑回归中我们被限制为使用数据中的原始特征我们虽然可以使用一些二项式项来组合这些特征但是我们仍然受到这些原始特征的限制在神经网络中原始特征只是输入层在我们上面三层的神经网络例子中第三层也就是输出层做出的预测利用的是第二层的特征而非输入层中的原始特征我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征神经网络中单层神经元无中间层的计算可用来表示逻辑运算比如逻辑与逻辑或逻辑与用下面的这样一个神经网络表示函数其中我们的输出函数即为为什么要这样设置参数根据的图像和真值表得出再根据同样的步骤设计函数三个权重分别为它与函数的区别就是参数的取值不同当输入特征为布尔值或时我们可以用一个单一的激活层可以作为二元逻辑运算符为了表示不同的运算符我们只需要选择不同的权重即可我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算实现功能输入的两个值必须一样均为或均为即首先构造一个能表达部分的神经元交并非然后将表示的神经元和表示的神经元以及表示的神经元进行组合多类分类如果我们要训练一个神经网络算法来识别路人汽车摩托车和卡车在输出层我们应该有个值例如第一个值为或用于预测是否是行人第二个值用于判断是否为汽车输入向量有三个维度两个中间层输出层个神经元分别用来表示类也就是每一个数据在输出层都会出现且中仅有一个为表示当前类下面是该神经网络的可能结构示例我们不用代表行人代表车这种形式而是构造四个分类器输出或根据哪个位置是来判断是什么神经网络的学习代价函数假设神经网络的训练样本有个每个包含一组输入和一组输出信号神经网络结构的总层数这是小第层的单元个数也就是神经元的数量不包括层的偏差单元代表最后一层中处理单元的个数二元分类问题只有一个输出单元所以只有一个输出结果多元分类问题也就是类分类问题会有个输出单元输出是一个维向量先来看一下逻辑回归问题中的代价函数在逻辑回归中我们只有一个输出变量又称标量也就是只有一个逻辑输出单元也只有一个因变量但在神经网络中会有个逻辑输出单元是一个维向量代表的是第个输出也就是它选择了输出向量中的第个元素首先要计算从到的每一个逻辑回归算法的代价函数的和最后的那个正则化的求和项它是将所有的参数除了的也就是偏差单位都加起来上面的和应该反了这样的根据这个图可以知道参数的行号是从开始计数的所以上面的是从开始的正则化的那一项只是排除了每一层后每一层的矩阵的和最里层的循环循环所有的行由层的激活单元数决定循环则循环所有的列由该层层的激活单元数所决定上面那个公式的意思就是与真实值之间的距离为每个样本每个类输出的加和对参数进行的项处理所有参数的平方和反向传播算法就是让代价函数最小化的算法因为是从最后往前算误差的所以叫作反向传播算法之前的是从第一层开始正向一层一层进行计算直到最后一层的所以是正向传播算法为了计算偏导数项我们需要采用一种反向传播算法也就是首先计算最后一层的误差然后再一层一层反向求出各层的误差直到倒数第二层以一个例子来说明反向传播算法参数是这样的假设我们的训练集只有一个实例我们的神经网络是一个四层的神经网络其中先利用前向传播算法计算一下输出结果接下来计算导数项代表第层的第个结点的误差由于现在用来举例的只有一个样本所以误差可以写为向量形式是当算出最后一层的误差后就向前计算前面几层的误差代表的是两个向量的对应元素相乘没有因为第一层是输入层所以不会存在误差一个单元造成的误差是这个单元对下一层的每一个单元造成的误差的总和所以参数矩阵要转置求这个单元造成的误差和可以求出偏差单位的误差但在计算的时候可以不加造成的影响不大假设即我们不做任何正则化处理时有上面式子中上下标的含义代表目前所计算的是第几层代表目前计算层中的激活单元的下标代表下一层层中误差单元的下标是受到权重矩阵中第行影响的下一层中的误差单元的下标假设有个训练样本用表示误差矩阵第层的第个激活单元受到第个参数影响而导致的误差公式里的那个是乘以后面和的梯度检验当我们对一个较为复杂的模型例如神经网络使用梯度下降算法时可能会存在一些不容易察觉的错误意味着虽然代价看上去在不断减小但最终的结果可能并不是最优解为了避免这样的问题我们采取一种叫做梯度的数值检验方法这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度即对于某个特定的我们计算出在处和的代价值是一个非常小的值通常选取不要取太小不然会出现数值问题然后求两个代价的平均用以估计在处的代价值用数值的方法计算近似的导数下面那张图的是一个实数更普遍的情况是一个维向量它可能是神经网络参数等的展开形式最后我们还需要对通过反向传播方法计算出的偏导数进行检验根据反向传播算法计算出来的偏导数存储在矩阵中然后将其与上面的数值计算的方法计算出来的近似的梯度值进行比较如果误差在几位小数之内就认为反向传播算法的实现是正确的然后在接下来的训练过程中都不再使用这个验证程序因为它的计算量很大而反向传播是一种更为简单的计算方法步骤就是反向传播算出来的导数值是数值计算算出来的近似梯度值用数值方法计算导数是用来确定反向传播实现是否正确的方法但是不止可以用来验证反向传播也可以用来验证类似的复杂模型的梯度下降算法随机初始化任何优化算法都需要一些初始的参数到目前为止我们都是初始所有参数为这样的初始方法对于逻辑回归来说是可行的但是对于神经网络来说是不可行的如果我们令所有的初始参数都为这将意味着我们第二层的所有激活单元都会有相同的值算出来的误差值也都是相同的值之后进行梯度下降后的值也都相同同理如果我们初始所有的参数都为一个非的数结果也是一样的为了解决这个问题神经网络变量的初始化方式采用随机初始化通常初始参数为正负之间接近于的随机值然后进行反向传播执行梯度检查使用梯度下降或者其它优化算法综合应用机器学习的建议决定下一步做什么当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差我们下一步可以做什么获得更多的训练实例通常是有效的但代价较大下面的方法也可能有效可考虑先采用下面的几种方法尝试减少特征的数量尝试获得更多的特征尝试增加多项式特征尝试减少正则化程度尝试增加正则化程度我们不应该随机选择上面的某种方法来改进我们的算法而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的诊断法的意思是这是一种测试法你通过执行这种测试能够深入了解某种算法到底是否有用这通常也能够告诉你要想改进一种算法的效果什么样的尝试才是有意义的评估一个假设如何判断一个假设函数是过拟合的呢对于特征变量只有一个的简单例子可以直接对假设函数进行画图但对于特征变量不止一个的这种一般情况还有像有很多特征变量的问题想要通过画出假设函数来进行观察就会变得很难甚至是不可能实现为了检验算法是否过拟合我们将数据分成训练集和测试集通常用的数据作为训练集用剩下的数据作为测试集很重要的一点是训练集和测试集均要含有各种类型的数据通常我们要对数据进行洗牌然后再分成训练集和测试集在通过训练集让我们的模型学习得出其参数后对测试集运用该模型我们有两种方式计算误差对于线性回归模型我们利用测试集数据计算代价函数对于逻辑回归模型我们除了可以利用测试数据集来计算代价函数外还可以计算误分类的比率感觉就是准确率也就是对每一个测试集实例计算然后对结果求平均模型选择和交叉验证集假设我们要在个不同次数的二项式模型之间进行选择虽然越高次数的多项式模型越能够适应我们的训练数据集但是适应训练数据集并不代表着能推广至一般情况我们应该选择一个更能适应一般情况的模型我们需要使用交叉验证集来帮助选择模型即使用的数据作为训练集使用的数据作为交叉验证集使用的数据作为测试集模型选择的方法为使用训练集训练出个模型用个模型分别对交叉验证集计算得出交叉验证误差代价函数的值选取代价函数值最小的模型用步骤中选出的模型对测试集计算得出推广误差代价函数的值诊断偏差和方差当一个学习算法的表现不理想时多半是出现两种情况要么是偏差比较大要么是方差比较大换句话说出现的情况要么是欠拟合要么是过拟合问题那么这两种情况哪个和偏差有关哪个和方差有关或者是不是和两个都有关搞清楚这一点就能判断出现的情况是这两种情况中的哪一种高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析对于训练集当较小时模型拟合程度更低误差较大随着的增长拟合程度提高误差减小对于交叉验证集当较小时模型拟合程度低误差较大但是随着的增长误差呈现先减小后增大的趋势转折点是我们的模型开始过拟合训练数据集的时候交叉验证集误差较大如何判断是方差还是偏差呢根据上面的图表可以知道训练集误差和交叉验证集误差近似时偏差欠拟合交叉验证集误差远大于训练集误差时方差过拟合正则化和偏差方差在训练模型的过程中一般会使用一些正则化方法来防止过拟合但是正则化的程度可能会太高或太小了即我们在选择的值时也需要思考与刚才选择多项式模型次数类似的问题选择一系列的想要测试的值通常是之间的呈现倍关系的值如共个同样把数据分为训练集交叉验证集和测试集选择的方法为使用训练集训练出个不同程度正则化的模型用个模型分别对交叉验证集计算的出交叉验证误差选择得出交叉验证误差最小的模型运用步骤中选出模型对测试集计算得出推广误差我们也可以同时将训练集和交叉验证集模型的代价函数误差与的值绘制在一张图表上在训练时代价函数是有加上正则项的而在后面计算训练集和交叉验证集的误差时是没有加上正则项的因为越大会导致正则项在训练时的代价函数中的比例越大那么在后面不算上正则项时代价函数就会比较大当较小时训练集误差较小过拟合而交叉验证集误差较大这对应着高偏差问题随着的增加训练集误差不断增加欠拟合而交叉验证集误差则是先减小后增加这对应着高方差问题学习曲线可以使用学习曲线来判断某一个学习算法是否处于偏差方差问题学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量的函数绘制的图表所以一般都是一个常数但我们需要自行对进行取值比如说取等然后绘制出曲线即如果我们有行数据我们从行数据开始逐渐学习更多行的数据绘制学习曲线先绘制出然后再画出当训练较少行数据的时候训练的模型将能够非常完美地适应较少的训练数据但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据在训练数据很少的情况下即使使用了正则化拟合的效果仍然会很好随着训练集样本的增加平均训练误差是逐渐增大的当学习算法处于高偏差欠拟合情形时学习曲线如下作为例子用一条直线来适应下面的数据可以看出无论训练集有多么大误差都不会有太大改观也就是说在高偏差欠拟合的情况下增加数据到训练集不一定能有帮助当学习算法处于高方差情形时假设使用一个非常高次的多项式模型并且正则化非常小可以看出当交叉验证集误差远大于训练集误差时往训练集增加更多数据可以提高模型的效果虽然随着训练样本的增多会越来越大因为训练样本越多时就越难与训练数据拟合得很好但总体来说训练集误差还是很小因为函数对数据过拟合所以交叉验证集误差会一直都很大即便选择了一个比较合适得训练集样本数所以交叉验证集和训练集误差之间始终会有一段很大的差距但如果继续增大样本数的话可以发现这两条线在相互靠近增大样本数也就是说在高方差过拟合的情况下增加更多数据到训练集可能可以提高算法效果决定下一步做什么获得更多的训练实例解决高方差尝试减少特征的数量解决高方差尝试获得更多的特征解决高偏差尝试增加多项式特征解决高偏差尝试减少正则化程度解决高偏差尝试增加正则化程度解决高方差神经网络的方差和偏差使用较小的神经网络类似于参数较少的情况容易导致高偏差和欠拟合但计算代价较小使用较大的神经网络类似于参数较多的情况容易导致高方差和过拟合虽然计算代价比较大但是可以通过正则化手段来调整而更加适应数据通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好对于神经网络中的隐藏层的层数的选择通常从一层开始逐渐增加层数为了更好地作选择可以把数据分为训练集交叉验证集和测试集针对不同隐藏层层数的神经网络训练神经网络然后选择交叉验证集代价最小的神经网络机器学习系统的设计以一个垃圾邮件分类器算法为例进行讨论为了解决这样一个问题我们首先要做的决定是如何选择并表达特征向量我们可以选择一个由个最常出现在垃圾邮件中的词所构成的列表根据这些词是否有在邮件中出现来获得我们的特征向量出现为不出现为尺寸为为了构建这个分类器算法我们可以做很多事例如收集更多的数据让我们有更多的垃圾邮件和非垃圾邮件的样本基于邮件的路由信息开发一系列复杂的特征基于邮件的正文信息开发一系列复杂的特征包括考虑截词的处理为探测刻意的拼写错误把写成开发复杂的算法在上面这些选项中非常难决定应该在哪一项上花费时间和精力作出明智的选择比随着感觉走要更好当我们使用机器学习时总是可以头脑风暴一下想出一堆方法来试试误差分析如果你准备研究机器学习的东西或者构造机器学习应用程序最好的实践方法不是建立一个非常复杂的系统拥有多么复杂的变量而是构建一个简单的算法这样你可以很快地实现它研究机器学习的问题时先很快地把结果搞出来即便运行得不完美但是也把它运行一遍最后通过交叉验证来检验数据一旦做完你可以画出学习曲线通过画出学习曲线以及检验误差来找出你的算法是否有高偏差和高方差的问题或者别的问题在这样分析之后再来决定用更多的数据训练或者加入更多的特征变量是否有用因为我们并不能提前知道是否需要复杂的特征变量或者是否需要更多的数据还是别的什么提前知道应该做什么是非常难的因为缺少证据缺少学习曲线因此很难知道应该把时间花在什么地方来提高算法的表现但是当实践一个非常简单即便不完美的方法时就可以通过画出学习曲线来做出进一步的选择除了画学习曲线外还有一个方法就是进行误差分析例如当我们在构造垃圾邮件分类器时我会看一看我的交叉验证数据集然后亲自看一看哪些邮件被算法错误地分类因此通过这些被算法错误分类的垃圾邮件与非垃圾邮件你可以发现某些系统性的规律什么类型的邮件总是被错误分类经常地这样做之后这个过程能启发你构造新的特征变量或者告诉你现在这个系统的短处然后启发你如何去提高它具体一点就是检验交叉验证集中我们的算法产生错误预测的所有邮件看是否能将这些邮件按照类分组例如医药品垃圾邮件仿冒品垃圾邮件或者密码窃取邮件等然后看分类器对哪一组邮件的预测误差最大并着手优化思考怎样能改进分类器例如发现是否缺少某些特征记下这些特征出现的次数误差分析并不总能帮助我们判断应该采取怎样的行动有时我们需要尝试不同的模型然后进行比较在模型比较时用数值来判断哪一个模型更好更有效通常我们是看交叉验证集的误差因此在构造学习算法的时候总是会去尝试很多新的想法实现出很多版本的学习算法如果每一次实践新想法的时候都要手动地检测这些例子去看看是表现差还是表现好那么这很难让你做出决定但是通过一个量化的数值评估哪些代码里自己计算的准确度你可以看看这个数字误差是变大还是变小了你可以通过它更快地实践你的新想法它基本上非常直观地告诉你你的想法是提高了算法表现还是让它变得更坏这会大大提高你实践算法时的速度在交叉验证集上来实施误差分析类偏斜的误差度量误差度量值设定某个实数来评估你的学习算法并衡量它的表现类偏斜情况表现为我们的训练集中有非常多的同一种类的实例只有很少或没有其他类的实例例如我们希望用算法来预测癌症是否是恶性的在我们的训练集中只有的实例是恶性肿瘤假设我们编写一个非学习而来的算法在所有情况下都预测肿瘤是良性的那么误差只有然而我们通过训练而得到的神经网络算法却有的误差这时误差的大小是不能视为评判算法效果的依据的我们将算法预测的结果分成四种情况正确肯定预测为真实际为真正确否定预测为假实际为假错误肯定预测为真实际为假错误否定预测为假实际为真查准率例在所有我们预测有恶性肿瘤的病人中实际上有恶性肿瘤的病人的百分比越高越好查全率例在所有实际上有恶性肿瘤的病人中成功预测有恶性肿瘤的病人的百分比越高越好对于刚才那个总是预测病人肿瘤为良性的算法其查全率是查准率和查全率之间的权衡假设我们的算法输出的结果在之间我们使用阀值来预测真和假如果我们希望只在非常确信的情况下预测为真肿瘤为恶性即我们希望更高的查准率我们可以使用比更大的阀值如这样做我们会减少错误预测病人为恶性肿瘤的情况同时却会增加未能成功预测肿瘤为恶性的情况如果我们希望提高查全率尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查诊断我们可以使用比更小的阀值如可以将不同阀值情况下查全率与查准率的关系绘制成图表曲线的形状根据数据的不同而不同是临界值的意思查准率召回率曲线会有多种形状有一个帮助我们选择这个阀值的方法一种方法是计算值其计算公式为我们选择使得值最高的阀值支持向量机优化目标在监督学习中许多学习算法的性能都非常类似因此重要的不是该选择使用学习算法还是学习算法而更重要的是应用这些算法时表现情况通常依赖于你的水平比如你为学习算法所设计的特征量的选择以及如何选择正则化参数诸如此类的事与逻辑回归和神经网络相比支持向量机或者简称在学习复杂的非线性方程时提供了一种更为清晰更加强大的方式从逻辑回归开始来展示如何一点一点修改来得到本质上的支持向量机逻辑回归中一个训练样本所对应代价函数表达式当时只有第一项起了作用对第一项进行修改得出中将要使用的代价函数粉色线由两段直线组成暂时不用考虑左侧直线的斜率因为那个不重要这个将要使用的代价函数是在的前提条件下的新的代价函数叫作当时新的代价函数叫作然后接下来开始构造支持向量机这是逻辑回归中所用到的代价函数对于支持向量机来说要将里面的两项进行替换但实际上对于支持向量机来说代价函数的书写会有所不同首先要去掉这一项去掉之后也会得出同样的最优值因为仅是个常量因此你知道在这个最小化问题中无论前面是否有这一项最终我所得到的最优值都是一样的这里我的意思是先给你举一个实例假定有一最小化问题即要求当取得最小值时的值这时最小值为当时取得最小值现在如果我们想要将这个目标函数乘上常数这里我的最小化问题就变成了求使得最小的值然而使得这里最小的值仍为因此将一些常数乘以你的最小化项这并不会改变最小化该方程时得到值因此这里我所做的是删去常量也相同的我将目标函数乘上一个常量并不会改变取得最小值时的值用来表示不包括正则项的部分也就是训练样本的代价用来表示不包括的正则项这就相当于我们想要最小化加上正则化参数乘以我们所做的是通过设置不同正则参数达到优化目的这样我们就能够权衡对应的项即最小化是使得训练样本拟合的更好还是保证正则参数足够小也即是对于项而言但对于支持向量机按照惯例我们将使用一个不同的参数替换这里使用的来权衡这两项就是第一项和第二项我们依照惯例使用一个不同的参数称为同时改为优化目标因此在逻辑回归中如果给定一个非常大的值意味着给予更大的权重而这里就对应于将设定为非常小的值那么相应的将会给比给更大的权重因此这只是一种不同的方式来控制这种权衡或者一种不同的方法即用参数来决定是更关心第一项的优化还是更关心第二项的优化当然你也可以把这里的参数考虑成同所扮演的角色相同并且这两个方程或这两个表达式并不相同因为但是也并不全是这样如果当时这两个优化目标应当得到相同的值相同的最优值因此这就得到了在支持向量机中我们的整个优化目标函数然后最小化这个目标函数得到学习到的参数有别于逻辑回归输出的概率在这里当最小化代价函数得到参数时支持向量机会直接预测的值等于还是等于大边界的直观理解人们有时将支持向量机看作是大间距分类器这是支持向量机模型的代价函数左边时关于的代价函数此函数用于正样本而右边是关于的代价函数横轴表示现在让我们考虑一下最小化这些代价函数的必要条件是什么如果你有一个正样本则只有在时代价函数才等于换句话说如果你有一个正样本我们会希望反之如果它只有在的区间里函数值为事实上可以放入逻辑回归问题中理解如果有一个正样本则其实我们仅仅要求大于等于就能将该样本恰当分出这是因为如果大的话我们的模型代价函数值为类似地如果有一个负样本则仅需要就会将负例正确分离但是支持向量机的要求更高不仅仅要能正确分开输入的样本即不仅仅要求我们需要的是比值大很多比如大于等于我也想分离负例时比小很多比如我希望它小于等于这就相当于在支持向量机中嵌入了一个额外的安全因子或者说安全的间距因子在支持向量机中这个因子会导致什么结果接下来考虑一个特例将这个常数设置成一个非常大的值比如假设的值为或者其它非常大的数然后来观察支持向量机会给出什么结果如果非常大则最小化代价函数的时候我们将会很希望找到一个使第一项为的最优解很大不就相当于斜率很大那当位于和之间时那个的值就会很大因此让我们尝试在代价项的第一项为的情形下理解该优化问题输入一个训练样本标签为想令第一项为需要做的是找到一个使得类似地对于一个训练样本标签为为了使函数的值为我们需要因此现在考虑优化问题选择参数使得第一项等于因此这个函数的第一项为因此是乘以加上二分之一乘以第二项这里第一项是乘以因此可以将其删去这将遵从以下的约束如果是等于的如果样本是一个负样本具体而言如果你考察这样一个数据集其中有正样本也有负样本可以看到这个数据集是线性可分的粉色和绿色的决策边界仅仅是勉强分开这些决策边界看起来都不是特别好的选择支持向量机将会选择这个黑色的决策边界黑线看起来是更稳健的决策界在分离正样本和负样本上它显得的更好数学上来讲这条黑线有更大的距离这个距离叫做间距当画出这两条额外的蓝线我们看到黑色的决策界和训练样本之间有更大的最短距离然而粉线和蓝线离训练样本就非常近在分离样本的时候就会比黑线表现差因此这个距离叫做支持向量机的间距而这是支持向量机具有鲁棒性的原因因为它努力用一个最大间距来分离样本因此支持向量机有时被称为大间距分类器鲁棒性指模型在面对数据中的噪声异常值干扰或环境变化时仍能保持稳定预测性能的能力也就是模型的抗干扰和抗压能力支持向量机模型的做法即努力将正样本和负样本用最大的间距分开在上面将这个大间距分类器中的正则化因子常数设置的非常大因此对这样的一个数据集也许我们将选择这样的决策界从而最大间距地分离开正样本和负样本那么在让代价函数最小化的过程中我们希望找出在和两种情况下都使得代价函数中左边的这一项尽量为零的参数如果我们找到了这样的参数则我们的最小化问题便转变成但是当你使用大间距分类器的时候你的学习算法会受异常点的影响比如我们加入一个额外的正样本在这里如果加了这个样本为了将样本用最大间距分开也许最终会得到一条类似粉色这样的决策界仅仅基于一个异常值仅仅基于一个样本就将我的决策界从这条黑线变到这条粉线这实在是不明智的而如果正则化参数设置的非常大这事实上正是支持向量机将会做的但如果将设置的不要太大则最终会得到这条黑线当不是非常非常大的时候它可以忽略掉一些异常点的影响得到更好的决策界数据如果不是线性可分的如果你在这里有一些正样本或者你在这里有一些负样本则支持向量机也会将它们恰当分开因此大间距分类器的描述仅仅是从直观上给出了正则化参数非常大的情形同时要提醒你的作用类似于是我们之前使用过的正则化参数因此较大时相当于较小可能会导致过拟合高方差较小时相当于较大可能会导致低拟合高偏差大边界分类背后的数学选修向量内积有两个向量和两个都是二维向量也叫做向量和之间的内积除了这种计算方式外还有一种计算方式先把这两个向量画出来向量即在横轴上取值为某个而在纵轴上高度是某个作为的第二个分量向量的范数向量也按同样的步骤画出来表示的范数即的长度即向量的欧几里得长度这是向量的长度它是一个实数计算内积将向量投影到向量上做一个直角投影或者说一个度投影将其投影到上接下来度量这条红线的长度称这条红线的长度为因此就是长度或者说是向量投影到向量上的量公式是因为因此如果你将和交换位置将投影到上而不是将投影到上然后做同样地计算只是把和的位置交换一下你事实上可以得到同样的结果申明一点在这个等式中的范数是一个实数也是一个实数因此就是两个实数正常相乘事实上是有符号的即它可能是正值也可能是负值这种情况下的就是负值这是支持向量机模型中的目标函数为了让它更容易分析忽略掉截距令将特征数置为因此仅有两个特征现在来看一下支持向量机的优化目标函数这个式子可以写作括号里面的这一项是向量的范数或者说是向量的长度因此支持向量机做的全部事情就是极小化参数向量范数的平方或者说长度的平方深入理解的含义和就类似于和看这个图考察一个单一的训练样本我有一个正样本在这里用一个叉来表示这个样本意思是在水平轴上取值为在竖直轴上取值为然后将参数向量也画上去那么内积将会是什么使用之前的计算方式就是将训练样本投影到参数向量然后将投影的长度画成红色用来表示这是第个训练样本在参数向量上的投影根据我们之前的内容将会等于乘以向量的长度或范数这就等于这两种方式是等价的都可以用来计算和之间的内积这里表达的意思是这个或者的约束是可以被这个约束所代替的因为将其写入我们的优化目标前面说过优化函数可以写为以上就是为什么支持向量机最终会找到大间距分类器的原因因为它试图极大化这些的范数它们是训练样本到决策边界的距离最后一点我们的推导自始至终使用了这个简化假设就是参数的意思是我们让决策界通过原点如果你令不是的话含义就是你希望决策界不通过原点即便不等于支持向量机仍然会找到正样本和负样本之间的大间距分隔核函数给定一个训练实例我们利用的各个特征与我们预先选定的地标的近似程度来选取新的特征例如其中为实例中所有特征与地标之间的距离的和上例中的就是核函数具体而言这里是一个高斯核函数这个函数与正态分布没什么实际上的关系只是看上去像而已这些地标的作用是什么如果一个训练实例与地标之间的距离近似于则新特征近似于如果训练实例与地标之间距离较远则近似于假设我们的训练实例含有两个特征给定地标与不同的值见下图图中水平面的坐标为而垂直坐标轴代表可以看出只有当与重合时才具有最大值随着的改变值改变的速率受到的控制如何选择地标通常是根据训练集的数量选择地标的数量即如果训练集中有个实例则选取个地标并且令这样做的好处在于现在得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的即下面我们将核函数运用到支持向量机中修改我们的支持向量机假设为给定计算新特征当时预测否则反之相应地修改代价函数为在计算这个的时候还需要做一些调整用代替其中是根据我们选择的核函数而不同的一个矩阵这样做的原因是为了简化计算理论上讲我们也可以在逻辑回归中使用核函数但是上面使用来简化计算的方法不适用与逻辑回归因此计算将非常耗费时间逻辑回归的核心是建模样本术语某个类别的概率输出是一个概率值到之间它本质上是概率模型的目标是找到一个几何间隔最大的超平面进行硬分类每个样本只能被明确划分到一个类别逻辑回归是软分类最终分类取概率最高的类别输出的是确定的类别标签如本质上是几何间隔最大化模型可以直接使用现有的软件包来最小化支持向量机的代价函数但在使用这些软件包最小化我们的代价函数之前我们通常需要编写核函数并且如果我们使用高斯核函数那么在使用之前进行特征缩放是非常必要的另外支持向量机也可以不使用核函数不使用核函数又称为线性核函数当我们不采用非常复杂的函数或者我们的训练集特征非常多而实例非常少的时候可以采用这种不带核函数的支持向量机',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-24 22:14:48',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">coygOdegaard</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 1.05rem;">大数据<sup>1</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">算法<sup>3</sup></a><a href="/tags/%E8%AF%AD%E8%A8%80/" style="font-size: 1.05rem;">语言<sup>3</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/07/"><span class="card-archive-list-date">七月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/04/"><span class="card-archive-list-date">四月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">一月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">12</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"></span></div></div><h1 class="post-title" itemprop="name headline">机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-05-08T12:23:38.000Z" title="发表于 2025-05-08 20:23:38">2025-05-08</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-07-24T14:14:48.982Z" title="更新于 2025-07-24 22:14:48">2025-07-24</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为长沙"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>长沙</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><header><h1 id="CrawlerTitle" itemprop="name headline">机器学习</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Odegaard</span><time itemprop="dateCreated datePublished" datetime="2025-05-08T12:23:38.000Z" title="发表于 2025-05-08 20:23:38">2025-05-08</time><time itemprop="dateCreated datePublished" datetime="2025-07-24T14:14:48.982Z" title="更新于 2025-07-24 22:14:48">2025-07-24</time></header><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h2 id="机器学习是什么"><a href="#机器学习是什么" class="headerlink" title="机器学习是什么"></a>机器学习是什么</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/3de7ec9ae75d52a8b074555bcefbc9fe_.jpg" alt="3de7ec9ae75d52a8b074555bcefbc9fe_"></p>
<p>目前存在几种不同类型的学习算法。主要的两种类型被我们称之为监督学习和无监督学习。监督学习这个想法是指，我们将教计算机如何去完成任务，而在无监督学习中，我们打算让它自己进行学习。</p>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>监督学习指的就是我们给学习算法一个数据集，这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。</p>
<p><strong>回归问题</strong>：试着推测出一个连续值的结果。下面的房子例子就是回归问题，要推测的结果就是房子的价格。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/67f278633b69c123d1c93d555408016.jpg" alt="67f278633b69c123d1c93d555408016" style="zoom:67%;" />

<p>回归这个词的意思是，我们在试着推测出这一系列连续值属性。</p>
<p><strong>分类问题</strong>，分类指的是，我们试着推测出<strong>离散的输出值</strong>：0或1，良性或恶性。感觉就是判断给出的数据属于哪一类。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/d7af2031bf79a5b02eeedb033358c4d.jpg" alt="d7af2031bf79a5b02eeedb033358c4d" style="zoom:67%;" />

<p>在这个例子中只有一个特征，就是肿瘤的尺寸，在其它一些机器学习问题中，可能会遇到不止一种特征。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄。在其他机器学习问题中，通常有更多的特征，比如肿块密度，肿瘤细胞尺寸的一致性和形状的一致性等等，还有一些其他的特征。之后会讲一个算法，叫支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。</p>
<p>回归问题和分类问题都属于监督学习，其基本思想是，数据集中的每个样本都有相应的“正确答案”，再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。回归问题，即通过回归来推出一个连续的输出；分类问题，其目标是推出一组离散的结果。</p>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么，任何信息都不知道，只知道是一个数据集。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/a4588fb01a01f7e1fb17e6c89241650.jpg" alt="a4588fb01a01f7e1fb17e6c89241650"></p>
<p>这个图是上面肿瘤的例子，⭕代表良性，❌代表恶性，在监督学习中有这种标志说明是什么情况，但在无监督学习中没有标志，只是数据。</p>
<p>针对数据集，无监督学习能判断出数据有两个不同的聚集簇。这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同的簇，这个就叫做聚类算法。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/46a41b85419ef28ecb55b469f6c7c89.jpg" alt="46a41b85419ef28ecb55b469f6c7c89" style="zoom:67%;" />

<p>无监督学习就是我们没法提前告知算法一些信息。</p>
<p>就是这里是有一堆数据，我不知道数据里面有什么，我不知道谁是什么类型，我甚至不知道人们有哪些不同的类型，这些类型又是什么。但你能自动地找到数据中的结构吗？就是说你要自动地聚类那些个体到各个类，我没法提前知道哪些是哪些。因为我们没有给算法正确答案来回应数据集中的数据，所以这就是无监督学习。</p>
<p>上面的都是<strong>聚类</strong>的例子，聚类只是无监督学习的一种。</p>
<p>接下来介绍的鸡尾酒宴问题属于无监督学习中的<strong>盲源分离</strong>问题。</p>
<p>可能在一个这样的鸡尾酒宴中的两个人，他俩同时都在说话，假设现在是在个有些小的鸡尾酒宴中。我们放两个麦克风在房间中，因为这些麦克风在两个地方，离说话人的距离不同每个麦克风记录下不同的声音，虽然是同样的两个说话人。听起来像是两份录音被叠加到一起，或是被归结到一起，产生了我们现在的这些录音。另外，这个算法还会区分出两个音频资源，这两个可以合成或合并成之前的录音，实际上，鸡尾酒算法的第一个输出结果是：</p>
<p>1，2，3，4，5，6，7，8，9，10,</p>
<p>第二个输出是这样：</p>
<p>1，2，3，4，5，6，7，8，9，10。</p>
<p>第一个输出代表分离出的第一个声源，第二个输出代表分离出的第二个声源。</p>
<p>这里的数字序列可能是对分离后信号的简化表示。实际应用中，输出是时间序列信号（如音频波形），每个数字可能代表某个时间点的信号强度或特征。无需去深度思考。</p>
<h1 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h1><h2 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h2><p>监督学习的第一个例子。预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。	</p>
<p>它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个回归问题。回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格。</p>
<p><strong>在监督学习中我们有一个数据集，这个数据集被称训练集。</strong></p>
<p><strong>在整个课程中用小写的m来表示训练样本的数目。</strong></p>
<p>假如上面房子的回归问题的训练集（<strong>Training Set</strong>）如下表所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250508215836945.png" alt="image-20250508215836945"></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250508215909094.png" alt="image-20250508215909094" style="zoom:40%;" />

<p>将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。</p>
<p>h表示的是一个函数，由学习算法根据训练集输出，输入是房屋尺寸大小，输出的是房子价格。</p>
<p>h的一种可能表达方式为：<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 x<br>$$<br>因为只含有<strong>一个特征&#x2F;输入变量</strong>，因此这样的问题叫作单变量线性回归问题。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520100402742.png" alt="image-20250520100402742"></p>
<p>有一个像这样的训练集， 𝑚代表了训练样本的数量，比如 𝑚 &#x3D; 47。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 x<br>$$<br>接下来为我们的模型选择合适的<strong>参数</strong>（ parameters） 𝜃0 和 𝜃1，在房价问题这个例子中便是直线的斜率和在𝑦 轴上的截距。  我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的<strong>差距</strong>（下图中蓝线所指）就是<strong>建模误差</strong>（ modeling error）。  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520101109903.png" alt="image-20250520101109903">目标便是选择出可以使得建模误差的平方和能够最小的模型参数， 即使得代价函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250520101757329.png" alt="image-20250520101757329" style="zoom:50%;" />最小。</p>
<p>绘制一个等高线图，三个坐标分别为𝜃0和𝜃1 和𝐽(𝜃0, 𝜃1)：  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250520101905831.png" alt="image-20250520101905831" style="zoom:33%;" />可以看出在三维空间中存在一个使得𝐽(𝜃0, 𝜃1)最小的点。  </p>
<p>代价函数也被称作平方误差函数，有时也被称为平方误差代价函数，代价函数是解决回归问题最常用的手段。</p>
<h2 id="代价函数的直观理解"><a href="#代价函数的直观理解" class="headerlink" title="代价函数的直观理解"></a>代价函数的直观理解</h2><p>代价函数是用来干嘛的，我们为什么要用它。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520102928659.png" alt="image-20250520102928659"></p>
<p>为了便于理解，使𝜃0&#x3D;0。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520103215675.png" alt="image-20250520103215675"></p>
<p>yi代表的是训练集中的数据。h𝜃的参数是x，J的参数是𝜃1。上图可以看出当𝜃1&#x3D;1时，代价函数J&#x3D;0。</p>
<p>接下来时当𝜃1&#x3D;0.5时：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520104130361.png" alt="image-20250520104130361"></p>
<p>等于1时：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520104249847.png" alt="image-20250520104249847"></p>
<p>对于每个𝜃1的值，都对应着一个假设函数的值或者一条直线，并且根据每个不同的𝜃1，我们都可以得到一个不同的J(𝜃1)的值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520104644091.png" alt="image-20250520104644091"></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数𝐽(𝜃0, 𝜃1) 的最小值。  </p>
<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合(𝜃0, 𝜃1, . . . . . . , 𝜃𝑛)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（ local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（ global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。  </p>
<p>这个算法时怎么工作的，可以这样想：想象一下你正站立在山的一点上，  在梯度下降算法中，我们要做的就是旋转 360 度，看看我们的周围哪个方向可以最快下山。来到山坡上，我们站在山坡上的一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。</p>
<p>批量梯度下降（ batch gradient descent）算法的公式为：  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250520110754153.png" alt="image-20250520110754153" style="zoom:50%;" /></p>
<p>上面那行英语的意思是，反复用这个公式直到收敛。其中𝑎是学习率（ learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。  </p>
<p>符号<code>:=</code>的意思是赋值，这是一个赋值运算符。单独的<code>=</code>代表的是比较运算符。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520145329030.png" alt="image-20250520145329030"></p>
<p>梯度下降中，我们要同时更新𝜃0和𝜃1，当 𝑗 &#x3D; 0 和𝑗 &#x3D; 1时，会产生更新，所以你将更新𝐽(𝜃0)和𝐽(𝜃1)。  记住，要<strong>同时更新</strong>，不能先更新一个再更新另一个，先更新其中一个的话会导致接下来算出的微分项的值出现变换，因为其中一个值变了。</p>
<h2 id="梯度下降的直观理解"><a href="#梯度下降的直观理解" class="headerlink" title="梯度下降的直观理解"></a>梯度下降的直观理解</h2><p>梯度下降算法：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250520150117798.png" alt="image-20250520150117798" style="zoom:50%;" />，描述：对𝜃赋值，使得𝐽(𝜃)按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中𝑎是学习率（ learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。  </p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520150210495.png" alt="image-20250520150210495"></p>
<p>求导的目的，基本上可以说取这个红点的切线，  现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的𝜃1， 𝜃1更新后等于𝜃1减去一个正数乘以𝑎。  </p>
<p>如果𝑎太小或𝑎太大会出现什么情况：  </p>
<p>如果𝑎太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果𝑎太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。</p>
<p>如果𝑎太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果𝑎太大，它会导致无法收敛，甚至发散。  </p>
<p>假设将𝜃1初始化在局部最低点，因为它已经在一个局部的最优处或局部最低点，结果是局部最优点的导数将等于零，使得𝜃1不再改变，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率𝑎保持不变时，梯度下降也可以收敛到局部最低点。  </p>
<p>在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小𝑎。  </p>
<p>可以用梯度下降算法来最小化任何代价函数𝐽，不只是线性回归中的代价函数𝐽。  </p>
<h2 id="梯度下降的线性回归"><a href="#梯度下降的线性回归" class="headerlink" title="梯度下降的线性回归"></a>梯度下降的线性回归</h2><p>用梯度下降算法，并将其应用于具体的拟合直线的线性回归算法里。  </p>
<p>先计算微分项：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520151937828.png" alt="image-20250520151937828"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520152054863.png" alt="image-20250520152054863"></p>
<p>所以，算法会被改写为：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520152212561.png" alt="image-20250520152212561"></p>
<p>不断重复，直到收敛。记住，𝜃0和𝜃1要同时更新。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520152415146.png" alt="image-20250520152415146"></p>
<p>使用梯度下降算法是因为它更容易到达局部最小值，而根据初始化的不同，会得到不同的局部最优解。但是，事实证明，用于线性回归的代价函数总是一个弓形样子的函数，叫作凸函数，这种函数没有局部最优解，只有一个全局最优解。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520152728280.png" alt="image-20250520152728280"></p>
<p>一般来说初始化参数的时候都设为0。</p>
<p>刚刚使用的算法，有时也称为批量梯度下降。  ”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有𝑚个训练样本求和。  </p>
<h1 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h1><h2 id="多维特征"><a href="#多维特征" class="headerlink" title="多维特征"></a>多维特征</h2><p>对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为(𝑥1, 𝑥1, . . . , 𝑥𝑛)。  </p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250705112627927.png" alt="image-20250705112627927"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250705112649877.png" alt="image-20250705112649877"></p>
<p>这上面的公式是𝜃TX的原因是上面的X(2)是一列，形状是[4,1]，jupyter里面的数据的形状是[1,4]，所以里面的公式是X𝜃T，具体的情况要具体分析，记住基本公式，参数乘以变量。</p>
<h2 id="多变量梯度下降"><a href="#多变量梯度下降" class="headerlink" title="多变量梯度下降"></a>多变量梯度下降</h2><p>在多变量线性回归中的代价函数，这个代价函数是所有建模误差的平方和，即： <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705145029436.png" alt="image-20250705145029436" style="zoom:50%;" />，其中<br>$$<br>h_θ (x)&#x3D;θ^T X&#x3D;θ_0+θ_1 x_1+θ_2 x_2+…+θ_n x_n<br>$$<br>多变量线性回归的批量梯度下降算法为：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705145632954.png" alt="image-20250705145632954" style="zoom:80%;" /></p>
<p>即<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705145701988.png" alt="image-20250705145701988" style="zoom:80%;" />，求导得：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705145729471.png" alt="image-20250705145729471" style="zoom:80%;" /></p>
<p>跟前面单变量的公式没有什么大变化，就是求导后需要计算的变多了。</p>
<p>计算代价函数的代码如下：</p>
<pre><code class="hljs plaintext">def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))</code></pre>

<h2 id="梯度下降法实践-1-特征缩放"><a href="#梯度下降法实践-1-特征缩放" class="headerlink" title="梯度下降法实践 1-特征缩放"></a>梯度下降法实践 1-特征缩放</h2><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有<strong>相近的尺度</strong>，这将帮助梯度下降算法更快地收敛。</p>
<p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0- 2000 平方英尺，而房间数量的值则是 0-5，以两个参数分别为横纵坐标，绘制<strong>代价函数</strong>的等高线图（在这里先忽略𝜃0），<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705151217067.png" alt="image-20250705151217067" style="zoom:50%;" /></p>
<p>能看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</p>
<p>解决的方法是尝试将所有特征的尺度都尽量缩放到**-1 到 1** 之间。 ，这也是在做特征缩放时的通常目的，但其实并不严格要求必须是-1到1，在这些附近都可以，重点是将范围靠近-1到1。所以，如果有一个特征也就是变量的范围是-0.0001到0.0001的话，得对其进行扩展。一般在-3到3，-1&#x2F;3到1&#x2F;3都是可以的。</p>
<p>除了将特征除以它的最大值外，还可以进行一种叫作均值归一化的工作，包括：</p>
<p>1、将原来的变量值减去平均值除以（最大值-最小值），一般用这个就足够了</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/3f7dac1d8c77ee704aca91562a78b88.jpg" alt="3f7dac1d8c77ee704aca91562a78b88"></p>
<p>2、<br>$$<br>x_n&#x3D;(x_n-μ_n)&#x2F;s_n，其中 μ_n是平均值，s_n是标准差。<br>$$</p>
<h2 id="梯度下降法实践-2-学习率"><a href="#梯度下降法实践-2-学习率" class="headerlink" title="梯度下降法实践 2-学习率"></a>梯度下降法实践 2-学习率</h2><p>如何确定梯度下降算法在正常工作，画图表：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250706210423707.png" alt="image-20250706210423707" style="zoom:80%;" />

<p>梯度下降算法的每次迭代受到学习率的影响，如果学习率𝑎过小，则达到收敛所需的迭代次数会非常高；如果学习率𝑎过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试些学习率：𝛼 &#x3D; 0.01， 0.03， 0.1， 0.3， 1， 3， 10</p>
<h2 id="特征和多项式回归"><a href="#特征和多项式回归" class="headerlink" title="特征和多项式回归"></a>特征和多项式回归</h2><p>多项式回归，可以使用线性回归的方式来拟合非常复杂的函数，或者是非线性函数。</p>
<p>以预测房价模型为例（在线性回归模型中你可以选择提供的特征作为特征，也可以选择自己创建一个新的特征，哎下面的例子中，题目给了临街宽度和纵深两个特征，我们也可以自己创建一个特征——面积，这样子可以简化线性回归模型，得到一个更好的模型）：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706211418121.png" alt="image-20250706211418121"></p>
<p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，二次方或者三次方模型：<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 x_1+θ_2 x_2^2，h_θ (x)&#x3D;θ_0+θ_1 x_1+θ_2 x_2^2+θ_3 x_3^3<br>$$<br>另外，我们可以令<br>$$<br>𝑥_2 &#x3D; 𝑥_2^2, 𝑥_3 &#x3D; 𝑥_3^3<br>$$<br>，从而将模型转化为线性回归模型，由于次方的存在导致参数范围被扩大了很多，所以在运行梯度下降算法前，必须进行<strong>特征缩放</strong>。除了上面给出的这一种，还有一种是开平方：<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 (size)+θ_2 \sqrt{size}<br>$$<br>通过不同的参数形式，最后的曲线也会有所不同。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706212254715.png" alt="image-20250706212254715"></p>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706214657023.png" alt="image-20250706214657023"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706214739079.png" alt="image-20250706214739079"></p>
<p>运用正规方程方法求解参数：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706214804481.png" alt="image-20250706214804481"></p>
<p>对于那些<strong>不可逆的矩阵</strong>（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是<strong>不能用</strong>的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706214841139.png" alt="image-20250706214841139"></p>
<p>只要特征变量的数目并不大，标准方程是一个很好的计算参数𝜃的替代方法。具体地说，只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法。  </p>
<pre><code class="hljs plaintext">import numpy as np
def normalEqn(X, y):
	theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X 等价于 X.T.dot(X)
	return theta</code></pre>

<p>注意，这里返回的theta不是一个数，而是一个元组。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>在分类问题中，要预测的变量y是离散的值。  </p>
<p>在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误），从二元的分类问题开始。</p>
<p>我们将因变量可能属于的两个类分别称为负向类和正向类，则因变量 y&#x3D;0或1 ，其中 0 表示负向类，1 表示正向类。</p>
<p>逻辑回归算法的性质是：它的输出值永远在 0 到 1 之间。  逻辑回归算法是一个分类算法，适用于y取离散的值的情况下。</p>
<h2 id="假说表示"><a href="#假说表示" class="headerlink" title="假说表示"></a>假说表示</h2><p>为什么线性回归算法不适用于分类问题？</p>
<p>根据线性回归模型我们只能预测连续的值，然而对于分类问题（例子是肿瘤分类），我们需要输出0或1，我们可以预测：<br>$$<br>当h_θ (x)&gt;&#x3D;0.5时，预测 y&#x3D;1。<br>当h_θ (x)&lt;0.5时，预测 y&#x3D;0 。<br>$$<br>没有极端数据出现的时候使用线性回归算法看着也可以，但一旦极端数据出现，整体的判断标准就会被破坏。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707095415565.png" alt="image-20250707095415565"></p>
<p>有极端数据出现：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707095442011.png" alt="image-20250707095442011"></p>
<p>所以线性回归模型并不适用于分类问题。</p>
<p>逻辑回归模型的假设是：<br>$$<br>h_θ(x)&#x3D;g(θ^T X)其中：X 代表特征向量，g代表逻辑函数是一个常用的逻辑函数，S形函数，公式为： g(z)&#x3D;1&#x2F;(1+e^{-z} )。<br>$$<br>θT*X，这个就是<strong>线性回归模型的结果</strong>（这里的参数向量θ是n行1列的，X的数据是一列一列的），所以逻辑回归模型是对线性回归模型的值进行处理。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707100936257.png" alt="image-20250707100936257"></p>
<pre><code class="hljs plaintext">import numpy as np
def sigmoid(z):
	return 1 / (1 + np.exp(-z))</code></pre>

<p>ℎ𝜃(𝑥)的意思是，对于给定的输入变量，根据选择的参数计算输出变量&#x3D;1 的可能性。如果对于给定的𝑥，通过已经确定的参数计算得出ℎ𝜃(𝑥) &#x3D; 0.7，则表示有 70%的几率𝑦为正向类，相应地𝑦为负向类的几率为 1-0.7&#x3D;0.3。</p>
<p><strong>逻辑回归的本质</strong>：逻辑回归是一种<strong>线性分类模型</strong>。它通过一个<strong>线性方程</strong>（例如，<em>z</em>&#x3D;<em>θ</em>0+<em>θ</em>1<em>x</em>1+<em>θ</em>2<em>x</em>2）将输入特征（如测试1和测试2的结果）<strong>映射</strong>到一个<strong>概率值</strong>（通过sigmoid函数）。决策边界（即区分接受&#x2F;抛弃的阈值）是线性的，比如一条直线（在二维特征空间中）。</p>
<h2 id="判定边界"><a href="#判定边界" class="headerlink" title="判定边界"></a>判定边界</h2><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250707102404958.png" alt="image-20250707102404958" style="zoom:67%;" />

<p>根据逻辑回归模型的这个图，我们知道当𝑧 &#x3D; 0 时，𝑔(𝑧) &#x3D; 0.5；𝑧 &gt; 0 时，𝑔(𝑧) &gt; 0.5；𝑧 &lt; 0 时，𝑔(𝑧) &lt; 0.5；</p>
<p>又 𝑧 &#x3D; 𝜃𝑇𝑥 ，即：</p>
<p>𝜃𝑇𝑥 &gt;&#x3D; 0 时，预测 𝑦 &#x3D; 1；𝜃𝑇𝑥 &lt; 0 时，预测 𝑦 &#x3D; 0。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707102617314.png" alt="image-20250707102617314"></p>
<p>对于上面那个模型，我们可以很明显地看出是一条直线将预测结果分成两部分。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707102727734.png" alt="image-20250707102727734"></p>
<p>又两个例子可以看出，我们要根据分界线的形状来判断我们应该使用的分界线函数是什么。</p>
<h2 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h2><p>对于线性回归模型，定义的代价函数是所有模型误差的平方和。要是将逻辑回归模型的函数代入到这个代价函数中，得到的代价函数将是一个非凸函数，这意味着代价函数会有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707103625333.png" alt="image-20250707103625333"></p>
<p>定义代价函数为：<br>$$<br>J(θ)&#x3D;\frac{1}{m} ∑_{i&#x3D;1}^mCost(h_θ (x^i ),y^i )<br>$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707104627579.png" alt="image-20250707104627579"></p>
<p>h𝜃x的取值范围在0~1。</p>
<p>这样构建的Cost函数的特点是：当实际的 𝑦 &#x3D; 1 且ℎ𝜃(𝑥)也为 1 时误差为 0，当 𝑦 &#x3D; 1 但ℎ𝜃(𝑥)不为 1 时误差随着ℎ𝜃(𝑥)变小而变大；当实际的 𝑦 &#x3D; 0 且ℎ𝜃(𝑥)也为 0 时代价为 0，当𝑦 &#x3D; 0 但ℎ𝜃(𝑥)不为 0 时误差随着 ℎ𝜃(𝑥)的变大而变大。</p>
<p>将Cost函数进行简化，就是用一个表达式表达出来，如下：<br>$$<br>Cost(h_θ (x),y)&#x3D;-y×log(h_θ (x))-(1-y)×log(1-h_θ (x))<br>$$<br>代入代价函数为：<br>$$<br>J(θ)&#x3D;-\frac{1}{m}∑_{i&#x3D;1}^m[y^{(i)} log(h_θ (x^{(i)} ))+(1-y^{(i)})log(1-h_θ (x^{(i)} ))]<br>$$</p>
<pre><code class="hljs plaintext">import numpy as np
def cost(theta, X, y):
  theta = np.matrix(theta)
  X = np.matrix(X)
  y = np.matrix(y)
  first = np.multiply(-y, np.log(sigmoid(X* theta.T)))
  second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T)))
  return np.sum(first - second) / (len(X))</code></pre>

<p>sigmoid函数在上面定义了。</p>
<p>梯度下降算法的公式和前面的一样，记住，<strong>同时更新</strong>所有参数。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707111211261.png" alt="image-20250707111211261"></p>
<p>通过观察梯度下降算法的式子可以发现，这个式子和之前线性回归的梯度下降算法的式子是一样的，但<strong>ℎ𝜃(𝑥)的式子是不同的</strong>。</p>
<p>特征缩放的技巧也适用于逻辑回归。</p>
<h2 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h2><p>共轭梯度法，BFGS (变尺度法) 和 L-BFGS (限制变尺度法) 就是一些更高级的优化算法，它们需要有一种方法来计算 𝐽(𝜃)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。  </p>
<p>这三种算法的具体细节可以不用取探究，因为过于复杂。</p>
<p>使用这其中任何一个算法，通常不需要手动选择学习率 𝛼，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索算法，它可以自动尝试不同的学习速率 𝛼，并自动选择一个好的学习速率 𝛼，因此它甚至可以为每次迭代选择不同的学习速率。  </p>
<p>最好不要使用 L-BGFS、 BFGS 这些算法，除非你是数值计算方面的专家。</p>
<p>如何使用这些算法，这些算法适合在很大的机器学习问题中使用。</p>
<p>在jupyter中利用的是python中的<code>scipy.optimize.fmin_tnc()</code>函数，这是一个使用截断牛顿法（TNC）寻找局部最小值的优化函数，特别适用于有界约束的优化问题。</p>
<h2 id="寻找决策边界"><a href="#寻找决策边界" class="headerlink" title="寻找决策边界"></a>寻找决策边界</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709171344091.png" alt="image-20250709171344091"></p>
<p>所以jupyter中的寻找决策边界会除以第三个参数值。</p>
<h2 id="构造多项式特征"><a href="#构造多项式特征" class="headerlink" title="构造多项式特征"></a>构造多项式特征</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710155849606.png" alt="image-20250710155849606"></p>
<p>由上图可知其中没有线性决策界限，来良好的分开两类数据，原数据有两个特征，x1和x2，可以明显看出就这两个特征无法较好拟合这些数据，所以要构造从原始特征的多项式中得到的特征，即通过数学变换，将原始特征（ <em>x</em>1 和 <em>x</em>2）扩展为一组新特征，这些新特征是原始特征的高阶多项式组合（例如，<em>x</em>1平方、<em>x</em>2平方、<em>x</em>1×<em>x</em>2、<em>x</em>1立方 等）。然后，在这些新特征上训练逻辑回归模型。</p>
<p><strong>为什么能解决非线性问题</strong>：尽管逻辑回归本身是线性的，但通过添加非线性特征（如平方项或交互项），模型在扩展后的高维特征空间中学习到的决策边界仍然是线性的，但这个边界在原始特征空间中会呈现为曲线、椭圆或其他非线性形状。这相当于给模型“添加了非线性能力”，而不改变其核心算法。</p>
<p>首先要选择<strong>阶数</strong>，阶数决定了多项式的复杂性。从二阶开始（通常足够处理大多数非线性问题），然后根据模型性能调整。</p>
<p>平方项：捕捉单个测试的非线性效应。</p>
<p>交互项（<em>x</em>1×<em>x</em>2）：捕捉两个测试的联合效应。</p>
<p>由于选择了高阶数的模型，为了避免过拟合，通常还要进行正则化操作。</p>
<h2 id="多类别分类：一对多"><a href="#多类别分类：一对多" class="headerlink" title="多类别分类：一对多"></a>多类别分类：一对多</h2><p>如何使用逻辑回归来解决多类别分类问题。</p>
<p>之前的二元分类问题的图，和现在的多类分类问题的图：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709173446894.png" alt="image-20250709173446894" style="zoom:80%;" /></p>
<p>用 3 种不同的符号来代表 3 个类别，问题就是给出 3 个类型的数据集，如何得到一个学习算法来进行分类呢？</p>
<p>面对二元分类问题可以使用逻辑回归，也可以将数据集一分为二为正类和负类，而一对多的分类思想，我们可以将其用在多类分类问题上，这个方法也被称为”一对余”方法。</p>
<p>现在我们有一个训练集，好比上图表示的有 3 个类别，我们用三角形表示 𝑦 &#x3D; 1，方框表示𝑦 &#x3D; 2，叉叉表示 𝑦 &#x3D; 3。使用一个训练集将三元分类问题转化为<strong>三个二元分类问题</strong>，先从用三角形代表的类别 1 开始，实际上我们可以创建一个，新的”伪”训练集，类型 2 和类型 3 定为负类，类型 1 设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709175113715.png" alt="image-20250709175113715"></p>
<p>这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为 1，圆形的值为 0，下面可以训练一个标准的逻辑回归分类器，这样就得到一个边界。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709175233890.png" alt="image-20250709175233890"></p>
<p>选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论𝑖值是多少，我们都有最高的概率值，我们预测𝑦就是那个值。<br>$$<br>最后我们得到一系列的模型简记为： h_θ^{(i) } (x)&#x3D;p(y&#x3D;i|x;θ)其中：i&#x3D;(1,2,3….k)<br>$$</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="过拟合的问题"><a href="#过拟合的问题" class="headerlink" title="过拟合的问题"></a>过拟合的问题</h2><p>就是过于强调拟合原始数据。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709201037517.png" alt="image-20250709201037517"></p>
<p>第一个模型是一个线性模型，欠拟合，不能很好地适应训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好。</p>
<p>分类问题中也存在这样的问题：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709201149549.png" alt="image-20250709201149549" style="zoom:80%;" /></p>
<p>以多项式理解， 𝑥 的次数越高，拟合的越好，但相应的预测的能力就可能变差。</p>
<p>如何处理过拟合问题：</p>
<p>1.丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙，例如 PCA</p>
<p>2.正则化。 保留所有的特征，但是减少参数的大小。</p>
<h2 id="代价函数-2"><a href="#代价函数-2" class="headerlink" title="代价函数"></a>代价函数</h2><p>在上面过拟合的回归问题中有以下模型：<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 x_1+θ_2 x_2^2+θ_3 x_3^3+θ_4 x_4^4<br>$$<br>正是高次项导致了过拟合的产生，所以如果能让这些高次项的系数接近于0的话，那就能很好的拟合了。</p>
<p>所以我们要做的就是在一定程度上<strong>减小</strong>这些参数𝜃的值，这就是<strong>正则化的基本方法</strong>。我们决定要减少𝜃3和𝜃4的大小，我们要做的便是修改代价函数，在其中𝜃3和𝜃4设置一点惩罚。这样做的话，在尝试最小化代价时也会将这个惩罚纳入考虑中，并最终导致选择较小一些的𝜃3和𝜃4。惩罚就是在代价函数中使𝜃3和𝜃4的占比变高，使得在最小化代价函数时，也会更多地考虑这两个参数。<br>$$<br>修改后的代价函数：min\frac{1}{2m}[∑_{i&#x3D;1}^m[(h_θ (x^{(i)} )-y^{(i)} )^2+1000θ_3^2+10000θ_4^2]]<br>$$<br>假如有非常多的特征，我们并不知道其中哪些特征要惩罚，那么就对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：<br>$$<br>J(θ)&#x3D;\frac{1}{2m}[∑_{i&#x3D;1}^m(h_θ (x^{(i)})-y^{(i)})^2+λ∑_{j&#x3D;1}^nθ_j^2 ]<br>$$<br>其中𝜆又称为正则化参数，根据惯例，我们<strong>不对𝜃0 进行惩罚</strong>。经过正则化处理的模型与原模型的可能对比如下图所示：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709202809943.png" alt="image-20250709202809943"></p>
<p>如果选择的正则化参数λ<strong>过大</strong>，则会把所有的参数<strong>都最小化</strong>了，导致模型变成 ℎ𝜃(𝑥) &#x3D; 𝜃0，也就是上图中红色直线所示的情况，造成<strong>欠拟合</strong>。  </p>
<h2 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h2><p>正则化<strong>线性回归</strong>的代价函数是：<br>$$<br>J(θ)&#x3D;\frac{1}{2m}[∑_{i&#x3D;1}^m(h_θ (x^{(i)})-y^{(i)})^2+λ∑_{j&#x3D;1}^nθ_j^2 ]<br>$$<br>由于𝜃0没有进行正则化，所以梯度下降算法将会分成两种情况：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709210159349.png" alt="image-20250709210159349" style="zoom:50%;" /></p>
<p>对第二个式子（𝑗 &#x3D; 1,2, . . . , 𝑛 ）进行调整可以得到：<br>$$<br>θ_j:&#x3D;θ_j (1-a \frac{λ}{m})-a \frac{1}{m} ∑_{i&#x3D;1}^m(h_θ (x^{(i)})-y^{(i)})x_j^{(i)}<br>$$<br>可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令𝜃值减少了一个额外的值。</p>
<p>利用正规方程来求解正则化线性回归模型：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709211926347.png" alt="image-20250709211926347" style="zoom:50%;" />，图中的矩阵尺寸为 (𝑛 + 1) ∗ (𝑛 + 1)，因为不算𝜃0还有n个特征。</p>
<h2 id="正则化的逻辑回归模型"><a href="#正则化的逻辑回归模型" class="headerlink" title="正则化的逻辑回归模型"></a>正则化的逻辑回归模型</h2><p>这是正则化的<strong>逻辑回归</strong>的代价函数。</p>
<p>给代价函数增加一个正则化的表达式，得到代价函数：<br>$$<br>J(θ)&#x3D;\frac{1}{m} ∑_{i&#x3D;1}^m[-y^{(i)} log(h_θ (x^{(i)} ))-(1-y^{(i)} )log(1-h_θ (x^{(i)} ))]+\frac{λ}{2m} ∑_{j&#x3D;1}^nθ_j^2<br>$$</p>
<pre><code class="hljs plaintext">import numpy as np
def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))
    reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2))
    return np.sum(first - second) / (len(X)) + reg</code></pre>

<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709213440950.png" alt="image-20250709213440950" style="zoom:67%;" />

<p>这边的 h 函数是sigmoid函数。</p>
<p><strong>𝜃0不参与其中的任何一个正则化</strong>。</p>
<p>接下来的课程中，我们将学习一个非常强大的非线性分类器，无论是<strong>线性回归</strong>问题，还是<strong>逻辑回归</strong>问题，都可以<strong>构造多项式</strong>来解决。你将逐渐发现还有更强大的非线性分类器，可以用来解决多项式回归问题。  </p>
<h1 id="神经网络：表述"><a href="#神经网络：表述" class="headerlink" title="神经网络：表述"></a>神经网络：表述</h1><h2 id="非线性假设"><a href="#非线性假设" class="headerlink" title="非线性假设"></a>非线性假设</h2><p>无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。</p>
<p>使用非线性的多项式项，能够帮助我们建立更好的分类模型，但相应的我们要计算的特征数会大大增多，普通的逻辑回归模型，不能有效地处理这么多的特征，这时候就需要神经网络。</p>
<h2 id="模型表示-1"><a href="#模型表示-1" class="headerlink" title="模型表示"></a>模型表示</h2><p>神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元）采纳一些特征作为输出，并且根据本身的模型提供一个输出。</p>
<p>以逻辑回归模型作为学习模型的神经元示例：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710173745443.png" alt="image-20250710173745443"></p>
<p>解读：上面的黄圈看作是神经元，左边的蓝圈和黄圈的连线看作是输入&#x2F;树突，黄圈右边的线看作是输出&#x2F;轴突。通过树突传递一些信息，然后神经元做一些计算，然后通过轴突输出计算结果。这个图表表示的是对h的计算，而h是sigmoid函数。x1、x2、x3是输入结点，额外的结点x0被称为<strong>偏置单位</strong>，因为x0总是等于1。x0可画可不画，根据具体情况来。</p>
<p>在神经网络中，参数𝜃又可被称为权重。</p>
<p>上面的一个小黄圈代表一个单一的神经元，而神经网络是不同的神经元组合在一起的集合。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710175318298.png" alt="image-20250710175318298"></p>
<p>第一层叫作输入层，在这一层输入特征项x1、x2、x3；最后一层（第3层）叫作输出层，因为这一层的神经元输出假设的最终计算结果；中间的一层称作隐藏层，神经网络中可以有不止一个隐藏层，非输出层和输入层的都叫做隐藏层。</p>
<p>在隐藏层出现的蓝色圈被称作偏置单位，它的值永远是1。</p>
<p>下面的图有3个输入单元和3个隐藏单元。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710213148069.png" alt="image-20250710213148069"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710213208144.png" alt="image-20250710213208144"></p>
<p>每一个𝑎都是由上一层所有的𝑥和每一个𝑥所对应的参数决定的，这样从左到右的算法称为前向传播算法。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250710213756450.png" alt="image-20250710213756450" style="zoom:67%;" />  

<p><code>𝜃*X</code>不会等于a，因为g(𝜃*X)&#x3D;a</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250710215327579.png" alt="image-20250710215327579" style="zoom:70%;" />

<p>这边要转置应该是因为是一行一行地输入数据的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710220504253.png" alt="image-20250710220504253"></p>
<p><strong>要注意偏置单位的添加。</strong></p>
<p>如果我们暂时只看第二层和第三层的话：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250710215557120.png" alt="image-20250710215557120" style="zoom:33%;" /></p>
<p>可以发现，其实神经网络就像是 logistic regression，只不过我们把 logistic regression 中的输入向量[𝑥1 ∼ 𝑥3] 变成了中间层的[𝑎1(2) ∼ 𝑎3(2)], 即:<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250710215730645.png" alt="image-20250710215730645" style="zoom:50%;" /></p>
<p>特征项a1、a2、a3是作为输入的函数来学习的，所以在神经网络中，它没有使用输入特征x1、x2、x3来训练逻辑回归，而是自己根据a1、a2、a3来训练逻辑回归，所以如果在𝜃1中选择了不同的参数，那就可以学习到比较复杂的特征，就可以得到一个更好的假设，比使用原始输入时得到的假设更好（构造多项式特征？这里的特征也是通过学习模型得出来的）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710221653096.png" alt="image-20250710221653096"></p>
<p>神经网络中神经元相互连接的方式称为神经网络的架构。</p>
<h2 id="特征和直观理解"><a href="#特征和直观理解" class="headerlink" title="特征和直观理解"></a>特征和直观理解</h2><p>从本质上讲，神经网络能够通过<strong>学习</strong>得出其自身的一系列特征（在预设的<strong>网络架构</strong>和<strong>激活函数</strong>框架下，通过训练数据<strong>动态调整权重参数</strong>，使用梯度下降等优化算法调整，使模型逼近目标函数。）。在普通的逻辑回归中，我们被限制为使用数据中的原始特征𝑥1, 𝑥2, . . . , 𝑥𝑛，我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。  </p>
<p>神经网络中，单层神经元（无中间层）的计算可用来表示逻辑运算，比如逻辑与(AND)、逻辑或(OR)。  </p>
<p>逻辑与，用下面的这样一个神经网络表示AND函数：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250711153715449.png" alt="image-20250711153715449" style="zoom:50%;" /></p>
<p>$$<br>其中θ_0&#x3D;-30,θ_1&#x3D;20,θ_2&#x3D;20 我们的输出函数h_θ (x)即为：h_Θ (x)&#x3D;g(-30+20x_1+20x_2 )<br>$$<br>为什么要这样设置参数？根据g(x)的图像和真值表得出。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250711153927581.png" alt="image-20250711153927581"></p>
<p>再根据同样的步骤设计OR函数（三个权重分别为-10， 20， 20），它与AND函数的区别就是参数的取值不同。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250711154030163.png" alt="image-20250711154030163"></p>
<p>当输入特征为布尔值（ 0 或 1）时，我们可以用一个单一的激活层可以作为二元逻辑运算符，为了表示不同的运算符，我们只需要选择不同的权重即可。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250711154327345.png" alt="image-20250711154327345"></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250711154521895.png" alt="image-20250711154521895" style="zoom:67%;" />

<p>我们可以利用神经元来<strong>组合</strong>成更为复杂的神经网络以实现更复杂的运算。  </p>
<p>实现 XNOR 功能（输入的两个值必须一样，均为 1 或均为 0），即：XNOR &#x3D; (x1 AND x2) OR((NOT x1)AND(NOT x2))</p>
<p>首先构造一个能表达(NOT x1)AND(NOT x2)部分的神经元：(!x1)交(!x2) &#x3D; !(x1并x2) &#x3D; 非AND<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250711155524524.png" alt="image-20250711155524524" style="zoom:36%;" /></p>
<p>然后将表示 AND 的神经元和表示(NOT x1)AND(NOT x2)的神经元以及表示 OR 的神经元进行组合：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250711155624715.png" alt="image-20250711155624715"></p>
<h2 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h2><p>如果我们要训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层我们应该有 4 个值。例如，第一个值为 1 或 0 用于预测是否是行人，第二个值用于判断是否为汽车。</p>
<p>输入向量𝑥有三个维度，两个中间层，输出层 4 个神经元分别用来表示 4 类，也就是每一个数据在输出层都会出现[𝑎 𝑏 𝑐 𝑑]𝑇，且𝑎, 𝑏, 𝑐, 𝑑中仅有一个为 1，表示当前类。下面是该神经网络的可能结构示例：  </p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250711160844709.png" alt="image-20250711160844709"></p>
<p>我们不用1代表行人，2代表车这种形式，而是构造四个分类器，输出1或0，根据哪个位置是1来判断是什么。</p>
<h1 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h1><h2 id="代价函数-3"><a href="#代价函数-3" class="headerlink" title="代价函数"></a>代价函数</h2><p>假设神经网络的训练样本有𝑚个，每个包含一组输入𝑥和一组输出信号𝑦。L：神经网络结构的总层数；Sl（这是小L）：第 l 层的单元个数也就是神经元的数量，不包括 l 层的偏差单元。𝑆𝐿代表最后一层中处理单元的个数。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250711165618966.png" alt="image-20250711165618966"></p>
<p>二元分类问题只有一个输出单元，所以只有一个输出结果。</p>
<p>多元分类问题，也就是K类分类问题，会有K个输出单元，输出是一个K维向量。</p>
<p>先来看一下逻辑回归问题中的代价函数：<br>$$<br>J(θ)&#x3D;\frac{1}{m} ∑_{i&#x3D;1}^m[-y^{(i)} log(h_θ (x^{(i)} ))-(1-y^{(i)} )log(1-h_θ (x^{(i)} ))]+\frac{λ}{2m} ∑_{j&#x3D;1}^nθ_j^2<br>$$<br>在逻辑回归中，我们只有一个输出变量，又称标量，也就是只有一个逻辑输出单元，也只有一个因变量y，但在神经网络中会有K个逻辑输出单元。<br>$$<br>h_θ (x)∈R^K，(h_θ (x))_i&#x3D;i^th output<br>$$<br>h(x)是一个K维向量；h(x)i 代表的是第 i 个输出，也就是它选择了输出向量中的第 i 个元素。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250711171756403.png" alt="image-20250711171756403"></p>
<p>首先要计算从1到K的每一个逻辑回归算法的代价函数的和。最后的那个正则化的求和项，它是将所有的参数（除了i &#x3D; 0的，也就是偏差单位）都加起来。</p>
<p><strong>上面θ的 j 和 i 应该反了</strong><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250713205157804.png" alt="image-20250713205157804" style="zoom:33%;" />这样的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250714102913373.png" alt="image-20250714102913373" style="zoom:33%;" /></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250711173149157.png" alt="image-20250711173149157" style="zoom:33%;" />根据这个图可以知道，参数的行号是从1开始计数的，所以上面的 i 是从1开始的。</p>
<p>正则化的那一项只是排除了每一层 θ0后，每一层的θ矩阵的和。最里层的循环 j 循环所有的行（由 Sl +1 层的激活单元数决定），循环 i 则循环所有的列，由该层（ Sl 层）的激活单元数所决定。</p>
<p>上面那个公式的意思就是：h(θ）与真实值之间的距离为每个样本-每个类输出的加和，对参数进行<strong>regularization</strong>的<strong>bias</strong>项处理所有参数的平方和。</p>
<h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>就是让代价函数最小化的算法，因为是从最后往前算误差的，所以叫作反向传播算法，之前的是从第一层开始正向一层一层进行计算，直到最后一层的ℎ𝜃(𝑥)，所以是正向传播算法。<br>$$<br>为了计算偏导数项\frac{∂}{∂Θ_{ij}^{(l)} } J(Θ)，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，<br>$$<br>直到倒数第二层。 以一个例子来说明反向传播算法，参数是这样的<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250714102913373.png" alt="image-20250714102913373" style="zoom:33%;" /></p>
<p>假设我们的训练集只有一个实例(𝑥, 𝑦），我们的神经网络是一个四层的神经网络，其中𝐾 &#x3D; 4， 𝑆𝐿 &#x3D; 4， 𝐿 &#x3D; 4：先利用前向传播算法计算一下输出结果。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250714103257364.png" alt="image-20250714103257364" style="zoom:50%;" /></p>
<p>接下来计算导数项。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250714103443302.png" alt="image-20250714103443302" style="zoom:33%;" />代表第 l 层的第 j 个结点的误差。</p>
<p>由于现在用来举例的只有一个样本，所以误差可以写为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250714103857506.png" alt="image-20250714103857506" style="zoom:33%;" />，向量形式是<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250714103924438.png" alt="image-20250714103924438" style="zoom:33%;" /></p>
<p>当算出最后一层的误差后，就向前计算前面几层的误差。</p>
<p><code>·*</code>代表的是两个向量的对应元素相乘。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250714104343510.png" alt="image-20250714104343510"></p>
<p>没有𝛿(1)，因为第一层是输入层，所以不会存在误差。</p>
<p>一个单元造成的误差是这个单元对下一层的每一个单元造成的误差的总和，所以参数矩阵要转置求这个单元造成的误差和。可以求出偏差单位的误差，但在计算的时候可以不加，造成的影响不大。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250714114449182.png" alt="image-20250714114449182"><br>$$<br>假设λ&#x3D;0，即我们不做任何正则化处理时有：\frac{∂}{∂Θ_{ij}^{(l)} } J(Θ)&#x3D;a_j^{(l)} δ_i^{l+1}<br>$$<br>上面式子中上下标的含义： </p>
<p>𝑙 代表目前所计算的是第几层。</p>
<p>𝑗 代表目前计算层中的激活单元的下标。</p>
<p>𝑖 代表下一层(l+1层)中误差单元的下标，是受到权重矩阵中第𝑖行影响的下一层中的误差单元的下标。</p>
<p>假设有m个训练样本，用<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250714110943856.png" alt="image-20250714110943856" style="zoom:50%;" />表示误差矩阵，第 𝑙 层的第 𝑖 个激活单元受到第 𝑗个参数影响而导致的误差。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250714110323781.png" alt="image-20250714110323781"></p>
<p>公式里的那个1&#x2F;m是乘以后面和的。</p>
<h2 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h2><p>当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。</p>
<p>为了避免这样的问题，我们采取一种叫做<strong>梯度的数值检验方法</strong>。这种方法的思想是通过<strong>估计梯度值</strong>来检验我们计算的导数值是否真的是我们要求的。</p>
<p>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的 𝜃，我们计算出在 𝜃-𝜀 处和 𝜃+𝜀 的代价值（ 𝜀是一个非常小的值，通常选取 0.001，𝜀不要取太小，不然会出现数值问题），然后求两个代价的平均，用以估计在 𝜃处的代价值（用数值的方法计算近似的导数）。下面那张图的𝜃是一个实数。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250714152302979.png" alt="image-20250714152302979"></p>
<p>更普遍的情况，𝜃是一个n维向量，它可能是神经网络参数𝜃(1),𝜃(2),𝜃(3)等的展开形式。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250714152538713.png" alt="image-20250714152538713"></p>
<p>最后我们还需要对通过反向传播方法计算出的偏导数进行检验。根据反向传播算法计算出来的偏导数存储在矩阵<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250714153307330.png" alt="image-20250714153307330" style="zoom:50%;" />中，然后将其与上面的数值计算的方法计算出来的近似的梯度值进行比较，如果误差在几位小数之内就认为反向传播算法的实现是<strong>正确</strong>的，然后在接下来的训练过程中都<strong>不再使用</strong>这个验证程序，因为它的计算量很大，而反向传播是一种更为简单的计算方法。</p>
<p>步骤：DVec就是反向传播算出来的导数值，gradApprox是数值计算算出来的近似梯度值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250714153726453.png" alt="image-20250714153726453"></p>
<p><strong>用数值方法计算导数是用来确定反向传播实现是否正确的方法，但是不止可以用来验证反向传播，也可以用来验证类似的复杂模型的梯度下降算法。</strong></p>
<h2 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h2><p>任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值，算出来的误差值也都是相同的值，之后进行梯度下降后的值也都相同。同理，如果我们初始所有的参数都为一个非 0 的数，结果也是一样的。</p>
<p>为了解决这个问题，神经网络变量的初始化方式采用随机初始化，通常初始参数为正负𝜀之间接近于0的随机值，然后进行反向传播，执行梯度检查，使用梯度下降或者其它优化算法。</p>
<h2 id="综合"><a href="#综合" class="headerlink" title="综合"></a>综合</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250714155124333.png" alt="image-20250714155124333"></p>
<h1 id="应用机器学习的建议"><a href="#应用机器学习的建议" class="headerlink" title="应用机器学习的建议"></a>应用机器学习的建议</h1><h2 id="决定下一步做什么"><a href="#决定下一步做什么" class="headerlink" title="决定下一步做什么"></a>决定下一步做什么</h2><p>当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？</p>
<p>获得更多的训练实例通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</p>
<p>1.尝试减少特征的数量；2.尝试获得更多的特征；3.尝试增加多项式特征；4.尝试减少正则化程度λ； 5.尝试增加正则化程度λ</p>
<p>我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。“诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。</p>
<h2 id="评估一个假设"><a href="#评估一个假设" class="headerlink" title="评估一个假设"></a>评估一个假设</h2><p>如何判断一个假设函数是过拟合的呢？</p>
<p>对于特征变量只有一个的简单例子，可以直接对假设函数h(x)进行画图，但对于特征变量不止一个的这种一般情况，还有像有很多特征变量的问题，想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现。</p>
<p>为了检验算法是否过拟合，我们将数据分成<strong>训练集</strong>和<strong>测试集</strong>，通常用 70%的数据作为训练集，用剩下 30%的数据作为测试集。很重要的一点是训练集和测试集均<strong>要含有各种类型</strong>的数据，通常我们要对数据进行“<strong>洗牌</strong>”，然后再分成训练集和测试集。</p>
<p>在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：</p>
<p> 1.对于线性回归模型，我们利用测试集数据计算代价函数 J</p>
<p>2.对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外，还可以计算误分类的比率（感觉就是准确率），也就是对每一个测试集实例计算<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250721163110688.png" alt="image-20250721163110688" style="zoom:50%;" />，然后对结果求平均。</p>
<h2 id="模型选择和交叉验证集"><a href="#模型选择和交叉验证集" class="headerlink" title="模型选择和交叉验证集"></a>模型选择和交叉验证集</h2><p>假设我们要在 10 个不同次数的二项式模型之间进行选择：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250721163448041.png" alt="image-20250721163448041" style="zoom:50%;" /></p>
<p>虽然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。</p>
<p>即：使用 60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用 20%的数据作为测试集。</p>
<p>模型选择的方法为：</p>
<ol>
<li><p>使用训练集训练出 10 个模型</p>
</li>
<li><p>用 10 个模型分别对<strong>交叉验证集</strong>计算得出交叉验证误差（代价函数的值）</p>
</li>
<li><p>选取代价函数值最小的模型</p>
</li>
<li><p>用步骤 3 中选出的模型对测试集计算得出推广误差（代价函数的值）</p>
</li>
</ol>
<h2 id="诊断偏差和方差"><a href="#诊断偏差和方差" class="headerlink" title="诊断偏差和方差"></a>诊断偏差和方差</h2><p>当一个学习算法的表现不理想时，多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？搞清楚这一点就能判断出现的情况是这两种情况中的哪一种。</p>
<p>高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250721165003029.png" alt="image-20250721165003029"></p>
<p>通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250721165252491.png" alt="image-20250721165252491" style="zoom:50%;" /><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250721165312076.png" alt="image-20250721165312076" style="zoom:50%;" /></p>
<p>对于训练集，当 𝑑 较小时，模型拟合程度更低，误差较大；随着 𝑑 的增长，拟合程度提高，误差减小。</p>
<p>对于交叉验证集，当 𝑑 较小时，模型拟合程度低，误差较大；但是随着 𝑑 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。</p>
<p>交叉验证集误差较大，如何判断是方差还是偏差呢？  根据上面的图表可以知道：</p>
<p>训练集误差和交叉验证集<strong>误差近似</strong>时：偏差&#x2F;欠拟合</p>
<p>交叉验证集误差<strong>远大于</strong>训练集误差时：方差&#x2F;过拟合  </p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250721165430462.png" alt="image-20250721165430462"></p>
<h2 id="正则化和偏差-方差"><a href="#正则化和偏差-方差" class="headerlink" title="正则化和偏差&#x2F;方差"></a>正则化和偏差&#x2F;方差</h2><p>在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是正则化的程度可能会太高或太小了，即我们在选择 λ 的值时也需要思考与刚才选择多项式模型次数类似的问题。  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250721165724178.png" alt="image-20250721165724178"></p>
<p>选择一系列的想要测试的 𝜆 值，通常是 0-10 之间的呈现 2 倍关系的值（如： 0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10共 12 个），同样把数据分为训练集、交叉验证集和测试集。  </p>
<p>选择𝜆的方法为：</p>
<p>1.使用训练集训练出 12 个不同程度正则化的模型</p>
<p>2.用 12 个模型分别对交叉验证集计算的出交叉验证误差</p>
<p>3.选择得出交叉验证误差最小的模型</p>
<p>4.运用步骤 3 中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与 λ 的值绘制在一张图表上：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250721170136816.png" alt="image-20250721170136816"></p>
<p>在训练时，代价函数是有加上正则项的，而在后面计算训练集和交叉验证集的误差时是没有加上正则项的，因为λ越大，会导致正则项在训练时的代价函数中的比例越大，那么在后面不算上正则项时代价函数就会比较大。</p>
<p>当 𝜆 较小时，训练集误差较小（过拟合）而交叉验证集误差较大，这对应着高偏差问题。</p>
<p>随着 𝜆 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加，这对应着高方差问题。</p>
<h2 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h2><p>可以使用学习曲线来判断某一个学习算法是否处于偏差、方差问题，学习曲线是将训练集误差和交叉验证集误差作为<strong>训练集实例数量（ 𝑚）</strong>的函数绘制的图表，所以m一般都是一个常数，但我们需要自行对m进行取值，比如说取10，20，30等，然后绘制出曲线。即，如果我们有 100 行数据，我们从 1 行数据开始，逐渐学习更多行的数据。</p>
<p>绘制学习曲线，先绘制出 J(train)，然后再画出 J(cv)。</p>
<p>当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。在训练数据很少的情况下，即使使用了正则化，拟合的效果仍然会很好。随着训练集样本的增加，平均训练误差是逐渐增大的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250721173115254.png" alt="image-20250721173115254"></p>
<p>当学习算法处于高偏差&#x2F;欠拟合情形时，学习曲线如下，作为例子，用一条直线来适应下面的数据，可以看出，无论训练集有多么大，误差都不会有太大改观：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250721173448391.png" alt="image-20250721173448391"></p>
<p><strong>也就是说在高偏差&#x2F;欠拟合的情况下，增加数据到训练集不一定能有帮助。</strong></p>
<p>当学习算法处于高方差情形时，假设使用一个非常高次的多项式模型，并且<strong>正则化非常小</strong>，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。虽然随着训练样本的增多， J(train)会越来越大，因为训练样本越多时，就越难与训练数据拟合得很好，但总体来说训练集误差还是很小。</p>
<p>因为函数对数据过拟合，所以交叉验证集误差会一直都很大，即便选择了一个比较合适得训练集样本数，所以交叉验证集和训练集误差之间始终会有一段很大的差距。但如果继续增大样本数的话，可以发现这两条线在相互靠近。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250721174433396.png" alt="image-20250721174433396" style="zoom:40%;" />增大样本数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250721174644797.png" alt="image-20250721174644797" style="zoom:60%;" /></p>
<p><strong>也就是说在高方差&#x2F;过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。</strong>  </p>
<h2 id="决定下一步做什么-1"><a href="#决定下一步做什么-1" class="headerlink" title="决定下一步做什么"></a>决定下一步做什么</h2><pre><code class="hljs plaintext">1. 获得更多的训练实例——解决高方差
2. 尝试减少特征的数量——解决高方差
3. 尝试获得更多的特征——解决高偏差
4. 尝试增加多项式特征——解决高偏差
5. 尝试减少正则化程度 λ——解决高偏差
6. 尝试增加正则化程度 λ——解决高方差</code></pre>

<p><strong>神经网络</strong>的方差和偏差：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250721175546856.png" alt="image-20250721175546856"></p>
<p>使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。</p>
<p>通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。</p>
<p>对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。</p>
<h1 id="机器学习系统的设计"><a href="#机器学习系统的设计" class="headerlink" title="机器学习系统的设计"></a>机器学习系统的设计</h1><p>以一个垃圾邮件分类器算法为例进行讨论。</p>
<p>为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量𝑥。我们可以选择一个由 100 个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为 1，不出现为 0），尺寸为 100×1。</p>
<p>为了构建这个分类器算法，我们可以做很多事，例如：</p>
<ol>
<li><p>收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本</p>
</li>
<li><p>基于邮件的路由信息开发一系列复杂的特征</p>
</li>
<li><p>基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理</p>
</li>
<li><p>为探测刻意的拼写错误（把 watch 写成 w4tch）开发复杂的算法</p>
</li>
</ol>
<p>在上面这些选项中，非常难决定应该在哪一项上花费时间和精力，作出明智的选择，比随着感觉走要更好。当我们使用机器学习时，总是可以“头脑风暴”一下，想出一堆方法来试试。</p>
<h2 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h2><p>如果你准备研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是<strong>构建一个简单的算法</strong>，这样你可以很快地实现它。</p>
<p>研究机器学习的问题时，先很快地把结果搞出来，即便运行得不完美，但是也把它运行一遍，最后通过<strong>交叉验证</strong>来检验数据。一旦做完，你可以画出<strong>学习曲线</strong>，通过画出学习曲线，以及<strong>检验误差</strong>，来找出你的算法是否有<strong>高偏差和高方差</strong>的问题，或者别的问题。在这样分析之后，再来决定用<strong>更多的数据</strong>训练，或者加入<strong>更多的特征变量</strong>是否有用。</p>
<p>因为我们并不能提前知道是否需要复杂的特征变量，或者是否需要更多的数据，还是别的什么。提前知道应该做什么，是非常难的，因为缺少证据，缺少学习曲线。因此，很难知道应该把时间花在什么地方来提高算法的表现。但是当实践一个非常简单即便不完美的方法时，就可以通过画出学习曲线来做出进一步的选择。</p>
<p>除了画学习曲线外还有一个方法就是进行误差分析，例如当我们在构造垃圾邮件分类器时，我会看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。</p>
<p>具体一点就是，检验交叉验证集中我们的算法产生错误预测的所有邮件，看：是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。思考怎样能改进分类器。例如，发现是否缺少某些特征，记下这些特征出现的次数。</p>
<p>误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要<strong>尝试不同的模型</strong>，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是<strong>看交叉验证集的误差</strong>。</p>
<p>因此，在构造学习算法的时候，总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次实践新想法的时候，都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。但是通过一个<strong>量化的数值评估</strong>（哪些代码里自己计算的准确度？），你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。</p>
<p><strong>在交叉验证集上来实施误差分析</strong></p>
<h2 id="类偏斜的误差度量"><a href="#类偏斜的误差度量" class="headerlink" title="类偏斜的误差度量"></a>类偏斜的误差度量</h2><p>误差度量值：设定某个实数来评估你的学习算法，并衡量它的表现。</p>
<p>类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。</p>
<p>例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有 0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有 0.5%。然而我们通过训练而得到的神经网络算法却有 1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。</p>
<p>我们将算法预测的结果分成四种情况：</p>
<ol>
<li><p>正确肯定（ True Positive,TP）：预测为真，实际为真</p>
</li>
<li><p>正确否定（ True Negative,TN）：预测为假，实际为假</p>
</li>
<li><p>错误肯定（ False Positive,FP）：预测为真，实际为假</p>
</li>
<li><p>错误否定（ False Negative,FN）：预测为假，实际为真</p>
</li>
</ol>
<p>查准率（Precision）&#x3D;TP&#x2F;(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。</p>
<p>查全率（Recall）&#x3D;TP&#x2F;(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。</p>
<p>对于刚才那个总是预测病人肿瘤为良性的算法，其查全率是 0。</p>
<h2 id="查准率和查全率之间的权衡"><a href="#查准率和查全率之间的权衡" class="headerlink" title="查准率和查全率之间的权衡"></a>查准率和查全率之间的权衡</h2><p>假设，我们的算法输出的结果在 0-1 之间，我们使用阀值 0.5 来预测真和假。</p>
<p>如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比 0.5 更大的阀值，如 0.7， 0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。</p>
<p>如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比 0.5 更小的阀值，如 0.3。</p>
<p>可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同：threshould是临界值的意思</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250721214238005.png" alt="image-20250721214238005" style="zoom:50%;" />

<p>查准率-召回率曲线会有多种形状</p>
<p>有一个帮助我们选择这个阀值的方法。一种方法是计算 F1 值（ F1 Score），其计算公式为：<br>$$<br>F_1 Score:2 \frac{PR}{P+R}<br>$$<br>我们选择使得 F1 值最高的阀值。</p>
<h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="优化目标"><a href="#优化目标" class="headerlink" title="优化目标"></a>优化目标</h2><p>在监督学习中，许多学习算法的性能都非常类似，因此，重要的不是该选择使用学习算法 A 还是学习算法 B，而更重要的是，应用这些算法时，表现情况通常依赖于你的水平。比如：你为学习算法所设计的特征量的选择，以及如何选择正则化参数，诸如此类的事。</p>
<p>与逻辑回归和神经网络相比，支持向量机，或者简称 SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p>
<p>从逻辑回归开始来展示如何一点一点修改来得到本质上的支持向量机。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250723154655767.png" alt="image-20250723154655767"></p>
<p>逻辑回归中一个训练样本所对应代价函数表达式：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250723154803433.png" alt="image-20250723154803433"></p>
<p>当y&#x3D;1时，只有第一项起了作用。对第一项进行修改，得出SVM中将要使用的代价函数（粉色线），由两段直线组成，暂时不用考虑左侧直线的斜率，因为那个不重要，这个将要使用的代价函数是在y&#x3D;1的前提条件下的。新的代价函数叫作cost1(z)</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723155621382.png" alt="image-20250723155621382" style="zoom:50%;" />

<p>当y&#x3D;0时，新的代价函数叫作cost0(z)</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723155656291.png" alt="image-20250723155656291" style="zoom:50%;" />

<p>然后接下来开始构造支持向量机。</p>
<p>这是逻辑回归中所用到的代价函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723155830604.png" alt="image-20250723155830604" style="zoom:35%;" /></p>
<p>对于支持向量机来说，要将里面的两项进行替换：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250723160141378.png" alt="image-20250723160141378"></p>
<p>但实际上，对于支持向量机来说，代价函数的书写会有所不同。首先要去掉1&#x2F;m这一项，去掉之后也会得出同样的theta最优值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250723160335561.png" alt="image-20250723160335561"></p>
<p>因为1&#x2F;𝑚 仅是个常量，因此，你知道在这个最小化问题中，无论前面是否有1&#x2F;𝑚 这一项，最终我所得到的最优值𝜃都是一样的。这里我的意思是，先给你举一个实例，假定有一最小化问题：即要求当<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723160432705.png" alt="image-20250723160432705" style="zoom:40%;" />取得最小值时的𝑢值，这时最小值为：当𝑢 &#x3D; 5时取得最小值。  </p>
<p>现在，如果我们想要将这个目标函数乘上常数 10，这里我的最小化问题就变成了：求使得10 × (𝑢 - 5)2 + 10最小的值𝑢，然而，使得这里最小的𝑢值仍为 5。因此将一些常数乘以你的最小化项，这并不会改变最小化该方程时得到𝑢值。因此，这里我所做的是删去常量𝑚。也相同的，我将目标函数乘上一个常量𝑚，并不会改变取得最小值时的𝜃值。  </p>
<p>用A来表示不包括正则项的部分，也就是训练样本的代价，用B来表示不包括lamda的正则项。这就相当于我们想要最小化𝐴加上正则化参数𝜆乘以𝐵，𝐴 + 𝜆 × 𝐵  ，我们所做的是通过设置不同正则参数𝜆达到优化目的。这样，我们就能够权衡对应的项，即最小化𝐴，是使得训练样本拟合的更好。还是保证正则参数足够小，也即是对于 B 项而言。  </p>
<p>但对于支持向量机，按照惯例，我们将使用一个不同的参数替换这里使用的𝜆来权衡这两项。就是第一项和第二项我们依照惯例使用一个不同的参数称为𝐶，同时改为优化目标， 𝐶 × 𝐴 + 𝐵因此，在逻辑回归中，如果给定𝜆，一个非常大的值，意味着给予 B 更大的权重。而这里，就对应于将𝐶 设定为非常小的值，那么，相应的将会给𝐵比给𝐴更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然你也可以把这里的参数𝐶 考虑成1&#x2F;𝜆，同 1&#x2F;𝜆所扮演的角色相同，并且这两个方程或这两个表达式并不相同，因为𝐶 &#x3D; 1&#x2F;𝜆，但是也并不全是这样，如果当𝐶 &#x3D; 1&#x2F;𝜆时，这两个优化目标应当得到相同的值，相同的最优值 𝜃。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723161453967.png" alt="image-20250723161453967" style="zoom:50%;" /></p>
<p>因此，这就得到了在支持向量机中我们的整个优化目标函数。然后最小化这个目标函数，得到 SVM 学习到的参数𝐶。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250723161626830.png" alt="image-20250723161626830"></p>
<p>有别于逻辑回归输出的概率，在这里，当最小化代价函数得到参数𝜃时，支持向量机会直接预测y的值等于1还是等于0</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250723161853417.png" alt="image-20250723161853417"></p>
<h2 id="大边界的直观理解"><a href="#大边界的直观理解" class="headerlink" title="大边界的直观理解"></a>大边界的直观理解</h2><p>人们有时将支持向量机看作是大间距分类器。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250723173243880.png" alt="image-20250723173243880"></p>
<p>这是支持向量机模型的代价函数，左边时关于𝑧的代价函数cos𝑡1(𝑧)，此函数用于正样本，而右边是关于𝑧的代价函数cos𝑡0(𝑧)，横轴表示𝑧，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本， 𝑦 &#x3D; 1，则只有在𝑧 &gt;&#x3D; 1时，代价函数cos𝑡1(𝑧)才等于 0。</p>
<p>换句话说，如果你有一个正样本，我们会希望𝜃𝑇𝑥&gt;&#x3D;1，反之，如果𝑦 &#x3D; 0，它只有在𝑧 &lt;&#x3D; -1的区间里函数值为 0。</p>
<p>事实上，（可以放入逻辑回归问题中理解）如果有一个正样本𝑦 &#x3D; 1，则其实我们仅仅要求𝜃𝑇𝑥大于等于 0，就能将该样本恰当分出，这是因为如果𝜃𝑇𝑥&gt;0 大的话，我们的模型代价函数值为 0，类似地，如果有一个负样本，则仅需要𝜃𝑇𝑥&lt;&#x3D;0 就会将负例正确分离，但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求𝜃𝑇𝑥&gt;0，我们需要的是比 0 值大很多，比如大于等于 1，我也想分离负例时比 0 小很多，比如我希望它小于等于-1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说<strong>安全的间距因子</strong>。</p>
<p>在支持向量机中，这个因子会导致什么结果。接下来考虑一个特例，将这个常数𝐶设置成一个非常大的值。比如假设𝐶的值为 100000 或者其它非常大的数，然后来观察支持向量机会给出什么结果？</p>
<p>如果 𝐶非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为 0 的最优解（C很大不就相当于斜率很大，那当z位于0和1之间时，那个cost的值就会很大）。因此，让我们尝试在代价项的第一项为 0 的情形下理解该优化问题。</p>
<p>输入一个训练样本标签为𝑦 &#x3D; 1，想令第一项为 0，需要做的是找到一个𝜃，使得𝜃𝑇𝑥 &gt;&#x3D; 1，类似地，对于一个训练样本，标签为𝑦 &#x3D; 0，为了使cos𝑡0(𝑧) 函数的值为 0，我们需要𝜃𝑇𝑥 &lt;&#x3D; -1。因此，现在考虑优化问题。选择参数，使得第一项等于 0，因此这个函数的第一项为 0，因此是𝐶乘以 0 加上二分之一乘以第二项。这里第一项是𝐶乘以 0，因此可以将其删去。</p>
<p>这将遵从以下的约束：<br>$$<br>θ^T x^{(i)}&gt;&#x3D;1，如果 y^{(i)}是等于1 的;θ^T x^{(i)}&lt;&#x3D;-1，如果样本i是一个负样本<br>$$<br>具体而言，如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723174423968.png" alt="image-20250723174423968" style="zoom:50%;" />

<p>粉色和绿色的决策边界仅仅是勉强分开，这些决策边界看起来都不是特别好的选择，支持向量机将会选择这个黑色的决策边界。</p>
<p>黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这条黑线有更大的距离，这个距离叫做间距。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723174612817.png" alt="image-20250723174612817" style="zoom:50%;" />

<p>当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做<strong>支持向量机的间距</strong>，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为<strong>大间距分类器</strong>。</p>
<p>鲁棒性：指模型在面对数据中的噪声、异常值、干扰或环境变化时，仍能保持稳定预测性能的能力。也就是模型的“抗干扰”和“抗压”能力。</p>
<p>支持向量机模型的做法，即努力将正样本和负样本用<strong>最大的间距</strong>分开。</p>
<p>在上面将这个大间距分类器中的正则化因子常数𝐶设置的非常大，因此对这样的一个数据集，也许我们将选择这样的决策界，从而最大间距地分离开正样本和负样本。那么在让代价函数最小化的过程中，我们希望找出在𝑦 &#x3D; 1和𝑦 &#x3D; 0两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723175052807.png" alt="image-20250723175052807" style="zoom:67%;" /></p>
<p>但是，当你使用大间距分类器的时候，你的学习算法会受异常点的影响。比如我们加入一个额外的正样本。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250723175151165.png" alt="image-20250723175151165" style="zoom:50%;" />

<p>在这里，如果加了这个样本，为了将样本用最大间距分开，也许最终会得到一条类似粉色这样的决策界，仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数𝐶，设置的非常大，这事实上正是支持向量机将会做的。</p>
<p>**但如果将 C 设置的不要太大，则最终会得到这条黑线。当𝐶不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。 **</p>
<p>数据如果不是线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会将它们恰当分开。</p>
<p>因此，大间距分类器的描述，仅仅是从直观上给出了<strong>正则化参数𝐶非常大</strong>的情形，同时，要提醒你𝐶的作用类似于1&#x2F;𝜆， 𝜆是我们之前使用过的正则化参数，因此：</p>
<p><strong>𝐶 较大时，相当于 𝜆 较小，可能会导致过拟合，高方差。</strong></p>
<p><strong>𝐶 较小时，相当于 𝜆 较大，可能会导致低拟合，高偏差。</strong></p>
<h2 id="大边界分类背后的数学（-选修）"><a href="#大边界分类背后的数学（-选修）" class="headerlink" title="大边界分类背后的数学（ 选修）"></a>大边界分类背后的数学（ 选修）</h2><p>向量内积：有两个向量u和v，两个都是二维向量<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724201740274.png" alt="image-20250724201740274" style="zoom:40%;" />，<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724201902313.png" alt="image-20250724201902313" style="zoom:50%;" />也叫做向量𝑢和𝑣之间的内积。</p>
<p>除了这种计算方式外还有一种计算方式。</p>
<p>先把这两个向量画出来，向量𝑢即在横轴上，取值为某个𝑢1，而在纵轴上，高度是某个𝑢2作为𝑢的第二个分量。向量𝑢的范数。向量v也按同样的步骤画出来。 ∥𝑢∥表示𝑢的范数，即𝑢的长度，即向量𝑢的欧几里得长度。<br>$$<br>∥u∥&#x3D;\sqrt{(u_1^2+u_2^2 )},这是向量𝑢的长度，它是一个实数。<br>$$<br>计算内积：将向量𝑣投影到向量𝑢上，做一个直角投影，或者说一个 90 度投影将其投影到𝑢上，接下来度量这条红线的长度。称这条红线的长度为𝑝，因此𝑝就是长度，或者说是向量𝑣投影到向量𝑢上的量。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724202521124.png" alt="image-20250724202521124" style="zoom:50%;" /><br>$$<br>公式是u^T v&#x3D;p⬝∥u∥<br>$$<br>因为𝑢𝑇𝑣 &#x3D; 𝑣𝑇𝑢。因此如果你将𝑢和𝑣交换位置，将𝑢投影到𝑣上，而不是将𝑣投影到𝑢上，然后做同样地计算，只是把𝑢和𝑣的位置交换一下，你事实上可以得到同样的结果。申明一点，在这个等式中𝑢的范数是一个实数， 𝑝也是一个实数，因此𝑢𝑇𝑣就是两个实数正常相乘。</p>
<p><strong>𝑝事实上是有符号的，即它可能是正值，也可能是负值。</strong></p>
<p>这种情况下的p就是负值<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724202737933.png" alt="image-20250724202737933" style="zoom:50%;" /></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724203531832.png" alt="image-20250724203531832" style="zoom:67%;" />

<p>这是支持向量机模型中的目标函数，为了让它更容易分析，忽略掉截距，令𝜃0 &#x3D; 0，将特征数𝑛置为 2，因此仅有两个特征𝑥1, 𝑥2，现在来看一下支持向量机的优化目标函数，这个式子可以写作：<br>$$<br>\frac{1}{2} (θ_1^2+θ_2^2 )&#x3D;\frac{1}{2} {(\sqrt{θ_1^2+θ_2^2 })}^2<br>$$<br>括号里面的这一项是向量𝜃的范数，或者说是向量𝜃的长度,因此支持向量机做的全部事情，就是<strong>极小化参数向量𝜃范数的平方，或者说长度的平方</strong>。</p>
<p>深入理解𝜃Tx的含义，𝜃和𝑥(𝑖)就类似于𝑢和𝑣 。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724204552407.png" alt="image-20250724204552407" style="zoom:67%;" /></p>
<p>看这个图，考察一个单一的训练样本，我有一个正样本在这里，用一个叉来表示这个样本𝑥(𝑖)，意思是在水平轴上取值为𝑥1(𝑖)，在竖直轴上取值为𝑥2(𝑖)，然后将参数向量也画上去，那么内积<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724204757937.png" alt="image-20250724204757937" style="zoom:50%;" />将会是什么？</p>
<p>使用之前的计算方式就是将训练样本投影到参数向量𝜃，然后将投影的长度画成红色。𝑝(𝑖)用来表示这是第 𝑖个训练样本在参数向量𝜃上的投影。根据我们之前的内容，𝜃𝑇𝑥(𝑖)将会等于𝑝 乘以向量 𝜃 的长度或范数。这就等于𝜃1 ⋅ 𝑥1(𝑖) + 𝜃2 ⋅ 𝑥2(𝑖)。这两种方式是等价的，都可以用来计算𝜃和𝑥(𝑖)之间的内积。</p>
<p>这里表达的意思是：这个𝜃𝑇𝑥(𝑖) &gt;&#x3D; 1 或者𝜃𝑇𝑥(𝑖) &lt; -1的,约束是可以被𝑝(𝑖) ⋅ ∥𝜃∥ &gt;&#x3D; 1这个约束所代替的。因为𝜃𝑇𝑥(𝑖) &#x3D; 𝑝(𝑖) ⋅ ∥𝜃∥ ，将其写入我们的优化目标。前面说过优化函数可以写为<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724210137118.png" alt="image-20250724210137118" style="zoom:50%;" /></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724205949128.png" alt="image-20250724205949128" style="zoom:67%;" />

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250724211020353.png" alt="image-20250724211020353"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250724211042793.png" alt="image-20250724211042793"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250724211153497.png" alt="image-20250724211153497"></p>
<p>以上就是为什么支持向量机最终会找到大间距分类器的原因。因为它试图<strong>极大化这些𝑝(𝑖)的范数</strong>，它们是训练样本到决策边界的距离。最后一点，我们的推导自始至终使用了这个简化<strong>假设</strong>，就是<strong>参数𝜃0 &#x3D; 0</strong>。</p>
<p>𝜃0 &#x3D; 0的意思是我们让决策界通过原点。如果你令𝜃0不是 0 的话，含义就是你希望决策界不通过原点。即便𝜃0不等于 0，支持向量机仍然会找到正样本和负样本之间的大间距分隔。</p>
<h2 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h2><p>给 定 一 个 训 练 实 例 𝑥 ， 我 们 利 用 𝑥 的 各 个 特 征 与 我 们 预 先 选 定 的 地 标(landmarks)𝑙(1), 𝑙(2), 𝑙(3)的近似程度来选取新的特征𝑓1, 𝑓2, 𝑓3。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724214443211.png" alt="image-20250724214443211" style="zoom:67%;" /><br>$$<br>例如：f_1&#x3D;similarity(x,l^{(1)})&#x3D;e^{(-\frac{∥x-l^{(1)} ∥^2}{2σ^2 })}<br>$$</p>
<p>$$<br>其中∥x-l^{(1)} ∥^2&#x3D;∑_{j&#x3D;1}^n(x_j-l_j^{(1)})^2，为实例𝑥中所有特征与地标𝑙(1)之间的距离的和。<br>$$</p>
<p>上例中的𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑥, 𝑙(1))就是核函数，具体而言，这里是一个高斯核函数。这个函数与正态分布没什么实际上的关系，只是看上去像而已。</p>
<p>这些地标的作用是什么？如果一个训练实例𝑥与地标𝐿之间的距离近似于 0，则新特征 𝑓近似于<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724214955023.png" alt="image-20250724214955023" style="zoom:50%;" />，如果训练实例𝑥与地标𝐿之间距离较远，则𝑓近似于<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724215029521.png" alt="image-20250724215029521" style="zoom:50%;" />。</p>
<p>假设我们的训练实例含有两个特征[𝑥1 𝑥2]，给定地标𝑙(1)与不同的𝜎值，见下图：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724215112385.png" alt="image-20250724215112385" style="zoom:60%;" /></p>
<p>图中水平面的坐标为 𝑥1， 𝑥2而垂直坐标轴代表𝑓。可以看出，只有当𝑥与𝑙(1)重合时𝑓才具有最大值。随着𝑥的改变𝑓值改变的速率受到𝜎2的控制。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250724215153621.png" alt="image-20250724215153621"></p>
<p><strong>如何选择地标？</strong></p>
<p>通常是根据训练集的数量选择地标的数量，即如果训练集中有𝑚个实例，则选取𝑚个地标，并且令:𝑙(1) &#x3D; 𝑥(1), 𝑙(2) &#x3D; 𝑥(2), . . . . . , 𝑙(𝑚) &#x3D; 𝑥(𝑚)。这样做的好处在于：现在得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724220214226.png" alt="image-20250724220214226" style="zoom:50%;" />

<p>下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为：</p>
<p>给定𝑥，计算新特征𝑓，当𝜃𝑇𝑓 &gt;&#x3D; 0 时，预测 𝑦 &#x3D; 1，否则反之。</p>
<p>相应地修改代价函数为：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724220321510.png" alt="image-20250724220321510" style="zoom:67%;" />，在计算这个的时候还需要做一些调整，用<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724220419238.png" alt="image-20250724220419238" style="zoom:50%;" />代替<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250724220439325.png" alt="image-20250724220439325" style="zoom:50%;" />，其中𝑀是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。</p>
<p>理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 𝑀来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。</p>
<p>逻辑回归的核心是建模样本术语某个类别的概率，输出是一个概率值（0到1之间），它本质上是概率模型。SVM的目标是找到一个几何间隔最大的超平面进行硬分类（每个样本只能被明确划分到一个类别，逻辑回归是软分类，最终分类取概率最高的类别），输出的是确定的类别标签（如+1&#x2F;-1），本质上是几何间隔最大化模型。</p>
<p>可以直接使用现有的软件包来最小化支持向量机的代价函数，但在使用这些软件包最小化我们的代价函数之前，我们通常需要<strong>编写核函数</strong>，并且如果我们使用高斯核函数，那么在使用之前进行<strong>特征缩放</strong>是非常必要的。</p>
<p>另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Odegaard</div><div class="post-copyright__author_desc"></div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/')">机器学习</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=机器学习&amp;url=http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">coygOdegaard</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"></div></div></div><div class="post_share"><div class="social-share" data-image="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/05/05/%E7%9C%9F%E9%A2%98%E6%8A%80%E5%B7%A7/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">真题技巧</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/08/python%E6%9A%91%E5%81%87/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">python暑假</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description"></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.</span> <span class="toc-text">监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.</span> <span class="toc-text">无监督学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.</span> <span class="toc-text">单变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA"><span class="toc-number">2.1.</span> <span class="toc-text">模型表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">2.3.</span> <span class="toc-text">代价函数的直观理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.4.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">2.5.</span> <span class="toc-text">梯度下降的直观理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.6.</span> <span class="toc-text">梯度下降的线性回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.</span> <span class="toc-text">多变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81"><span class="toc-number">3.1.</span> <span class="toc-text">多维特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">3.2.</span> <span class="toc-text">多变量梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B5-1-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">3.3.</span> <span class="toc-text">梯度下降法实践 1-特征缩放</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B5-2-%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">3.4.</span> <span class="toc-text">梯度下降法实践 2-学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-number">3.5.</span> <span class="toc-text">特征和多项式回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">3.6.</span> <span class="toc-text">正规方程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">4.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">4.1.</span> <span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%87%E8%AF%B4%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.2.</span> <span class="toc-text">假说表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C"><span class="toc-number">4.3.</span> <span class="toc-text">判定边界</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-1"><span class="toc-number">4.4.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96"><span class="toc-number">4.5.</span> <span class="toc-text">高级优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BB%E6%89%BE%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="toc-number">4.6.</span> <span class="toc-text">寻找决策边界</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E9%80%A0%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81"><span class="toc-number">4.7.</span> <span class="toc-text">构造多项式特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%EF%BC%9A%E4%B8%80%E5%AF%B9%E5%A4%9A"><span class="toc-number">4.8.</span> <span class="toc-text">多类别分类：一对多</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">5.1.</span> <span class="toc-text">过拟合的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-2"><span class="toc-number">5.2.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.3.</span> <span class="toc-text">正则化线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.4.</span> <span class="toc-text">正则化的逻辑回归模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%A1%A8%E8%BF%B0"><span class="toc-number">6.</span> <span class="toc-text">神经网络：表述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%81%87%E8%AE%BE"><span class="toc-number">6.1.</span> <span class="toc-text">非线性假设</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA-1"><span class="toc-number">6.2.</span> <span class="toc-text">模型表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%92%8C%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">6.3.</span> <span class="toc-text">特征和直观理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="toc-number">6.4.</span> <span class="toc-text">多类分类</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">7.</span> <span class="toc-text">神经网络的学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-3"><span class="toc-number">7.1.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95"><span class="toc-number">7.2.</span> <span class="toc-text">反向传播算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C"><span class="toc-number">7.3.</span> <span class="toc-text">梯度检验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">7.4.</span> <span class="toc-text">随机初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%BC%E5%90%88"><span class="toc-number">7.5.</span> <span class="toc-text">综合</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BB%BA%E8%AE%AE"><span class="toc-number">8.</span> <span class="toc-text">应用机器学习的建议</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E5%AE%9A%E4%B8%8B%E4%B8%80%E6%AD%A5%E5%81%9A%E4%BB%80%E4%B9%88"><span class="toc-number">8.1.</span> <span class="toc-text">决定下一步做什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E4%B8%80%E4%B8%AA%E5%81%87%E8%AE%BE"><span class="toc-number">8.2.</span> <span class="toc-text">评估一个假设</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%92%8C%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="toc-number">8.3.</span> <span class="toc-text">模型选择和交叉验证集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8A%E6%96%AD%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="toc-number">8.4.</span> <span class="toc-text">诊断偏差和方差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE"><span class="toc-number">8.5.</span> <span class="toc-text">正则化和偏差&#x2F;方差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF"><span class="toc-number">8.6.</span> <span class="toc-text">学习曲线</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E5%AE%9A%E4%B8%8B%E4%B8%80%E6%AD%A5%E5%81%9A%E4%BB%80%E4%B9%88-1"><span class="toc-number">8.7.</span> <span class="toc-text">决定下一步做什么</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">9.</span> <span class="toc-text">机器学习系统的设计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E5%88%86%E6%9E%90"><span class="toc-number">9.1.</span> <span class="toc-text">误差分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B1%BB%E5%81%8F%E6%96%9C%E7%9A%84%E8%AF%AF%E5%B7%AE%E5%BA%A6%E9%87%8F"><span class="toc-number">9.2.</span> <span class="toc-text">类偏斜的误差度量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9F%A5%E5%87%86%E7%8E%87%E5%92%8C%E6%9F%A5%E5%85%A8%E7%8E%87%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9D%83%E8%A1%A1"><span class="toc-number">9.3.</span> <span class="toc-text">查准率和查全率之间的权衡</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">10.</span> <span class="toc-text">支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-number">10.1.</span> <span class="toc-text">优化目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E8%BE%B9%E7%95%8C%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">10.2.</span> <span class="toc-text">大边界的直观理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E8%BE%B9%E7%95%8C%E5%88%86%E7%B1%BB%E8%83%8C%E5%90%8E%E7%9A%84%E6%95%B0%E5%AD%A6%EF%BC%88-%E9%80%89%E4%BF%AE%EF%BC%89"><span class="toc-number">10.3.</span> <span class="toc-text">大边界分类背后的数学（ 选修）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">10.4.</span> <span class="toc-text">核函数</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/08/python%E6%9A%91%E5%81%87/" title="python暑假">python暑假</a><time datetime="2025-07-08T01:52:38.000Z" title="发表于 2025-07-08 09:52:38">2025-07-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="机器学习">机器学习</a><time datetime="2025-05-08T12:23:38.000Z" title="发表于 2025-05-08 20:23:38">2025-05-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/05/%E7%9C%9F%E9%A2%98%E6%8A%80%E5%B7%A7/" title="真题技巧">真题技巧</a><time datetime="2025-05-05T11:30:32.000Z" title="发表于 2025-05-05 19:30:32">2025-05-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/25/%E8%A1%A5%E5%85%85/" title="无题">无题</a><time datetime="2025-04-25T03:23:01.478Z" title="发表于 2025-04-25 11:23:01">2025-04-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/21/%E7%AE%97%E6%B3%95/" title="算法">算法</a><time datetime="2025-01-21T02:01:50.000Z" title="发表于 2025-01-21 10:01:50">2025-01-21</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2025 By <a class="footer-bar-link" href="/" title="Odegaard" target="_blank">Odegaard</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">20</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">0</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 0.88rem;">大数据<sup>1</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 0.88rem;">算法<sup>3</sup></a><a href="/tags/%E8%AF%AD%E8%A8%80/" style="font-size: 0.88rem;">语言<sup>3</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 Odegaard 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>