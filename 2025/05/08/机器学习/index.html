<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>机器学习 | coygOdegaard</title><meta name="author" content="Odegaard"><meta name="copyright" content="Odegaard"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="机器学习"><meta name="application-name" content="机器学习"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="机器学习"><meta property="og:url" content="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.html"><meta property="og:site_name" content="coygOdegaard"><meta property="og:description" content="引言机器学习是什么 目前存在几种不同类型的学习算法。主要的两种类型被我们称之为监督学习和无监督学习。监督学习这个想法是指，我们将教计算机如何去完成任务，而在无监督学习中，我们打算让它自己进行学习。 监督学习监督学习指的就是我们给学习算法一个数据集，这个数据集由“正确答案”组成。在房价的例子中，我们给"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="Odegaard"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="引言机器学习是什么 目前存在几种不同类型的学习算法。主要的两种类型被我们称之为监督学习和无监督学习。监督学习这个想法是指，我们将教计算机如何去完成任务，而在无监督学习中，我们打算让它自己进行学习。 监督学习监督学习指的就是我们给学习算法一个数据集，这个数据集由“正确答案”组成。在房价的例子中，我们给"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2023/09/03/125766904/ee23df8517f3c3e3efc4145658269c06_5714860933110284659.png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: undefined,
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: Odegaard","link":"链接: ","source":"来源: coygOdegaard","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: 'coygOdegaard',
  title: '机器学习',
  postAI: '',
  pageFillDescription: '引言, 机器学习是什么, 监督学习, 无监督学习, 单变量线性回归, 模型表示, 代价函数, 代价函数的直观理解, 梯度下降, 梯度下降的直观理解, 梯度下降的线性回归, 多变量线性回归, 多维特征, 多变量梯度下降, 梯度下降法实践 1-特征缩放, 梯度下降法实践 2-学习率, 特征和多项式回归, 正规方程, 逻辑回归, 分类问题, 假说表示, 判定边界, 代价函数, 高级优化, 寻找决策边界, 构造多项式特征, 多类别分类：一对多, 正则化, 过拟合的问题, 代价函数, 正则化线性回归, 正则化的逻辑回归模型, 神经网络：表述, 非线性假设, 模型表示引言机器学习是什么目前存在几种不同类型的学习算法主要的两种类型被我们称之为监督学习和无监督学习监督学习这个想法是指我们将教计算机如何去完成任务而在无监督学习中我们打算让它自己进行学习监督学习监督学习指的就是我们给学习算法一个数据集这个数据集由正确答案组成在房价的例子中我们给了一系列房子的数据我们给定数据集中每个样本的正确价格即它们实际的售价然后运用学习算法算出更多的正确答案回归问题试着推测出一个连续值的结果下面的房子例子就是回归问题要推测的结果就是房子的价格回归这个词的意思是我们在试着推测出这一系列连续值属性分类问题分类指的是我们试着推测出离散的输出值或良性或恶性感觉就是判断给出的数据属于哪一类在这个例子中只有一个特征就是肿瘤的尺寸在其它一些机器学习问题中可能会遇到不止一种特征举个例子我们不仅知道肿瘤的尺寸还知道对应患者的年龄在其他机器学习问题中通常有更多的特征比如肿块密度肿瘤细胞尺寸的一致性和形状的一致性等等还有一些其他的特征之后会讲一个算法叫支持向量机里面有一个巧妙的数学技巧能让计算机处理无限多个特征回归问题和分类问题都属于监督学习其基本思想是数据集中的每个样本都有相应的正确答案再根据这些样本作出预测就像房子和肿瘤的例子中做的那样回归问题即通过回归来推出一个连续的输出分类问题其目标是推出一组离散的结果无监督学习不同于监督学习的数据的样子即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签所以我们已知数据集却不知如何处理也未告知每个数据点是什么任何信息都不知道只知道是一个数据集这个图是上面肿瘤的例子代表良性代表恶性在监督学习中有这种标志说明是什么情况但在无监督学习中没有标志只是数据针对数据集无监督学习能判断出数据有两个不同的聚集簇这是一个那是另一个二者不同是的无监督学习算法可能会把这些数据分成两个不同的簇这个就叫做聚类算法无监督学习就是我们没法提前告知算法一些信息就是这里是有一堆数据我不知道数据里面有什么我不知道谁是什么类型我甚至不知道人们有哪些不同的类型这些类型又是什么但你能自动地找到数据中的结构吗就是说你要自动地聚类那些个体到各个类我没法提前知道哪些是哪些因为我们没有给算法正确答案来回应数据集中的数据所以这就是无监督学习上面的都是聚类的例子聚类只是无监督学习的一种接下来介绍的鸡尾酒宴问题属于无监督学习中的盲源分离问题可能在一个这样的鸡尾酒宴中的两个人他俩同时都在说话假设现在是在个有些小的鸡尾酒宴中我们放两个麦克风在房间中因为这些麦克风在两个地方离说话人的距离不同每个麦克风记录下不同的声音虽然是同样的两个说话人听起来像是两份录音被叠加到一起或是被归结到一起产生了我们现在的这些录音另外这个算法还会区分出两个音频资源这两个可以合成或合并成之前的录音实际上鸡尾酒算法的第一个输出结果是第二个输出是这样第一个输出代表分离出的第一个声源第二个输出代表分离出的第二个声源这里的数字序列可能是对分离后信号的简化表示实际应用中输出是时间序列信号如音频波形每个数字可能代表某个时间点的信号强度或特征无需去深度思考单变量线性回归模型表示监督学习的第一个例子预测住房价格的我们要使用一个数据集数据集包含俄勒冈州波特兰市的住房价格在这里我要根据不同房屋尺寸所售出的价格画出我的数据集比方说如果你朋友的房子是平方尺大小你要告诉他们这房子能卖多少钱它被称作监督学习是因为对于每个数据来说我们给出了正确的答案即告诉我们根据我们的数据来说房子实际的价格是多少而且更具体来说这是一个回归问题回归一词指的是我们根据之前的数据预测出一个准确的输出值对于这个例子就是价格在监督学习中我们有一个数据集这个数据集被称训练集在整个课程中用小写的来表示训练样本的数目假如上面房子的回归问题的训练集如下表所示将训练集喂给我们的学习算法进而学习得到一个假设然后将我们要预测的房屋的尺寸作为输入变量输入给预测出该房屋的交易价格作为输出变量输出为结果表示的是一个函数由学习算法根据训练集输出输入是房屋尺寸大小输出的是房子价格的一种可能表达方式为因为只含有一个特征输入变量因此这样的问题叫作单变量线性回归问题代价函数有一个像这样的训练集代表了训练样本的数量比如而我们的假设函数也就是用来进行预测的函数是这样的线性函数形式接下来为我们的模型选择合适的参数和在房价问题这个例子中便是直线的斜率和在轴上的截距我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度模型所预测的值与训练集中实际值之间的差距下图中蓝线所指就是建模误差目标便是选择出可以使得建模误差的平方和能够最小的模型参数即使得代价函数最小绘制一个等高线图三个坐标分别为和和可以看出在三维空间中存在一个使得最小的点代价函数也被称作平方误差函数有时也被称为平方误差代价函数代价函数是解决回归问题最常用的手段代价函数的直观理解代价函数是用来干嘛的我们为什么要用它为了便于理解使代表的是训练集中的数据的参数是的参数是上图可以看出当时代价函数接下来时当时等于时对于每个的值都对应着一个假设函数的值或者一条直线并且根据每个不同的我们都可以得到一个不同的的值梯度下降梯度下降是一个用来求函数最小值的算法我们将使用梯度下降算法来求出代价函数的最小值梯度下降背后的思想是开始时我们随机选择一个参数的组合计算代价函数然后我们寻找下一个能让代价函数值下降最多的参数组合我们持续这么做直到到到一个局部最小值因为我们并没有尝试完所有的参数组合所以不能确定我们得到的局部最小值是否便是全局最小值选择不同的初始参数组合可能会找到不同的局部最小值这个算法时怎么工作的可以这样想想象一下你正站立在山的一点上在梯度下降算法中我们要做的就是旋转度看看我们的周围哪个方向可以最快下山来到山坡上我们站在山坡上的一点你看一下周围你会发现最佳的下山方向你再看看周围然后再一次想想我应该从什么方向下山然后你按照自己的判断又迈出一步重复上面的步骤从这个新的点你环顾四周并决定从什么方向将会最快下山然后又迈进了一小步并依此类推直到你接近局部最低点的位置批量梯度下降算法的公式为上面那行英语的意思是反复用这个公式直到收敛其中是学习率它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大在批量梯度下降中我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数符号的意思是赋值这是一个赋值运算符单独的代表的是比较运算符梯度下降中我们要同时更新和当和时会产生更新所以你将更新和记住要同时更新不能先更新一个再更新另一个先更新其中一个的话会导致接下来算出的微分项的值出现变换因为其中一个值变了梯度下降的直观理解梯度下降算法描述对赋值使得按梯度下降最快方向进行一直迭代下去最终得到局部最小值其中是学习率它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大求导的目的基本上可以说取这个红点的切线现在这条线有一个正斜率也就是说它有正导数因此我得到的新的更新后等于减去一个正数乘以如果太小或太大会出现什么情况如果太小了即我的学习速率太小结果就是只能这样像小宝宝一样一点点地挪动去努力接近最低点这样就需要很多步才能到达最低点所以如果太小的话可能会很慢因为它会一点点挪动它会需要很多步才能到达全局最低点如果太大那么梯度下降法可能会越过最低点甚至可能无法收敛下一次迭代又移动了一大步越过一次又越过一次一次次越过最低点直到你发现实际上离最低点越来越远所以如果太大它会导致无法收敛甚至发散假设将初始化在局部最低点因为它已经在一个局部的最优处或局部最低点结果是局部最优点的导数将等于零使得不再改变因此如果你的参数已经处于局部最低点那么梯度下降法更新其实什么都没做它不会改变参数的值这也解释了为什么即使学习速率保持不变时梯度下降也可以收敛到局部最低点在梯度下降法中当我们接近局部最低点时梯度下降法会自动采取更小的幅度这是因为当我们接近局部最低点时很显然在局部最低时导数等于零所以当我们接近局部最低时导数值会自动变得越来越小所以梯度下降将自动采取较小的幅度这就是梯度下降的做法所以实际上没有必要再另外减小可以用梯度下降算法来最小化任何代价函数不只是线性回归中的代价函数梯度下降的线性回归用梯度下降算法并将其应用于具体的拟合直线的线性回归算法里先计算微分项所以算法会被改写为不断重复直到收敛记住和要同时更新使用梯度下降算法是因为它更容易到达局部最小值而根据初始化的不同会得到不同的局部最优解但是事实证明用于线性回归的代价函数总是一个弓形样子的函数叫作凸函数这种函数没有局部最优解只有一个全局最优解一般来说初始化参数的时候都设为刚刚使用的算法有时也称为批量梯度下降批量梯度下降指的是在梯度下降的每一步中我们都用到了所有的训练样本在梯度下降中在计算微分求导项时我们需要进行求和运算所以在每一个单独的梯度下降中我们最终都要计算这样一个东西这个项需要对所有个训练样本求和多变量线性回归多维特征对房价模型增加更多的特征例如房间数楼层等构成一个含有多个变量的模型模型中的特征为这上面的公式是的原因是上面的是一列形状是里面的数据的形状是所以里面的公式是具体的情况要具体分析记住基本公式参数乘以变量多变量梯度下降在多变量线性回归中的代价函数这个代价函数是所有建模误差的平方和即其中多变量线性回归的批量梯度下降算法为即求导得跟前面单变量的公式没有什么大变化就是求导后需要计算的变多了计算代价函数的代码如下梯度下降法实践特征缩放在我们面对多维特征问题的时候我们要保证这些特征都具有相近的尺度这将帮助梯度下降算法更快地收敛以房价问题为例假设我们使用两个特征房屋的尺寸和房间的数量尺寸的值为平方英尺而房间数量的值则是以两个参数分别为横纵坐标绘制代价函数的等高线图在这里先忽略能看出图像会显得很扁梯度下降算法需要非常多次的迭代才能收敛解决的方法是尝试将所有特征的尺度都尽量缩放到到之间这也是在做特征缩放时的通常目的但其实并不严格要求必须是到在这些附近都可以重点是将范围靠近到所以如果有一个特征也就是变量的范围是到的话得对其进行扩展一般在到到都是可以的除了将特征除以它的最大值外还可以进行一种叫作均值归一化的工作包括将原来的变量值减去平均值除以最大值最小值一般用这个就足够了其中是平均值是标准差梯度下降法实践学习率如何确定梯度下降算法在正常工作画图表梯度下降算法的每次迭代受到学习率的影响如果学习率过小则达到收敛所需的迭代次数会非常高如果学习率过大每次迭代可能不会减小代价函数可能会越过局部最小值导致无法收敛通常可以考虑尝试些学习率特征和多项式回归多项式回归可以使用线性回归的方式来拟合非常复杂的函数或者是非线性函数以预测房价模型为例在线性回归模型中你可以选择提供的特征作为特征也可以选择自己创建一个新的特征哎下面的例子中题目给了临街宽度和纵深两个特征我们也可以自己创建一个特征面积这样子可以简化线性回归模型得到一个更好的模型线性回归并不适用于所有数据有时我们需要曲线来适应我们的数据二次方或者三次方模型另外我们可以令从而将模型转化为线性回归模型由于次方的存在导致参数范围被扩大了很多所以在运行梯度下降算法前必须进行特征缩放除了上面给出的这一种还有一种是开平方通过不同的参数形式最后的曲线也会有所不同正规方程到目前为止我们都在使用梯度下降算法但是对于某些线性回归问题正规方程方法是更好的解决方案如运用正规方程方法求解参数对于那些不可逆的矩阵通常是因为特征之间不独立如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征也有可能是特征数量大于训练集的数量正规方程方法是不能用的只要特征变量的数目并不大标准方程是一个很好的计算参数的替代方法具体地说只要特征变量数量小于一万通常使用标准方程法而不使用梯度下降法等价于注意这里返回的不是一个数而是一个元组逻辑回归分类问题在分类问题中要预测的变量是离散的值在分类问题中我们尝试预测的是结果是否属于某一个类例如正确或错误从二元的分类问题开始我们将因变量可能属于的两个类分别称为负向类和正向类则因变量或其中表示负向类表示正向类逻辑回归算法的性质是它的输出值永远在到之间逻辑回归算法是一个分类算法适用于取离散的值的情况下假说表示为什么线性回归算法不适用于分类问题根据线性回归模型我们只能预测连续的值然而对于分类问题例子是肿瘤分类我们需要输出或我们可以预测当时预测当时预测没有极端数据出现的时候使用线性回归算法看着也可以但一旦极端数据出现整体的判断标准就会被破坏有极端数据出现所以线性回归模型并不适用于分类问题逻辑回归模型的假设是其中代表特征向量代表逻辑函数是一个常用的逻辑函数形函数公式为这个就是线性回归模型的结果这里的参数向量是行列的的数据是一列一列的所以逻辑回归模型是对线性回归模型的值进行处理的意思是对于给定的输入变量根据选择的参数计算输出变量的可能性如果对于给定的通过已经确定的参数计算得出则表示有的几率为正向类相应地为负向类的几率为逻辑回归的本质逻辑回归是一种线性分类模型它通过一个线性方程例如将输入特征如测试和测试的结果映射到一个概率值通过函数决策边界即区分接受抛弃的阈值是线性的比如一条直线在二维特征空间中判定边界根据逻辑回归模型的这个图我们知道当时时时又即时预测时预测对于上面那个模型我们可以很明显地看出是一条直线将预测结果分成两部分又两个例子可以看出我们要根据分界线的形状来判断我们应该使用的分界线函数是什么代价函数对于线性回归模型定义的代价函数是所有模型误差的平方和要是将逻辑回归模型的函数代入到这个代价函数中得到的代价函数将是一个非凸函数这意味着代价函数会有许多局部最小值这将影响梯度下降算法寻找全局最小值定义代价函数为的取值范围在这样构建的函数的特点是当实际的且也为时误差为当但不为时误差随着变小而变大当实际的且也为时代价为当但不为时误差随着的变大而变大将函数进行简化就是用一个表达式表达出来如下代入代价函数为函数在上面定义了梯度下降算法的公式和前面的一样记住同时更新所有参数通过观察梯度下降算法的式子可以发现这个式子和之前线性回归的梯度下降算法的式子是一样的但的式子是不同的特征缩放的技巧也适用于逻辑回归高级优化共轭梯度法变尺度法和限制变尺度法就是一些更高级的优化算法它们需要有一种方法来计算以及需要一种方法计算导数项然后使用比梯度下降更复杂的算法来最小化代价函数这三种算法的具体细节可以不用取探究因为过于复杂使用这其中任何一个算法通常不需要手动选择学习率所以对于这些算法的一种思路是给出计算导数项和代价函数的方法你可以认为算法有一个智能的内部循环而且事实上他们确实有一个智能的内部循环称为线性搜索算法它可以自动尝试不同的学习速率并自动选择一个好的学习速率因此它甚至可以为每次迭代选择不同的学习速率最好不要使用这些算法除非你是数值计算方面的专家如何使用这些算法这些算法适合在很大的机器学习问题中使用在中利用的是中的函数这是一个使用截断牛顿法寻找局部最小值的优化函数特别适用于有界约束的优化问题寻找决策边界所以中的寻找决策边界会除以第三个参数值构造多项式特征由上图可知其中没有线性决策界限来良好的分开两类数据原数据有两个特征和可以明显看出就这两个特征无法较好拟合这些数据所以要构造从原始特征的多项式中得到的特征即通过数学变换将原始特征和扩展为一组新特征这些新特征是原始特征的高阶多项式组合例如平方平方立方等然后在这些新特征上训练逻辑回归模型为什么能解决非线性问题尽管逻辑回归本身是线性的但通过添加非线性特征如平方项或交互项模型在扩展后的高维特征空间中学习到的决策边界仍然是线性的但这个边界在原始特征空间中会呈现为曲线椭圆或其他非线性形状这相当于给模型添加了非线性能力而不改变其核心算法首先要选择阶数阶数决定了多项式的复杂性从二阶开始通常足够处理大多数非线性问题然后根据模型性能调整平方项捕捉单个测试的非线性效应交互项捕捉两个测试的联合效应由于选择了高阶数的模型为了避免过拟合通常还要进行正则化操作多类别分类一对多如何使用逻辑回归来解决多类别分类问题之前的二元分类问题的图和现在的多类分类问题的图用种不同的符号来代表个类别问题就是给出个类型的数据集如何得到一个学习算法来进行分类呢面对二元分类问题可以使用逻辑回归也可以将数据集一分为二为正类和负类而一对多的分类思想我们可以将其用在多类分类问题上这个方法也被称为一对余方法现在我们有一个训练集好比上图表示的有个类别我们用三角形表示方框表示叉叉表示使用一个训练集将三元分类问题转化为三个二元分类问题先从用三角形代表的类别开始实际上我们可以创建一个新的伪训练集类型和类型定为负类类型设定为正类我们创建一个新的训练集如下图所示的那样我们要拟合出一个合适的分类器这里的三角形是正样本而圆形代表负样本可以这样想设置三角形的值为圆形的值为下面可以训练一个标准的逻辑回归分类器这样就得到一个边界选择出哪一个分类器是可信度最高效果最好的那么就可认为得到一个正确的分类无论值是多少我们都有最高的概率值我们预测就是那个值最后我们得到一系列的模型简记为其中正则化过拟合的问题就是过于强调拟合原始数据第一个模型是一个线性模型欠拟合不能很好地适应训练集第三个模型是一个四次方的模型过于强调拟合原始数据而丢失了算法的本质预测新数据可以看出若给出一个新的值使之预测它将表现的很差是过拟合虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好分类问题中也存在这样的问题以多项式理解的次数越高拟合的越好但相应的预测的能力就可能变差如何处理过拟合问题丢弃一些不能帮助我们正确预测的特征可以是手工选择保留哪些特征或者使用一些模型选择的算法来帮忙例如正则化保留所有的特征但是减少参数的大小代价函数在上面过拟合的回归问题中有以下模型正是高次项导致了过拟合的产生所以如果能让这些高次项的系数接近于的话那就能很好的拟合了所以我们要做的就是在一定程度上减小这些参数的值这就是正则化的基本方法我们决定要减少和的大小我们要做的便是修改代价函数在其中和设置一点惩罚这样做的话在尝试最小化代价时也会将这个惩罚纳入考虑中并最终导致选择较小一些的和惩罚就是在代价函数中使和的占比变高使得在最小化代价函数时也会更多地考虑这两个参数修改后的代价函数假如有非常多的特征我们并不知道其中哪些特征要惩罚那么就对所有的特征进行惩罚并且让代价函数最优化的软件来选择这些惩罚的程度这样的结果是得到了一个较为简单的能防止过拟合问题的假设其中又称为正则化参数根据惯例我们不对进行惩罚经过正则化处理的模型与原模型的可能对比如下图所示如果选择的正则化参数过大则会把所有的参数都最小化了导致模型变成也就是上图中红色直线所示的情况造成欠拟合正则化线性回归正则化线性回归的代价函数是由于没有进行正则化所以梯度下降算法将会分成两种情况对第二个式子进行调整可以得到可以看出正则化线性回归的梯度下降算法的变化在于每次都在原有算法更新规则的基础上令值减少了一个额外的值利用正规方程来求解正则化线性回归模型图中的矩阵尺寸为因为不算还有个特征正则化的逻辑回归模型这是正则化的逻辑回归的代价函数给代价函数增加一个正则化的表达式得到代价函数这边的函数是函数不参与其中的任何一个正则化接下来的课程中我们将学习一个非常强大的非线性分类器无论是线性回归问题还是逻辑回归问题都可以构造多项式来解决你将逐渐发现还有更强大的非线性分类器可以用来解决多项式回归问题神经网络表述非线性假设无论是线性回归还是逻辑回归都有这样一个缺点即当特征太多时计算的负荷会非常大使用非线性的多项式项能够帮助我们建立更好的分类模型但相应的我们要计算的特征数会大大增多普通的逻辑回归模型不能有效地处理这么多的特征这时候就需要神经网络模型表示神经网络模型建立在很多神经元之上每一个神经元又是一个个学习模型这些神经元也叫激活单元采纳一些特征作为输出并且根据本身的模型提供一个输出以逻辑回归模型作为学习模型的神经元示例解读上面的黄圈看作是神经元左边的蓝圈和黄圈的连线看作是输入树突黄圈右边的线看作是输出轴突通过树突传递一些信息然后神经元做一些计算然后通过轴突输出计算结果这个图表表示的是对的计算而是函数是输入结点额外的结点被称为偏置单位因为总是等于可画可不画根据具体情况来在神经网络中参数又可被称为权重上面的一个小黄圈代表一个单一的神经元而神经网络是不同的神经元组合在一起的集合第一层叫作输入层在这一层输入特征项最后一层叫作输出层因为这一层的神经元输出假设的最终计算结果中间的一层称作隐藏层神经网络中可以有不止一个隐藏层非输出层和输入层的都叫做隐藏层在隐藏层出现的蓝色圈被称作偏置单位它的值永远是下面的图有个输入单元和个隐藏单元每一个都是由上一层所有的和每一个所对应的参数决定的这样从左到右的算法称为前向传播算法不会等于因为要注意偏置单位的添加如果我们暂时只看第二层和第三层的话可以发现其实神经网络就像是只不过我们把中的输入向量变成了中间层的即特征项是作为输入的函数来学习的所以在神经网络中它没有使用输入特征来训练逻辑回归而是自己根据来训练逻辑回归所以如果在中选择了不同的参数那就可以学习到比较复杂的特征就可以得到一个更好的假设比使用原始输入时得到的假设更好构造多项式特征神经网络中神经元相互连接的方式称为神经网络的架构',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-10 22:18:11',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">coygOdegaard</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 1.05rem;">大数据<sup>1</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 1.05rem;">算法<sup>2</sup></a><a href="/tags/%E8%AF%AD%E8%A8%80/" style="font-size: 1.05rem;">语言<sup>3</sup></a></div></div><hr/></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/04/"><span class="card-archive-list-date">四月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">一月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">4</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/10/"><span class="card-archive-list-date">十月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">12</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"></span></div></div><h1 class="post-title" itemprop="name headline">机器学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-05-08T12:23:38.000Z" title="发表于 2025-05-08 20:23:38">2025-05-08</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-07-10T14:18:11.163Z" title="更新于 2025-07-10 22:18:11">2025-07-10</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为长沙"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>长沙</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><header><h1 id="CrawlerTitle" itemprop="name headline">机器学习</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">Odegaard</span><time itemprop="dateCreated datePublished" datetime="2025-05-08T12:23:38.000Z" title="发表于 2025-05-08 20:23:38">2025-05-08</time><time itemprop="dateCreated datePublished" datetime="2025-07-10T14:18:11.163Z" title="更新于 2025-07-10 22:18:11">2025-07-10</time></header><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h2 id="机器学习是什么"><a href="#机器学习是什么" class="headerlink" title="机器学习是什么"></a>机器学习是什么</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/3de7ec9ae75d52a8b074555bcefbc9fe_.jpg" alt="3de7ec9ae75d52a8b074555bcefbc9fe_"></p>
<p>目前存在几种不同类型的学习算法。主要的两种类型被我们称之为监督学习和无监督学习。监督学习这个想法是指，我们将教计算机如何去完成任务，而在无监督学习中，我们打算让它自己进行学习。</p>
<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>监督学习指的就是我们给学习算法一个数据集，这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。</p>
<p><strong>回归问题</strong>：试着推测出一个连续值的结果。下面的房子例子就是回归问题，要推测的结果就是房子的价格。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/67f278633b69c123d1c93d555408016.jpg" alt="67f278633b69c123d1c93d555408016" style="zoom:67%;" />

<p>回归这个词的意思是，我们在试着推测出这一系列连续值属性。</p>
<p><strong>分类问题</strong>，分类指的是，我们试着推测出<strong>离散的输出值</strong>：0或1，良性或恶性。感觉就是判断给出的数据属于哪一类。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/d7af2031bf79a5b02eeedb033358c4d.jpg" alt="d7af2031bf79a5b02eeedb033358c4d" style="zoom:67%;" />

<p>在这个例子中只有一个特征，就是肿瘤的尺寸，在其它一些机器学习问题中，可能会遇到不止一种特征。举个例子，我们不仅知道肿瘤的尺寸，还知道对应患者的年龄。在其他机器学习问题中，通常有更多的特征，比如肿块密度，肿瘤细胞尺寸的一致性和形状的一致性等等，还有一些其他的特征。之后会讲一个算法，叫支持向量机，里面有一个巧妙的数学技巧，能让计算机处理无限多个特征。</p>
<p>回归问题和分类问题都属于监督学习，其基本思想是，数据集中的每个样本都有相应的“正确答案”，再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。回归问题，即通过回归来推出一个连续的输出；分类问题，其目标是推出一组离散的结果。</p>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么，任何信息都不知道，只知道是一个数据集。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/a4588fb01a01f7e1fb17e6c89241650.jpg" alt="a4588fb01a01f7e1fb17e6c89241650"></p>
<p>这个图是上面肿瘤的例子，⭕代表良性，❌代表恶性，在监督学习中有这种标志说明是什么情况，但在无监督学习中没有标志，只是数据。</p>
<p>针对数据集，无监督学习能判断出数据有两个不同的聚集簇。这是一个，那是另一个，二者不同。是的，无监督学习算法可能会把这些数据分成两个不同的簇，这个就叫做聚类算法。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/46a41b85419ef28ecb55b469f6c7c89.jpg" alt="46a41b85419ef28ecb55b469f6c7c89" style="zoom:67%;" />

<p>无监督学习就是我们没法提前告知算法一些信息。</p>
<p>就是这里是有一堆数据，我不知道数据里面有什么，我不知道谁是什么类型，我甚至不知道人们有哪些不同的类型，这些类型又是什么。但你能自动地找到数据中的结构吗？就是说你要自动地聚类那些个体到各个类，我没法提前知道哪些是哪些。因为我们没有给算法正确答案来回应数据集中的数据，所以这就是无监督学习。</p>
<p>上面的都是<strong>聚类</strong>的例子，聚类只是无监督学习的一种。</p>
<p>接下来介绍的鸡尾酒宴问题属于无监督学习中的<strong>盲源分离</strong>问题。</p>
<p>可能在一个这样的鸡尾酒宴中的两个人，他俩同时都在说话，假设现在是在个有些小的鸡尾酒宴中。我们放两个麦克风在房间中，因为这些麦克风在两个地方，离说话人的距离不同每个麦克风记录下不同的声音，虽然是同样的两个说话人。听起来像是两份录音被叠加到一起，或是被归结到一起，产生了我们现在的这些录音。另外，这个算法还会区分出两个音频资源，这两个可以合成或合并成之前的录音，实际上，鸡尾酒算法的第一个输出结果是：</p>
<p>1，2，3，4，5，6，7，8，9，10,</p>
<p>第二个输出是这样：</p>
<p>1，2，3，4，5，6，7，8，9，10。</p>
<p>第一个输出代表分离出的第一个声源，第二个输出代表分离出的第二个声源。</p>
<p>这里的数字序列可能是对分离后信号的简化表示。实际应用中，输出是时间序列信号（如音频波形），每个数字可能代表某个时间点的信号强度或特征。无需去深度思考。</p>
<h1 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h1><h2 id="模型表示"><a href="#模型表示" class="headerlink" title="模型表示"></a>模型表示</h2><p>监督学习的第一个例子。预测住房价格的，我们要使用一个数据集，数据集包含俄勒冈州波特兰市的住房价格。在这里，我要根据不同房屋尺寸所售出的价格，画出我的数据集。比方说，如果你朋友的房子是1250平方尺大小，你要告诉他们这房子能卖多少钱。	</p>
<p>它被称作监督学习是因为对于每个数据来说，我们给出了“正确的答案”，即告诉我们：根据我们的数据来说，房子实际的价格是多少，而且，更具体来说，这是一个回归问题。回归一词指的是，我们根据之前的数据预测出一个准确的输出值，对于这个例子就是价格。</p>
<p><strong>在监督学习中我们有一个数据集，这个数据集被称训练集。</strong></p>
<p><strong>在整个课程中用小写的m来表示训练样本的数目。</strong></p>
<p>假如上面房子的回归问题的训练集（<strong>Training Set</strong>）如下表所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250508215836945.png" alt="image-20250508215836945"></p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250508215909094.png" alt="image-20250508215909094" style="zoom:40%;" />

<p>将训练集“喂”给我们的学习算法，进而学习得到一个假设h，然后将我们要预测的房屋的尺寸作为输入变量输入给h，预测出该房屋的交易价格作为输出变量输出为结果。</p>
<p>h表示的是一个函数，由学习算法根据训练集输出，输入是房屋尺寸大小，输出的是房子价格。</p>
<p>h的一种可能表达方式为：<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 x<br>$$<br>因为只含有<strong>一个特征&#x2F;输入变量</strong>，因此这样的问题叫作单变量线性回归问题。</p>
<h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520100402742.png" alt="image-20250520100402742"></p>
<p>有一个像这样的训练集， 𝑚代表了训练样本的数量，比如 𝑚 &#x3D; 47。而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 x<br>$$<br>接下来为我们的模型选择合适的<strong>参数</strong>（ parameters） 𝜃0 和 𝜃1，在房价问题这个例子中便是直线的斜率和在𝑦 轴上的截距。  我们选择的参数决定了我们得到的直线相对于我们的训练集的准确程度，模型所预测的值与训练集中实际值之间的<strong>差距</strong>（下图中蓝线所指）就是<strong>建模误差</strong>（ modeling error）。  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520101109903.png" alt="image-20250520101109903">目标便是选择出可以使得建模误差的平方和能够最小的模型参数， 即使得代价函数<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250520101757329.png" alt="image-20250520101757329" style="zoom:50%;" />最小。</p>
<p>绘制一个等高线图，三个坐标分别为𝜃0和𝜃1 和𝐽(𝜃0, 𝜃1)：  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250520101905831.png" alt="image-20250520101905831" style="zoom:33%;" />可以看出在三维空间中存在一个使得𝐽(𝜃0, 𝜃1)最小的点。  </p>
<p>代价函数也被称作平方误差函数，有时也被称为平方误差代价函数，代价函数是解决回归问题最常用的手段。</p>
<h2 id="代价函数的直观理解"><a href="#代价函数的直观理解" class="headerlink" title="代价函数的直观理解"></a>代价函数的直观理解</h2><p>代价函数是用来干嘛的，我们为什么要用它。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520102928659.png" alt="image-20250520102928659"></p>
<p>为了便于理解，使𝜃0&#x3D;0。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520103215675.png" alt="image-20250520103215675"></p>
<p>yi代表的是训练集中的数据。h𝜃的参数是x，J的参数是𝜃1。上图可以看出当𝜃1&#x3D;1时，代价函数J&#x3D;0。</p>
<p>接下来时当𝜃1&#x3D;0.5时：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520104130361.png" alt="image-20250520104130361"></p>
<p>等于1时：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520104249847.png" alt="image-20250520104249847"></p>
<p>对于每个𝜃1的值，都对应着一个假设函数的值或者一条直线，并且根据每个不同的𝜃1，我们都可以得到一个不同的J(𝜃1)的值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520104644091.png" alt="image-20250520104644091"></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数𝐽(𝜃0, 𝜃1) 的最小值。  </p>
<p>梯度下降背后的思想是：开始时我们随机选择一个参数的组合(𝜃0, 𝜃1, . . . . . . , 𝜃𝑛)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（ local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（ global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。  </p>
<p>这个算法时怎么工作的，可以这样想：想象一下你正站立在山的一点上，  在梯度下降算法中，我们要做的就是旋转 360 度，看看我们的周围哪个方向可以最快下山。来到山坡上，我们站在山坡上的一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。</p>
<p>批量梯度下降（ batch gradient descent）算法的公式为：  <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250520110754153.png" alt="image-20250520110754153" style="zoom:50%;" /></p>
<p>上面那行英语的意思是，反复用这个公式直到收敛。其中𝑎是学习率（ learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。  </p>
<p>符号<code>:=</code>的意思是赋值，这是一个赋值运算符。单独的<code>=</code>代表的是比较运算符。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520145329030.png" alt="image-20250520145329030"></p>
<p>梯度下降中，我们要同时更新𝜃0和𝜃1，当 𝑗 &#x3D; 0 和𝑗 &#x3D; 1时，会产生更新，所以你将更新𝐽(𝜃0)和𝐽(𝜃1)。  记住，要<strong>同时更新</strong>，不能先更新一个再更新另一个，先更新其中一个的话会导致接下来算出的微分项的值出现变换，因为其中一个值变了。</p>
<h2 id="梯度下降的直观理解"><a href="#梯度下降的直观理解" class="headerlink" title="梯度下降的直观理解"></a>梯度下降的直观理解</h2><p>梯度下降算法：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250520150117798.png" alt="image-20250520150117798" style="zoom:50%;" />，描述：对𝜃赋值，使得𝐽(𝜃)按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中𝑎是学习率（ learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。  </p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520150210495.png" alt="image-20250520150210495"></p>
<p>求导的目的，基本上可以说取这个红点的切线，  现在，这条线有一个正斜率，也就是说它有正导数，因此，我得到的新的𝜃1， 𝜃1更新后等于𝜃1减去一个正数乘以𝑎。  </p>
<p>如果𝑎太小或𝑎太大会出现什么情况：  </p>
<p>如果𝑎太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果𝑎太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。</p>
<p>如果𝑎太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果𝑎太大，它会导致无法收敛，甚至发散。  </p>
<p>假设将𝜃1初始化在局部最低点，因为它已经在一个局部的最优处或局部最低点，结果是局部最优点的导数将等于零，使得𝜃1不再改变，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率𝑎保持不变时，梯度下降也可以收敛到局部最低点。  </p>
<p>在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小𝑎。  </p>
<p>可以用梯度下降算法来最小化任何代价函数𝐽，不只是线性回归中的代价函数𝐽。  </p>
<h2 id="梯度下降的线性回归"><a href="#梯度下降的线性回归" class="headerlink" title="梯度下降的线性回归"></a>梯度下降的线性回归</h2><p>用梯度下降算法，并将其应用于具体的拟合直线的线性回归算法里。  </p>
<p>先计算微分项：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520151937828.png" alt="image-20250520151937828"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520152054863.png" alt="image-20250520152054863"></p>
<p>所以，算法会被改写为：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520152212561.png" alt="image-20250520152212561"></p>
<p>不断重复，直到收敛。记住，𝜃0和𝜃1要同时更新。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520152415146.png" alt="image-20250520152415146"></p>
<p>使用梯度下降算法是因为它更容易到达局部最小值，而根据初始化的不同，会得到不同的局部最优解。但是，事实证明，用于线性回归的代价函数总是一个弓形样子的函数，叫作凸函数，这种函数没有局部最优解，只有一个全局最优解。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250520152728280.png" alt="image-20250520152728280"></p>
<p>一般来说初始化参数的时候都设为0。</p>
<p>刚刚使用的算法，有时也称为批量梯度下降。  ”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有𝑚个训练样本求和。  </p>
<h1 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h1><h2 id="多维特征"><a href="#多维特征" class="headerlink" title="多维特征"></a>多维特征</h2><p>对房价模型增加更多的特征，例如房间数楼层等，构成一个含有多个变量的模型，模型中的特征为(𝑥1, 𝑥1, . . . , 𝑥𝑛)。  </p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250705112627927.png" alt="image-20250705112627927"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250705112649877.png" alt="image-20250705112649877"></p>
<p>这上面的公式是𝜃TX的原因是上面的X(2)是一列，形状是[4,1]，jupyter里面的数据的形状是[1,4]，所以里面的公式是X𝜃T，具体的情况要具体分析，记住基本公式，参数乘以变量。</p>
<h2 id="多变量梯度下降"><a href="#多变量梯度下降" class="headerlink" title="多变量梯度下降"></a>多变量梯度下降</h2><p>在多变量线性回归中的代价函数，这个代价函数是所有建模误差的平方和，即： <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705145029436.png" alt="image-20250705145029436" style="zoom:50%;" />，其中<br>$$<br>h_θ (x)&#x3D;θ^T X&#x3D;θ_0+θ_1 x_1+θ_2 x_2+…+θ_n x_n<br>$$<br>多变量线性回归的批量梯度下降算法为：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705145632954.png" alt="image-20250705145632954" style="zoom:80%;" /></p>
<p>即<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705145701988.png" alt="image-20250705145701988" style="zoom:80%;" />，求导得：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705145729471.png" alt="image-20250705145729471" style="zoom:80%;" /></p>
<p>跟前面单变量的公式没有什么大变化，就是求导后需要计算的变多了。</p>
<p>计算代价函数的代码如下：</p>
<pre><code class="hljs plaintext">def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))</code></pre>

<h2 id="梯度下降法实践-1-特征缩放"><a href="#梯度下降法实践-1-特征缩放" class="headerlink" title="梯度下降法实践 1-特征缩放"></a>梯度下降法实践 1-特征缩放</h2><p>在我们面对多维特征问题的时候，我们要保证这些特征都具有<strong>相近的尺度</strong>，这将帮助梯度下降算法更快地收敛。</p>
<p>以房价问题为例，假设我们使用两个特征，房屋的尺寸和房间的数量，尺寸的值为 0- 2000 平方英尺，而房间数量的值则是 0-5，以两个参数分别为横纵坐标，绘制<strong>代价函数</strong>的等高线图（在这里先忽略𝜃0），<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250705151217067.png" alt="image-20250705151217067" style="zoom:50%;" /></p>
<p>能看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</p>
<p>解决的方法是尝试将所有特征的尺度都尽量缩放到**-1 到 1** 之间。 ，这也是在做特征缩放时的通常目的，但其实并不严格要求必须是-1到1，在这些附近都可以，重点是将范围靠近-1到1。所以，如果有一个特征也就是变量的范围是-0.0001到0.0001的话，得对其进行扩展。一般在-3到3，-1&#x2F;3到1&#x2F;3都是可以的。</p>
<p>除了将特征除以它的最大值外，还可以进行一种叫作均值归一化的工作，包括：</p>
<p>1、将原来的变量值减去平均值除以（最大值-最小值），一般用这个就足够了</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/3f7dac1d8c77ee704aca91562a78b88.jpg" alt="3f7dac1d8c77ee704aca91562a78b88"></p>
<p>2、<br>$$<br>x_n&#x3D;(x_n-μ_n)&#x2F;s_n，其中 μ_n是平均值，s_n是标准差。<br>$$</p>
<h2 id="梯度下降法实践-2-学习率"><a href="#梯度下降法实践-2-学习率" class="headerlink" title="梯度下降法实践 2-学习率"></a>梯度下降法实践 2-学习率</h2><p>如何确定梯度下降算法在正常工作，画图表：</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250706210423707.png" alt="image-20250706210423707" style="zoom:80%;" />

<p>梯度下降算法的每次迭代受到学习率的影响，如果学习率𝑎过小，则达到收敛所需的迭代次数会非常高；如果学习率𝑎过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</p>
<p>通常可以考虑尝试些学习率：𝛼 &#x3D; 0.01， 0.03， 0.1， 0.3， 1， 3， 10</p>
<h2 id="特征和多项式回归"><a href="#特征和多项式回归" class="headerlink" title="特征和多项式回归"></a>特征和多项式回归</h2><p>多项式回归，可以使用线性回归的方式来拟合非常复杂的函数，或者是非线性函数。</p>
<p>以预测房价模型为例（在线性回归模型中你可以选择提供的特征作为特征，也可以选择自己创建一个新的特征，哎下面的例子中，题目给了临街宽度和纵深两个特征，我们也可以自己创建一个特征——面积，这样子可以简化线性回归模型，得到一个更好的模型）：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706211418121.png" alt="image-20250706211418121"></p>
<p>线性回归并不适用于所有数据，有时我们需要曲线来适应我们的数据，二次方或者三次方模型：<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 x_1+θ_2 x_2^2，h_θ (x)&#x3D;θ_0+θ_1 x_1+θ_2 x_2^2+θ_3 x_3^3<br>$$<br>另外，我们可以令<br>$$<br>𝑥_2 &#x3D; 𝑥_2^2, 𝑥_3 &#x3D; 𝑥_3^3<br>$$<br>，从而将模型转化为线性回归模型，由于次方的存在导致参数范围被扩大了很多，所以在运行梯度下降算法前，必须进行<strong>特征缩放</strong>。除了上面给出的这一种，还有一种是开平方：<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 (size)+θ_2 \sqrt{size}<br>$$<br>通过不同的参数形式，最后的曲线也会有所不同。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706212254715.png" alt="image-20250706212254715"></p>
<h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706214657023.png" alt="image-20250706214657023"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706214739079.png" alt="image-20250706214739079"></p>
<p>运用正规方程方法求解参数：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706214804481.png" alt="image-20250706214804481"></p>
<p>对于那些<strong>不可逆的矩阵</strong>（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是<strong>不能用</strong>的。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250706214841139.png" alt="image-20250706214841139"></p>
<p>只要特征变量的数目并不大，标准方程是一个很好的计算参数𝜃的替代方法。具体地说，只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法。  </p>
<pre><code class="hljs plaintext">import numpy as np
def normalEqn(X, y):
	theta = np.linalg.inv(X.T@X)@X.T@y #X.T@X 等价于 X.T.dot(X)
	return theta</code></pre>

<p>注意，这里返回的theta不是一个数，而是一个元组。</p>
<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>在分类问题中，要预测的变量y是离散的值。  </p>
<p>在分类问题中，我们尝试预测的是结果是否属于某一个类（例如正确或错误），从二元的分类问题开始。</p>
<p>我们将因变量可能属于的两个类分别称为负向类和正向类，则因变量 y&#x3D;0或1 ，其中 0 表示负向类，1 表示正向类。</p>
<p>逻辑回归算法的性质是：它的输出值永远在 0 到 1 之间。  逻辑回归算法是一个分类算法，适用于y取离散的值的情况下。</p>
<h2 id="假说表示"><a href="#假说表示" class="headerlink" title="假说表示"></a>假说表示</h2><p>为什么线性回归算法不适用于分类问题？</p>
<p>根据线性回归模型我们只能预测连续的值，然而对于分类问题（例子是肿瘤分类），我们需要输出0或1，我们可以预测：<br>$$<br>当h_θ (x)&gt;&#x3D;0.5时，预测 y&#x3D;1。<br>当h_θ (x)&lt;0.5时，预测 y&#x3D;0 。<br>$$<br>没有极端数据出现的时候使用线性回归算法看着也可以，但一旦极端数据出现，整体的判断标准就会被破坏。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707095415565.png" alt="image-20250707095415565"></p>
<p>有极端数据出现：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707095442011.png" alt="image-20250707095442011"></p>
<p>所以线性回归模型并不适用于分类问题。</p>
<p>逻辑回归模型的假设是：<br>$$<br>h_θ(x)&#x3D;g(θ^T X)其中：X 代表特征向量，g代表逻辑函数是一个常用的逻辑函数，S形函数，公式为： g(z)&#x3D;1&#x2F;(1+e^{-z} )。<br>$$<br>θT*X，这个就是<strong>线性回归模型的结果</strong>（这里的参数向量θ是n行1列的，X的数据是一列一列的），所以逻辑回归模型是对线性回归模型的值进行处理。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707100936257.png" alt="image-20250707100936257"></p>
<pre><code class="hljs plaintext">import numpy as np
def sigmoid(z):
	return 1 / (1 + np.exp(-z))</code></pre>

<p>ℎ𝜃(𝑥)的意思是，对于给定的输入变量，根据选择的参数计算输出变量&#x3D;1 的可能性。如果对于给定的𝑥，通过已经确定的参数计算得出ℎ𝜃(𝑥) &#x3D; 0.7，则表示有 70%的几率𝑦为正向类，相应地𝑦为负向类的几率为 1-0.7&#x3D;0.3。</p>
<p><strong>逻辑回归的本质</strong>：逻辑回归是一种<strong>线性分类模型</strong>。它通过一个<strong>线性方程</strong>（例如，<em>z</em>&#x3D;<em>θ</em>0+<em>θ</em>1<em>x</em>1+<em>θ</em>2<em>x</em>2）将输入特征（如测试1和测试2的结果）<strong>映射</strong>到一个<strong>概率值</strong>（通过sigmoid函数）。决策边界（即区分接受&#x2F;抛弃的阈值）是线性的，比如一条直线（在二维特征空间中）。</p>
<h2 id="判定边界"><a href="#判定边界" class="headerlink" title="判定边界"></a>判定边界</h2><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250707102404958.png" alt="image-20250707102404958" style="zoom:67%;" />

<p>根据逻辑回归模型的这个图，我们知道当𝑧 &#x3D; 0 时，𝑔(𝑧) &#x3D; 0.5；𝑧 &gt; 0 时，𝑔(𝑧) &gt; 0.5；𝑧 &lt; 0 时，𝑔(𝑧) &lt; 0.5；</p>
<p>又 𝑧 &#x3D; 𝜃𝑇𝑥 ，即：</p>
<p>𝜃𝑇𝑥 &gt;&#x3D; 0 时，预测 𝑦 &#x3D; 1；𝜃𝑇𝑥 &lt; 0 时，预测 𝑦 &#x3D; 0。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707102617314.png" alt="image-20250707102617314"></p>
<p>对于上面那个模型，我们可以很明显地看出是一条直线将预测结果分成两部分。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707102727734.png" alt="image-20250707102727734"></p>
<p>又两个例子可以看出，我们要根据分界线的形状来判断我们应该使用的分界线函数是什么。</p>
<h2 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h2><p>对于线性回归模型，定义的代价函数是所有模型误差的平方和。要是将逻辑回归模型的函数代入到这个代价函数中，得到的代价函数将是一个非凸函数，这意味着代价函数会有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707103625333.png" alt="image-20250707103625333"></p>
<p>定义代价函数为：<br>$$<br>J(θ)&#x3D;\frac{1}{m} ∑_{i&#x3D;1}^mCost(h_θ (x^i ),y^i )<br>$$<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707104627579.png" alt="image-20250707104627579"></p>
<p>h𝜃x的取值范围在0~1。</p>
<p>这样构建的Cost函数的特点是：当实际的 𝑦 &#x3D; 1 且ℎ𝜃(𝑥)也为 1 时误差为 0，当 𝑦 &#x3D; 1 但ℎ𝜃(𝑥)不为 1 时误差随着ℎ𝜃(𝑥)变小而变大；当实际的 𝑦 &#x3D; 0 且ℎ𝜃(𝑥)也为 0 时代价为 0，当𝑦 &#x3D; 0 但ℎ𝜃(𝑥)不为 0 时误差随着 ℎ𝜃(𝑥)的变大而变大。</p>
<p>将Cost函数进行简化，就是用一个表达式表达出来，如下：<br>$$<br>Cost(h_θ (x),y)&#x3D;-y×log(h_θ (x))-(1-y)×log(1-h_θ (x))<br>$$<br>代入代价函数为：<br>$$<br>J(θ)&#x3D;-\frac{1}{m}∑_{i&#x3D;1}^m[y^{(i)} log(h_θ (x^{(i)} ))+(1-y^{(i)})log(1-h_θ (x^{(i)} ))]<br>$$</p>
<pre><code class="hljs plaintext">import numpy as np
def cost(theta, X, y):
  theta = np.matrix(theta)
  X = np.matrix(X)
  y = np.matrix(y)
  first = np.multiply(-y, np.log(sigmoid(X* theta.T)))
  second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T)))
  return np.sum(first - second) / (len(X))</code></pre>

<p>sigmoid函数在上面定义了。</p>
<p>梯度下降算法的公式和前面的一样，记住，<strong>同时更新</strong>所有参数。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250707111211261.png" alt="image-20250707111211261"></p>
<p>通过观察梯度下降算法的式子可以发现，这个式子和之前线性回归的梯度下降算法的式子是一样的，但<strong>ℎ𝜃(𝑥)的式子是不同的</strong>。</p>
<p>特征缩放的技巧也适用于逻辑回归。</p>
<h2 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h2><p>共轭梯度法，BFGS (变尺度法) 和 L-BFGS (限制变尺度法) 就是一些更高级的优化算法，它们需要有一种方法来计算 𝐽(𝜃)，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。  </p>
<p>这三种算法的具体细节可以不用取探究，因为过于复杂。</p>
<p>使用这其中任何一个算法，通常不需要手动选择学习率 𝛼，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索算法，它可以自动尝试不同的学习速率 𝛼，并自动选择一个好的学习速率 𝛼，因此它甚至可以为每次迭代选择不同的学习速率。  </p>
<p>最好不要使用 L-BGFS、 BFGS 这些算法，除非你是数值计算方面的专家。</p>
<p>如何使用这些算法，这些算法适合在很大的机器学习问题中使用。</p>
<p>在jupyter中利用的是python中的<code>scipy.optimize.fmin_tnc()</code>函数，这是一个使用截断牛顿法（TNC）寻找局部最小值的优化函数，特别适用于有界约束的优化问题。</p>
<h2 id="寻找决策边界"><a href="#寻找决策边界" class="headerlink" title="寻找决策边界"></a>寻找决策边界</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709171344091.png" alt="image-20250709171344091"></p>
<p>所以jupyter中的寻找决策边界会除以第三个参数值。</p>
<h2 id="构造多项式特征"><a href="#构造多项式特征" class="headerlink" title="构造多项式特征"></a>构造多项式特征</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710155849606.png" alt="image-20250710155849606"></p>
<p>由上图可知其中没有线性决策界限，来良好的分开两类数据，原数据有两个特征，x1和x2，可以明显看出就这两个特征无法较好拟合这些数据，所以要构造从原始特征的多项式中得到的特征，即通过数学变换，将原始特征（ <em>x</em>1 和 <em>x</em>2）扩展为一组新特征，这些新特征是原始特征的高阶多项式组合（例如，<em>x</em>1平方、<em>x</em>2平方、<em>x</em>1×<em>x</em>2、<em>x</em>1立方 等）。然后，在这些新特征上训练逻辑回归模型。</p>
<p><strong>为什么能解决非线性问题</strong>：尽管逻辑回归本身是线性的，但通过添加非线性特征（如平方项或交互项），模型在扩展后的高维特征空间中学习到的决策边界仍然是线性的，但这个边界在原始特征空间中会呈现为曲线、椭圆或其他非线性形状。这相当于给模型“添加了非线性能力”，而不改变其核心算法。</p>
<p>首先要选择<strong>阶数</strong>，阶数决定了多项式的复杂性。从二阶开始（通常足够处理大多数非线性问题），然后根据模型性能调整。</p>
<p>平方项：捕捉单个测试的非线性效应。</p>
<p>交互项（<em>x</em>1×<em>x</em>2）：捕捉两个测试的联合效应。</p>
<p>由于选择了高阶数的模型，为了避免过拟合，通常还要进行正则化操作。</p>
<h2 id="多类别分类：一对多"><a href="#多类别分类：一对多" class="headerlink" title="多类别分类：一对多"></a>多类别分类：一对多</h2><p>如何使用逻辑回归来解决多类别分类问题。</p>
<p>之前的二元分类问题的图，和现在的多类分类问题的图：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709173446894.png" alt="image-20250709173446894" style="zoom:80%;" /></p>
<p>用 3 种不同的符号来代表 3 个类别，问题就是给出 3 个类型的数据集，如何得到一个学习算法来进行分类呢？</p>
<p>面对二元分类问题可以使用逻辑回归，也可以将数据集一分为二为正类和负类，而一对多的分类思想，我们可以将其用在多类分类问题上，这个方法也被称为”一对余”方法。</p>
<p>现在我们有一个训练集，好比上图表示的有 3 个类别，我们用三角形表示 𝑦 &#x3D; 1，方框表示𝑦 &#x3D; 2，叉叉表示 𝑦 &#x3D; 3。使用一个训练集将三元分类问题转化为<strong>三个二元分类问题</strong>，先从用三角形代表的类别 1 开始，实际上我们可以创建一个，新的”伪”训练集，类型 2 和类型 3 定为负类，类型 1 设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709175113715.png" alt="image-20250709175113715"></p>
<p>这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为 1，圆形的值为 0，下面可以训练一个标准的逻辑回归分类器，这样就得到一个边界。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709175233890.png" alt="image-20250709175233890"></p>
<p>选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论𝑖值是多少，我们都有最高的概率值，我们预测𝑦就是那个值。<br>$$<br>最后我们得到一系列的模型简记为： h_θ^{(i) } (x)&#x3D;p(y&#x3D;i|x;θ)其中：i&#x3D;(1,2,3….k)<br>$$</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="过拟合的问题"><a href="#过拟合的问题" class="headerlink" title="过拟合的问题"></a>过拟合的问题</h2><p>就是过于强调拟合原始数据。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709201037517.png" alt="image-20250709201037517"></p>
<p>第一个模型是一个线性模型，欠拟合，不能很好地适应训练集；第三个模型是一个四次方的模型，过于强调拟合原始数据，而丢失了算法的本质：预测新数据。可以看出，若给出一个新的值使之预测，它将表现的很差，是过拟合，虽然能非常好地适应我们的训练集但在新输入变量进行预测时可能会效果不好。</p>
<p>分类问题中也存在这样的问题：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709201149549.png" alt="image-20250709201149549" style="zoom:80%;" /></p>
<p>以多项式理解， 𝑥 的次数越高，拟合的越好，但相应的预测的能力就可能变差。</p>
<p>如何处理过拟合问题：</p>
<p>1.丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙，例如 PCA</p>
<p>2.正则化。 保留所有的特征，但是减少参数的大小。</p>
<h2 id="代价函数-2"><a href="#代价函数-2" class="headerlink" title="代价函数"></a>代价函数</h2><p>在上面过拟合的回归问题中有以下模型：<br>$$<br>h_θ (x)&#x3D;θ_0+θ_1 x_1+θ_2 x_2^2+θ_3 x_3^3+θ_4 x_4^4<br>$$<br>正是高次项导致了过拟合的产生，所以如果能让这些高次项的系数接近于0的话，那就能很好的拟合了。</p>
<p>所以我们要做的就是在一定程度上<strong>减小</strong>这些参数𝜃的值，这就是<strong>正则化的基本方法</strong>。我们决定要减少𝜃3和𝜃4的大小，我们要做的便是修改代价函数，在其中𝜃3和𝜃4设置一点惩罚。这样做的话，在尝试最小化代价时也会将这个惩罚纳入考虑中，并最终导致选择较小一些的𝜃3和𝜃4。惩罚就是在代价函数中使𝜃3和𝜃4的占比变高，使得在最小化代价函数时，也会更多地考虑这两个参数。<br>$$<br>修改后的代价函数：min\frac{1}{2m}[∑_{i&#x3D;1}^m[(h_θ (x^{(i)} )-y^{(i)} )^2+1000θ_3^2+10000θ_4^2]]<br>$$<br>假如有非常多的特征，我们并不知道其中哪些特征要惩罚，那么就对所有的特征进行惩罚，并且让代价函数最优化的软件来选择这些惩罚的程度。这样的结果是得到了一个较为简单的能防止过拟合问题的假设：<br>$$<br>J(θ)&#x3D;\frac{1}{2m}[∑_{i&#x3D;1}^m(h_θ (x^{(i)})-y^{(i)})^2+λ∑_{j&#x3D;1}^nθ_j^2 ]<br>$$<br>其中𝜆又称为正则化参数，根据惯例，我们<strong>不对𝜃0 进行惩罚</strong>。经过正则化处理的模型与原模型的可能对比如下图所示：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250709202809943.png" alt="image-20250709202809943"></p>
<p>如果选择的正则化参数λ<strong>过大</strong>，则会把所有的参数<strong>都最小化</strong>了，导致模型变成 ℎ𝜃(𝑥) &#x3D; 𝜃0，也就是上图中红色直线所示的情况，造成<strong>欠拟合</strong>。  </p>
<h2 id="正则化线性回归"><a href="#正则化线性回归" class="headerlink" title="正则化线性回归"></a>正则化线性回归</h2><p>正则化<strong>线性回归</strong>的代价函数是：<br>$$<br>J(θ)&#x3D;\frac{1}{2m}[∑_{i&#x3D;1}^m(h_θ (x^{(i)})-y^{(i)})^2+λ∑_{j&#x3D;1}^nθ_j^2 ]<br>$$<br>由于𝜃0没有进行正则化，所以梯度下降算法将会分成两种情况：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709210159349.png" alt="image-20250709210159349" style="zoom:50%;" /></p>
<p>对第二个式子（𝑗 &#x3D; 1,2, . . . , 𝑛 ）进行调整可以得到：<br>$$<br>θ_j:&#x3D;θ_j (1-a \frac{λ}{m})-a \frac{1}{m} ∑_{i&#x3D;1}^m(h_θ (x^{(i)})-y^{(i)})x_j^{(i)}<br>$$<br>可以看出，正则化线性回归的梯度下降算法的变化在于，每次都在原有算法更新规则的基础上令𝜃值减少了一个额外的值。</p>
<p>利用正规方程来求解正则化线性回归模型：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709211926347.png" alt="image-20250709211926347" style="zoom:50%;" />，图中的矩阵尺寸为 (𝑛 + 1) ∗ (𝑛 + 1)，因为不算𝜃0还有n个特征。</p>
<h2 id="正则化的逻辑回归模型"><a href="#正则化的逻辑回归模型" class="headerlink" title="正则化的逻辑回归模型"></a>正则化的逻辑回归模型</h2><p>这是正则化的<strong>逻辑回归</strong>的代价函数。</p>
<p>给代价函数增加一个正则化的表达式，得到代价函数：<br>$$<br>J(θ)&#x3D;\frac{1}{m} ∑_{i&#x3D;1}^m[-y^{(i)} log(h_θ (x^{(i)} ))-(1-y^{(i)} )log(1-h_θ (x^{(i)} ))]+\frac{λ}{2m} ∑_{j&#x3D;1}^nθ_j^2<br>$$</p>
<pre><code class="hljs plaintext">import numpy as np
def costReg(theta, X, y, learningRate):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    first = np.multiply(-y, np.log(sigmoid(X*theta.T)))
    second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))
    reg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:theta.shape[1]],2))
    return np.sum(first - second) / (len(X)) + reg</code></pre>

<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250709213440950.png" alt="image-20250709213440950" style="zoom:67%;" />

<p>这边的 h 函数是sigmoid函数。</p>
<p><strong>𝜃0不参与其中的任何一个正则化</strong>。</p>
<p>接下来的课程中，我们将学习一个非常强大的非线性分类器，无论是<strong>线性回归</strong>问题，还是<strong>逻辑回归</strong>问题，都可以<strong>构造多项式</strong>来解决。你将逐渐发现还有更强大的非线性分类器，可以用来解决多项式回归问题。  </p>
<h1 id="神经网络：表述"><a href="#神经网络：表述" class="headerlink" title="神经网络：表述"></a>神经网络：表述</h1><h2 id="非线性假设"><a href="#非线性假设" class="headerlink" title="非线性假设"></a>非线性假设</h2><p>无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。</p>
<p>使用非线性的多项式项，能够帮助我们建立更好的分类模型，但相应的我们要计算的特征数会大大增多，普通的逻辑回归模型，不能有效地处理这么多的特征，这时候就需要神经网络。</p>
<h2 id="模型表示-1"><a href="#模型表示-1" class="headerlink" title="模型表示"></a>模型表示</h2><p>神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元）采纳一些特征作为输出，并且根据本身的模型提供一个输出。</p>
<p>以逻辑回归模型作为学习模型的神经元示例：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710173745443.png" alt="image-20250710173745443"></p>
<p>解读：上面的黄圈看作是神经元，左边的蓝圈和黄圈的连线看作是输入&#x2F;树突，黄圈右边的线看作是输出&#x2F;轴突。通过树突传递一些信息，然后神经元做一些计算，然后通过轴突输出计算结果。这个图表表示的是对h的计算，而h是sigmoid函数。x1、x2、x3是输入结点，额外的结点x0被称为<strong>偏置单位</strong>，因为x0总是等于1。x0可画可不画，根据具体情况来。</p>
<p>在神经网络中，参数𝜃又可被称为权重。</p>
<p>上面的一个小黄圈代表一个单一的神经元，而神经网络是不同的神经元组合在一起的集合。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710175318298.png" alt="image-20250710175318298"></p>
<p>第一层叫作输入层，在这一层输入特征项x1、x2、x3；最后一层叫作输出层，因为这一层的神经元输出假设的最终计算结果；中间的一层称作隐藏层，神经网络中可以有不止一个隐藏层，非输出层和输入层的都叫做隐藏层。</p>
<p>在隐藏层出现的蓝色圈被称作偏置单位，它的值永远是1。</p>
<p>下面的图有3个输入单元和3个隐藏单元。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710213148069.png" alt="image-20250710213148069"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710213208144.png" alt="image-20250710213208144"></p>
<p>每一个𝑎都是由上一层所有的𝑥和每一个𝑥所对应的参数决定的，这样从左到右的算法称为前向传播算法。</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250710213756450.png" alt="image-20250710213756450" style="zoom:67%;" />  

<p><code>𝜃*X</code>不会等于a，因为g(𝜃*X)&#x3D;a</p>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250710215327579.png" alt="image-20250710215327579" style="zoom:70%;" />

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710220504253.png" alt="image-20250710220504253"></p>
<p><strong>要注意偏置单位的添加。</strong></p>
<p>如果我们暂时只看第二层和第三层的话：<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250710215557120.png" alt="image-20250710215557120" style="zoom:33%;" /></p>
<p>可以发现，其实神经网络就像是 logistic regression，只不过我们把 logistic regression 中的输入向量[𝑥1 ∼ 𝑥3] 变成了中间层的[𝑎1(2) ∼ 𝑎3(2)], 即:<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="../source/imgs/${fiilname}/image-20250710215730645.png" alt="image-20250710215730645" style="zoom:50%;" /></p>
<p>特征项a1、a2、a3是作为输入的函数来学习的，所以在神经网络中，它没有使用输入特征x1、x2、x3来训练逻辑回归，而是自己根据a1、a2、a3来训练逻辑回归，所以如果在𝜃1中选择了不同的参数，那就可以学习到比较复杂的特征，就可以得到一个更好的假设，比使用原始输入时得到的假设更好（构造多项式特征？）</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/../source/imgs/$%7Bfiilname%7D/image-20250710221653096.png" alt="image-20250710221653096"></p>
<p>神经网络中神经元相互连接的方式称为神经网络的架构。</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">Odegaard</div><div class="post-copyright__author_desc"></div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/')">机器学习</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=机器学习&amp;url=http://example.com/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">coygOdegaard</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"></div></div></div><div class="post_share"><div class="social-share" data-image="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2025/05/05/%E7%9C%9F%E9%A2%98%E6%8A%80%E5%B7%A7/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">真题技巧</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__description"></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">引言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">1.1.</span> <span class="toc-text">机器学习是什么</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.</span> <span class="toc-text">监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.</span> <span class="toc-text">无监督学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.</span> <span class="toc-text">单变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA"><span class="toc-number">2.1.</span> <span class="toc-text">模型表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">2.3.</span> <span class="toc-text">代价函数的直观理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.4.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%9B%B4%E8%A7%82%E7%90%86%E8%A7%A3"><span class="toc-number">2.5.</span> <span class="toc-text">梯度下降的直观理解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">2.6.</span> <span class="toc-text">梯度下降的线性回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">3.</span> <span class="toc-text">多变量线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81"><span class="toc-number">3.1.</span> <span class="toc-text">多维特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">3.2.</span> <span class="toc-text">多变量梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B5-1-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="toc-number">3.3.</span> <span class="toc-text">梯度下降法实践 1-特征缩放</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AE%9E%E8%B7%B5-2-%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">3.4.</span> <span class="toc-text">梯度下降法实践 2-学习率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%92%8C%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-number">3.5.</span> <span class="toc-text">特征和多项式回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-number">3.6.</span> <span class="toc-text">正规方程</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">4.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">4.1.</span> <span class="toc-text">分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%81%87%E8%AF%B4%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.2.</span> <span class="toc-text">假说表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A4%E5%AE%9A%E8%BE%B9%E7%95%8C"><span class="toc-number">4.3.</span> <span class="toc-text">判定边界</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-1"><span class="toc-number">4.4.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E7%BA%A7%E4%BC%98%E5%8C%96"><span class="toc-number">4.5.</span> <span class="toc-text">高级优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%BB%E6%89%BE%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="toc-number">4.6.</span> <span class="toc-text">寻找决策边界</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%84%E9%80%A0%E5%A4%9A%E9%A1%B9%E5%BC%8F%E7%89%B9%E5%BE%81"><span class="toc-number">4.7.</span> <span class="toc-text">构造多项式特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%EF%BC%9A%E4%B8%80%E5%AF%B9%E5%A4%9A"><span class="toc-number">4.8.</span> <span class="toc-text">多类别分类：一对多</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">正则化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">5.1.</span> <span class="toc-text">过拟合的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-2"><span class="toc-number">5.2.</span> <span class="toc-text">代价函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">5.3.</span> <span class="toc-text">正则化线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E7%9A%84%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">5.4.</span> <span class="toc-text">正则化的逻辑回归模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%9A%E8%A1%A8%E8%BF%B0"><span class="toc-number">6.</span> <span class="toc-text">神经网络：表述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%81%87%E8%AE%BE"><span class="toc-number">6.1.</span> <span class="toc-text">非线性假设</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%A1%A8%E7%A4%BA-1"><span class="toc-number">6.2.</span> <span class="toc-text">模型表示</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="机器学习">机器学习</a><time datetime="2025-05-08T12:23:38.000Z" title="发表于 2025-05-08 20:23:38">2025-05-08</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/05/05/%E7%9C%9F%E9%A2%98%E6%8A%80%E5%B7%A7/" title="真题技巧">真题技巧</a><time datetime="2025-05-05T11:30:32.000Z" title="发表于 2025-05-05 19:30:32">2025-05-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/04/25/%E8%A1%A5%E5%85%85/" title="无题">无题</a><time datetime="2025-04-25T03:23:01.478Z" title="发表于 2025-04-25 11:23:01">2025-04-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/21/%E7%AE%97%E6%B3%95/" title="算法">算法</a><time datetime="2025-01-21T02:01:50.000Z" title="发表于 2025-01-21 10:01:50">2025-01-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/15/%E5%AF%92%E5%81%87%E5%AD%97%E8%8A%82%E9%9D%92%E8%AE%AD%E8%90%A5/" title="寒假字节青训营">寒假字节青训营</a><time datetime="2025-01-15T02:07:18.000Z" title="发表于 2025-01-15 10:07:18">2025-01-15</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2020 - 2025 By <a class="footer-bar-link" href="/" title="Odegaard" target="_blank">Odegaard</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">19</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">0</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" style="font-size: 0.88rem;">大数据<sup>1</sup></a><a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 0.88rem;">算法<sup>2</sup></a><a href="/tags/%E8%AF%AD%E8%A8%80/" style="font-size: 0.88rem;">语言<sup>3</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("04/01/2021 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2020 By 安知鱼 V1.6.12",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 Odegaard 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>